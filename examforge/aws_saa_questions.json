[
  {
    "문제번호": 1,
    "질문": "A company collects data for temperature, humidity, and atmospheric pressure in cities across multiple \ncontinents. The average volume of data that the company collects from each site daily is 500 GB. \nEach site has a high-speed Internet connection.\nThe company wants to aggregate the data from all these global sites as quickly as possible in a \nsingle Amazon S3 bucket. The solution must minimize operational complexity.\nWhich solution meets these requirements?\n회사는 여러 대륙에 걸쳐 도시의 온도, 습도 및 대기압에 대한 데이터를 수집합니다 . 회사가 매일 각 \n사이트에서 수집하는 데이터의 평균 볼륨은 500GB 입니다 . 각 사이트에는 고속 인터넷 연결이 있습니\n다.\n이 회사는 이러한 모든 글로벌 사이트의 데이터를 단일 Amazon S3 버킷에 최대한 빨리 집계하려고 합\n니다. 솔루션은 운영 복잡성을 최소화해야 합니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n​",
    "보기": {
      "B": "Upload the data from each site to an S3 bucket in the closest Region. Use S3 Cross-Region \nReplication to copy objects to the destination S3 bucket. Then remove the data from the origin S3 \nbucket.\n각 사이트의 데이터를 가장 가까운 리전의 S3 버킷에 업로드합니다 . S3 교차 리전 복제를 사용하여 대\n상 S3 버킷에 객체를 복사합니다 . 그런 다음 원본 S3 버킷에서 데이터를 제거합니다 .\n​",
      "A": "Turn on S3 Transfer Acceleration on the destination S3 bucket. Use multipart uploads to directly \nupload site data to the destination S3 bucket.\n대상 S3 버킷에서 S3 Transfer Acceleration 을 켭니다 . 멀티파트 업로드를 사용하여 사이트 데이터를 \n대상 S3 버킷에 직접 업로드합니다 .\n​\nAmazon S3 Transfer Acceleration\n→ 클라이언트와 S3 버킷 간의 장거리 파일 전송을 파일을 빠르고 쉽고 안전하게 전송할 수 있는 버킷 \n수준 기능이다 . \nTransfer Acceleration 은 전 세계에서 S3 버킷으로 전송 속도를 최적화하도록 설계되어있다 . \nTransfer Acceleration 은 Amazon CloudFront 에서 전 세계에 분산된 엣지 로케이션을 활용하며 , 엣지 로\n케이션에 도착한 데이터는 최적화된 네트워크 경로를 통해 Amazon S3로 라우팅된다 .\n​\nTransfer Acceleration 을 사용하는 이유\n​\n•전 세계 각지에서 중앙의 버킷으로 업로드하는 고객이 있는 경우\n•전 세계에 정기적으로 수 기가바이트에서 수 테라바이트의 데이터를 전송할 경우\n•Amazon S3에 업로드할 때 인터넷을 통해 사용 가능한 대역폭을 충분히 활용할 수 없는 경우\n​\n멀티파트 업로드\n→ 멀티파트 업로드를 사용하면 단일 객체를 여러 부분의 집합으로 업로드할 수 있다. \n각 부분은 객체 데이터의 연속적인 부분이며 , 객체 부분은 독립적으로 그리고 임의의 순서로 업로드할 \n수 있다. \n부분의 전송이 실패할 경우 다른 부분에 영향을 주지 않고도 해당 부분을 재전송할 수 있다. 객체의 모\n든 부분이 업로드되면 Amazon S3가 이들 부분을 수집하여 객체를 생성한다 .",
      "C": "Schedule AWS Snowball Edge Storage Optimized device jobs daily to transfer data from each site to \nthe closest Region. Use S3 Cross-Region Replication to copy objects to the destination S3 bucket.\nAWS Snowball Edge Storage Optimized 디바이스 작업을 매일 예약하여 각 사이트에서 가장 가까운 \n리전으로 데이터를 전송합니다 . S3 교차 리전 복제를 사용하여 대상 S3 버킷에 객체를 복사합니다 .\n​",
      "D": "Upload the data from each site to an Amazon EC2 instance in the closest Region. Store the data in \nan Amazon Elastic Block Store (Amazon EBS) volume. At regular intervals, take an EBS snapshot \nand copy it to the Region that contains the destination S3 bucket. Restore the EBS volume in that \nRegion.\n각 사이트의 데이터를 가장 가까운 리전의 Amazon EC2 인스턴스로 업로드합니다 . Amazon Elastic \nBlock Store(Amazon EBS) 볼륨에 데이터를 저장합니다 . 정기적으로 EBS 스냅샷을 만들어 대상 S3 버\n킷이 포함된 리전에 복사합니다 . 해당 리전에서 EBS 볼륨을 복원합니다 .\n​\n​"
    },
    "정답": "A"
  },
  {
    "문제번호": 2,
    "질문": "A company needs the ability to analyze the log files of its proprietary application. The logs are \nstored in JSON format in an Amazon S3 bucket. Queries will be simple and will run on-demand. A \nsolutions architect needs to perform the analysis with minimal changes to the existing architecture.\nWhat should the solutions architect do to meet these requirements with the LEAST amount of \noperational overhead?\n회사는 독점 애플리케이션의 로그 파일을 분석할 수 있는 능력이 필요합니다 . 로그는 Amazon S3 버킷\n에 JSON 형식으로 저장됩니다 . 쿼리는 간단하고 주문형으로 실행됩니다 . 솔루션 설계자는 기존 아키텍\n처에 대한 최소한의 변경으로 분석을 수행해야 합니다 .\n솔루션 설계자는 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까 ?\n​\n​",
    "보기": {
      "A": "Use Amazon Redshift to load all the content into one place and run the SQL queries as needed.\nAmazon Redshift 를 사용하여 모든 콘텐츠를 한 곳에 로드하고 필요에 따라 SQL 쿼리를 실행합니다 .\n​",
      "B": "Use Amazon CloudWatch Logs to store the logs. Run SQL queries as needed from the Amazon \nCloudWatch console.\nAmazon CloudWatch Logs를 사용하여 로그를 저장합니다 . Amazon CloudWatch 콘솔에서 필요에 따라 \nSQL 쿼리를 실행합니다 .\n​",
      "C": "Use Amazon Athena directly with Amazon S3 to run the queries as needed.\nAmazon S3와 함께 Amazon Athena 를 직접 사용하여 필요에 따라 쿼리를 실행합니다 .\n​\nAmazon Athena \n→ 표준 SQL을 사용하여 Amazon S3(Amazon Simple Storage Service) 에 있는 데이터를 직접 간편하\n게 분석할 수 있는 대화형 쿼리 서비스이다 . \nAWS Management Console 에서 몇 가지 작업을 수행하면 Athena 에서 Amazon S3에 저장된 데이터를 \n지정하고 표준 SQL을 사용하여 임시 쿼리를 실행하여 몇 초 안에 결과를 얻을 수 있다.",
      "D": "Use AWS Glue to catalog the logs. Use a transient Apache Spark cluster on Amazon EMR to run \nthe SQL queries as needed.\nAWS Glue를 사용하여 로그를 분류합니다 . Amazon EMR에서 임시 Apache Spark 클러스터를 사용하\n여 필요에 따라 SQL 쿼리를 실행합니다 .\n​\n​"
    },
    "정답": "C"
  },
  {
    "문제번호": 3,
    "질문": "A company uses AWS Organizations to manage multiple AWS accounts for different departments. \nThe management account has an Amazon S3 bucket that contains project reports. The company \nwants to limit access to this S3 bucket to only users of accounts within the organization in AWS \nOrganizations.\nWhich solution meets these requirements with the LEAST amount of operational overhead?\n회사는 AWS Organizations 를 사용하여 여러 부서의 여러 AWS 계정을 관리합니다 . 관리 계정에는 프\n로젝트 보고서가 포함된 Amazon S3 버킷이 있습니다 . 회사는 이 S3 버킷에 대한 액세스를 AWS \nOrganizations 의 조직 내 계정 사용자로만 제한하려고 합니다 .\n최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까 ?\n​",
    "보기": {
      "A": "Add the aws PrincipalOrgID global condition key with a reference to the organization ID to the S3 \nbucket policy.\n조직 ID에 대한 참조와 함께 aws PrincipalOrgID 글로벌 조건 키를 S3 버킷 정책에 추가합니다 .\n​\n→ aws:PrincipalOrgID\n: aws:PrincipalOrgID 조건 키는 IAM 보안 주체의 AWS Organizations 을 사용하여 AWS 리소스에 대한 \n액세스를 제어하도록 설계되어있다 .\n리소스 기반 정책에서 aws:PrincipalOrgID 조건 키를 사용하여 AWS Organizations 의 계정에서 IAM 보\n안 주체에 대한 액세스를 더 쉽게 제한할 수 있다.\n​\n▶오답\n→ aws:PrincipalTag\n조건 키는 요청 주체에 부착된 태그를 IAM 정책의 태그와 일치시키는 데 사용된다 . \nPrincipalTag 키의 값은 요청 주체가 있는 경우 IAM 태그의 값과 일치하는 태그 키의 값과 함께 입력한\n다.\n​\n→ aws:PrincipalOrgPaths\n이 키를 사용하여 요청 중인 보안 주체의 AWS Organizations 경로를 정책의 경로와 비교한다 . \n이 보안 주체는 IAM 사용자 , IAM 역할, 페더레이션 사용자 또는 AWS 계정 루트 사용자일 수 있다. \n정책에서 이 조건 키는 요청자가 AWS Organizations 의 지정된 조직 루트 또는 조직 단위(OU) 내의 계\n정 멤버인지 확인한다 .",
      "B": "Create an organizational unit (OU) for each department. Add the aws:PrincipalOrgPaths global \ncondition key to the S3 bucket policy.\n각 부서에 대한 조직 단위(OU)를 만듭니다 . aws:PrincipalOrgPaths 전역 조건 키를 S3 버킷 정책에 추\n가합니다 .\n​",
      "C": "Use AWS CloudTrail to monitor the CreateAccount, InviteAccountToOrganization, LeaveOrganization, \nand RemoveAccountFromOrganization events. Update the S3 bucket policy accordingly.\nAWS CloudTrail 을 사용하여 CreateAccount, InviteAccountToOrganization, LeaveOrganization 및 \nRemoveAccountFromOrganization 이벤트를 모니터링합니다 . 그에 따라 S3 버킷 정책을 업데이트합니\n다.\n​",
      "D": "Tag each user that needs access to the S3 bucket. Add the aws:PrincipalTag global condition key to \nthe S3 bucket policy.\nS3 버킷에 액세스해야 하는 각 사용자에 태그를 지정합니다 . aws:PrincipalTag 전역 조건 키를 S3 버킷 \n정책에 추가합니다 .\n​\n​"
    },
    "정답": "A"
  },
  {
    "문제번호": 4,
    "질문": "An application runs on an Amazon EC2 instance in a VPC. The application processes logs that are \nstored in an Amazon S3 bucket. The EC2 instance needs to access the S3 bucket without \nconnectivity to the internet.\nWhich solution will provide private network connectivity to Amazon S3?\n애플리케이션은 VPC의 Amazon EC2 인스턴스에서 실행됩니다 . 애플리케이션은 Amazon S3 버킷에 저\n장된 로그를 처리합니다 . EC2 인스턴스는 인터넷 연결 없이 S3 버킷에 액세스 해야 합니다 .\nAmazon S3에 대한 프라이빗 네트워크 연결을 제공하는 솔루션은 무엇입니까 ?\n​\n​",
    "보기": {
      "C": "Create an instance profile on Amazon EC2 to allow S3 access.\nAmazon EC2에 *인스턴스 프로파일을 생성하여 S3 액세스를 허용합니다 .\n*인스턴스 프로파일 : IAM 역할을 위한 컨테이너로서 인스턴스 시작 시 Amazon EC2 인스턴스에 역할 정보를 전달하\n는 데 사용됨 .\n​",
      "A": "Create a gateway VPC endpoint to the S3 bucket.\nS3 버킷에 대한 게이트웨이 VPC 엔드포인트를 생성합니다 .\n​\n게이트웨이 VPC 엔드포인트\n→ 게이트웨이 엔드포인트는 VPC용 인터넷 게이트웨이 또는 NAT 디바이스가 없어도 Amazon S3 및 \nDynamoDB 에 대한 안정적인 연결을 제공한다 . \n게이트웨이 엔드포인트는 AWS PrivateLink 를 활성화하지 않는다 .\n​\nGateway endpoint\n→ Gateway endpoint 를 생성한 후, Subnet 에서 Routing 만을 추가로 생성한다 . 자동으로 생성되므로 변\n경할 수 없다.\n따라서 해당 Routing table 경로를 보고 VPC Endpoint 를 통해 S3에 접근 가능한 형태이다 .\n​\n•라우팅 테이블에서 경로의 대상으로 지정해서 사용\n•VPC 내부에 위치\n•S3와 DynamoDB, 등의 일부 서비스만을 지원",
      "B": "Stream the logs to Amazon CloudWatch Logs. Export the logs to the S3 bucket.\nAmazon CloudWatch Logs로 로그를 스트리밍합니다 . 로그를 S3 버킷으로 내보냅니다 .\n​",
      "D": "Create an Amazon API Gateway API with a private link to access the S3 endpoint.\nS3 엔드포인트에 액세스하기 위한 프라이빗 링크가 있는 Amazon API Gateway API를 생성합니다 .\n​\n​"
    },
    "정답": "A"
  },
  {
    "문제번호": 5,
    "질문": "A company is hosting a web application on AWS using a single Amazon EC2 instance that stores \nuser-uploaded documents in an Amazon EBS volume. For better scalability and availability, the \ncompany duplicated the architecture and created a second EC2 instance and EBS volume in another \nAvailability Zone, placing both behind an Application Load Balancer. After completing this change, \nusers reported that, each time they refreshed the website, they could see one subset of their \ndocuments or the other, but never all of the documents at the same time.\nWhat should a solutions architect propose to ensure users see all of their documents at once?\n회사는 사용자 업로드 문서를 Amazon EBS 볼륨에 저장하는 단일 Amazon EC2 인스턴스를 사용하여 \nAWS에서 웹 애플리케이션을 호스팅하고 있습니다 . 더 나은 확장성과 가용성을 위해 이 회사는 아키텍\n처를 복제하고 다른 가용 영역에 두 번째 EC2 인스턴스와 EBS 볼륨을 생성하여 Application Load \nBalancer 뒤에 배치했습니다 . 이 변경을 완료한 후 사용자는 웹 사이트를 새로 고칠 때마다 문서의 일\n부 또는 다른 하위 집합을 볼 수 있지만 모든 문서를 동시에 볼 수는 없다고 보고했습니다 .\n솔루션 설계자는 사용자가 모든 문서를 한 번에 볼 수 있도록 무엇을 제안해야 합니까 ?\n​\n​",
    "보기": {
      "A": "Copy the data so both EBS volumes contain all the documents\n두 EBS 볼륨에 모든 문서가 포함되도록 데이터를 복사합니다 .\n​",
      "B": "Configure the Application Load Balancer to direct a user to the server with the documents\n사용자를 문서가 있는 서버로 안내하도록 Application Load Balancer 구성한다 .\n​",
      "C": "Copy the data from both EBS volumes to Amazon EFS. Modify the application to save new \ndocuments to Amazon EFS\n두 EBS 볼륨의 데이터를 Amazon EFS로 복사합니다 . Amazon EFS에 새 문서를 저장하도록 애플리케\n이션 수정 \n​\n→ EFS의 성능은 스토리지와 함께 인라인으로 확장될 수 있으며 , 갑작스러운 대용량 파일 덤프에 대해 \n더 높은 처리량으로 작동하여 500,000 IOPS 이상의 처리량과 1밀리초 미만 또는 짧은 한 자리 밀리초\n의 지연 시간을 통해 10GiBps( 초당 기비바이트 ) 이상의 처리량을 제공한다 . \nEFS는 다른 EC2 인스턴스에서 마운트할 수 있지만 VPC 피어링을 사용하여 AWS 리전 경계를 넘을 \n수도 있다.",
      "D": "Configure the Application Load Balancer to send the request to both servers. Return each document \nfrom the correct server\n두 서버 모두에 요청을 보내도록 Application Load Balancer 를 구성합니다 . 올바른 서버에서 각 문서 반\n환한다 .\n​\n​"
    },
    "정답": "C"
  },
  {
    "문제번호": 6,
    "질문": "A company uses NFS to store large video files in on-premises network attached storage. Each video \nfile ranges in size from 1 MB to 500 GB. The total storage is 70 TB and is no longer growing. The \ncompany decides to migrate the video files to Amazon S3. The company must migrate the video \nfiles as soon as possible while using the least possible network bandwidth.\nWhich solution will meet these requirements?\n회사는 NFS를 사용하여 온프레미스 네트워크 연결 스토리지에 대용량 비디오 파일을 저장합니다 . 각 \n비디오 파일의 크기 범위는 1MB에서 500GB 입니다 . 총 스토리지는 70TB이며 더 이상 증가하지 않습니\n다. 회사는 비디오 파일을 Amazon S3로 마이그레이션하기로 결정합니다 . 회사는 가능한 한 최소한의 \n네트워크 대역폭을 사용하면서 가능한 한 빨리 비디오 파일을 마이그레이션 해야 합니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n​",
    "보기": {
      "B": "Create an AWS Snowball Edge job. Receive a Snowball Edge device on premises. Use the \nSnowball Edge client to transfer data to the device. Return the device so that AWS can import the \ndata into Amazon S3.\nAWS Snowball Edge 작업을 생성합니다 . 온프레미스에서 Snowball Edge 장치를 받습니다 . Snowball \nEdge 클라이언트를 사용하여 장치로 데이터를 전송합니다 . AWS가 데이터를 Amazon S3로 가져올 수 \n있도록 디바이스를 반환합니다 .\n​\n→ AWS Snowball Edge\n: Snowball Edge Storage Optimized 디바이스를 사용하여 약 80TB까지 전송할 수 있으며 이보다 더 \n큰 용량의 데이터 세트도 여러 디바이스에 병렬 또는 순차적으로 전송할 수 있다. Snowball Edge를 사\n용해 최대 80TB의 데이터를 AWS로 전송하는 데 걸리는 종단 간 시간은 AWS 데이터 센터의 일반적인 \n운송 및 처리 시간을 포함하여 약 1주 정도 소요된다 .",
      "A": "Create an S3 bucket. Create an IAM role that has permissions to write to the S3 bucket. Use the \nAWS CLI to copy all files locally to the S3 bucket.\nS3 버킷을 생성합니다 . S3 버킷에 대한 쓰기 권한이 있는 IAM 역할을 생성합니다 . AWS CLI를 사용하\n여 모든 파일을 S3 버킷에 로컬로 복사합니다 .\n​",
      "C": "Deploy an S3 File Gateway on premises. Create a public service endpoint to connect to the S3 File \nGateway. Create an S3 bucket. Create a new NFS file share on the S3 File Gateway. Point the \nnew file share to the S3 bucket. Transfer the data \nfrom the existing NFS file share to the S3 File Gateway.\n온프레미스에 S3 파일 게이트웨이를 배포합니다 . S3 파일 게이트웨이에 연결할 퍼블릭 서비스 엔드포\n인트를 생성합니다 . S3 버킷을 생성합니다 . S3 파일 게이트웨이에서 새 NFS 파일 공유를 생성합니다 . \n새 파일 공유가 S3 버킷을 가리키도록 합니다 . 기존 NFS 파일 공유에서 S3 파일 게이트웨이로 데이터\n를 전송합니다 .\n​",
      "D": "Set up an AWS Direct Connect connection between the on-premises network and AWS. Deploy an \nS3 File Gateway on premises. Create a public virtual interface (VIF) to connect to the S3 File \nGateway. Create an S3 bucket. Create a new NFS file share on the S3 File Gateway. Point the \nnew file share to the S3 bucket. Transfer the data from the existing NFS file share to the S3 File \nGateway.\n온프레미스 네트워크와 AWS 간에 AWS Direct Connect 연결을 설정합니다 . 온프레미스에 S3 파일 게\n이트웨이를 배포합니다 . S3 파일 게이트웨이에 연결할 공용 VIF(가상 인터페이스 )를 생성합니다 . S3 버\n킷을 생성합니다 . S3 파일 게이트웨이에서 새 NFS 파일 공유를 생성합니다 . 새 파일 공유가 S3 버킷\n을 가리키도록 합니다 . 기존 NFS 파일 공유에서 S3 파일 게이트웨이로 데이터를 전송합니다 .\n​\n​"
    },
    "정답": "B"
  },
  {
    "문제번호": 7,
    "질문": "A company has an application that ingests incoming messages. Dozens of other applications and \nmicroservices then quickly consume these messages. The number of messages varies drastically and \nsometimes increases suddenly to 100,000 each second. The company wants to decouple the solution \nand increase scalability.\nWhich solution meets these requirements?\n회사에 들어오는 메시지를 수집하는 응용 프로그램이 있습니다 . 그러면 수십 개의 다른 애플리케이션과 \n마이크로서비스가 이러한 메시지를 빠르게 소비합니다 . 메시지 수는 급격하게 변하며 때로는 초당 \n100,000 개로 갑자기 증가하기도 합니다 . 이 회사는 솔루션을 분리하고 확장성을 높이고자 합니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n​",
    "보기": {
      "A": "Persist the messages to Amazon Kinesis Data Analytics. Configure the consumer applications to read \nand process the messages.\nAmazon Kinesis Data Analytics 에 대한 메시지를 유지합니다 . 메시지를 읽고 처리하도록 소비자 애플리\n케이션을 구성합니다 .\n​",
      "B": "Deploy the ingestion application on Amazon EC2 instances in an Auto Scaling group to scale the \nnumber of EC2 instances based on CPU metrics.\nAuto Scaling 그룹의 Amazon EC2 인스턴스에 수집 애플리케이션을 배포하여 CPU 지표를 기반으로 \nEC2 인스턴스 수를 확장합니다 .\n​",
      "C": "Write the messages to Amazon Kinesis Data Streams with a single shard. Use an AWS Lambda \nfunction to preprocess messages and store them in Amazon DynamoDB. Configure the consumer \napplications to read from DynamoDB to process the messages.\n단일 샤드를 사용하여 Amazon Kinesis Data Streams 에 메시지를 씁니다 . AWS Lambda 함수를 사용하\n여 메시지를 사전 처리하고 Amazon DynamoDB 에 저장합니다 . 메시지를 처리하기 위해 DynamoDB 에\n서 읽도록 소비자 애플리케이션을 구성합니다 .\n​",
      "D": "Publish the messages to an Amazon Simple Notification Service (Amazon SNS) topic with multiple \nAmazon Simple Queue Service (Amazon SOS) subscriptions. Configure the consumer applications to \nprocess the messages from the queues.\n여러 Amazon Simple Queue Service(Amazon SOS) 구독이 있는 Amazon Simple Notification \nService(Amazon SNS) 주제에 메시지를 게시합니다 . 대기열의 메시지를 처리하도록 소비자 애플리케이\n션을 구성합니다 .\n​\n→ Decoupling( 분리) 키워드 + 메세지 처리 수 = SNS , SQS\n​\n​\n▶오답\nAmazon Kinesis Data Analytics\n: Kinesis Data Analytics 를 사용하면 로그 분석, 클릭스트림 분석, 사물 인터넷 (IoT), 광고 기술, 게임 등\n을 위한 포괄적인 스트림 처리 애플리케이션을 신속하게 구축할 수 있다.\n​\nAmazon Kinesis Data Streams with a single shard( 단일 샤드를 사용하여 Amazon Kinesis Data \nStreams)\n: 단일 샤드의 경우 1MB 또는 1000개 메시지 /초로 제한된다 ."
    },
    "정답": "D"
  },
  {
    "문제번호": 8,
    "질문": "A company is migrating a distributed application to AWS. The application serves variable workloads. \nThe legacy platform consists of a primary server that coordinates jobs across multiple compute \nnodes. The company wants to modernize the application with a solution that maximizes resiliency \nand scalability.\nHow should a solutions architect design the architecture to meet these requirements?\n회사에서 분산 애플리케이션을 AWS로 마이그레이션 하고 있습니다 . 애플리케이션은 다양한 워크로드를 \n처리합니다 . *레거시 플랫폼은 여러 컴퓨팅 노드에서 작업을 조정하는 기본 서버로 구성됩니다 . 이 회\n사는 탄력성과 확장성을 극대화 하는 솔루션으로 애플리케이션을 현대화하려고 합니다 .\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 아키텍처를 어떻게 설계해야 합니까 ?\n​\n* 레거시 : 프로그래밍 언어, 플랫폼 그리고 기술 등에 있어서 과거로부터 물려 내려온 것을 의미한다 . \n​",
    "보기": {
      "A": "→ 예약된 조정을 사용하도록 EC2 Auto Scaling 을 구성하는것은 의미없다 .\n​\nC, D\n→ 기본 서버는 컴퓨팅 노드와 동일한 Auto Scaling 그룹에 있으면 안 된다. \n​\nAmazon EventBridge(Amazon CloudWatch Events)\n: 다양한 소스의 데이터와 애플리케이션을 쉽게 연결할 수 있는 서버리스 이벤트 버스 서비스이다 . \nEventBridge 는 자체 애플리케이션 , SaaS(Software-as-a-Service) 애플리케이션 및 AWS 서비스의 실시\n간 데이터 스트림을 제공한 다음, 해당 데이터를 AWS Lambda 등의 대상으로 라우팅한다 . 데이터를 전\n송할 대상을 결정하는 라우팅 규칙을 설정하여 모든 데이터 원본에 실시간으로 대응하는 애플리케이션 \n아키텍처를 구축할 수 있다. \nEventBridge 를 사용하면 느슨하게 결합되고 분산된 이벤트 중심 아키텍처를 구축할 수 있다.",
      "B": "Configure an Amazon Simple Queue Service (Amazon SQS) queue as a destination for the jobs. \nImplement the compute nodes with Amazon EC2 instances that are managed in an Auto Scaling \ngroup. Configure EC2 Auto Scaling based on the size of the queue.\n작업의 대상으로 Amazon Simple Queue Service(Amazon SQS) 대기열을 구성합니다 . Auto Scaling 그\n룹에서 관리되는 Amazon EC2 인스턴스로 컴퓨팅 노드를 구현합니다 . 대기열 크기에 따라 EC2 Auto \nScaling 을 구성합니다 .\n​\n​\nAmazon SQS (Amazon Simple Queue Service)\n: 마이크로 서비스 , 분산 시스템 및 서버리스 애플리케이션을 쉽게 분리하고 확장할 수 있도록 지원하\n는 완전관리형 메시지 대기열 서비스이다 . SQS는 메시지 중심 미들웨어를 관리하고 운영하는 데 따른 \n복잡성과 오버헤드를 없애고 개발자가 차별화 작업에 집중할 수 있도록 지원한다 . Amazon SQS는 \nAWS를 활용하여 필요에 따라 동적으로 확장됩니다 . SQS는 애플리케이션에 따라 탄력적으로 확장됩니\n다. 따라서 용량 계획과 사전 프리프로비저닝에 대해 걱정할 필요가 없습니다 . 사용할 수 있는 대기열\n당 메시지 수에 제한이 없으며 , 표준 대기열은 거의 무제한의 처리량을 제공합니다 .\n​\n▶오답",
      "C": "Implement the primary server and the compute nodes with Amazon EC2 instances that are managed \nin an Auto Scaling group. Configure AWS CloudTrail as a destination for the jobs. Configure EC2 \nAuto Scaling based on the load on the primary server.\nAuto Scaling 그룹에서 관리되는 Amazon EC2 인스턴스로 기본 서버와 컴퓨팅 노드를 구현합니다 . 작\n업의 대상으로 AWS CloudTrail 을 구성합니다 . 기본 서버의 부하를 기반으로 EC2 Auto Scaling 을 구성\n합니다 .\n​",
      "D": "Implement the primary server and the compute nodes with Amazon EC2 instances that are managed \nin an Auto Scaling group. Configure Amazon EventBridge (Amazon CloudWatch Events) as a \ndestination for the jobs. Configure EC2 Auto Scaling based on the load on the compute nodes.\nAuto Scaling 그룹에서 관리되는 Amazon EC2 인스턴스로 기본 서버와 컴퓨팅 노드를 구현합니다 . 작\n업의 대상으로 Amazon EventBridge(Amazon CloudWatch Events) 를 구성합니다 . 컴퓨팅 노드의 부하를 \n기반으로 EC2 Auto Scaling 을 구성합니다 .\n​\n​"
    },
    "정답": "B"
  },
  {
    "문제번호": 9,
    "질문": "A company is running an SMB file server in its data center. The file server stores large files that \nare accessed frequently for the first few days after the files are created. After 7 days the files are \nrarely accessed.\nThe total data size is increasing and is close to the company's total storage capacity. A solutions \narchitect must increase the company's available storage space without losing low-latency access to \nthe most recently accessed files. The solutions architect must also provide file lifecycle management \nto avoid future storage issues.\nWhich solution will meet these requirements?\n회사는 데이터 센터에서 SMB 파일 서버를 실행하고 있습니다 . 파일 서버는 파일이 생성된 후 처음 며\n칠 동안 자주 액세스하는 대용량 파일을 저장합니다 . 7일이 지나면 파일에 거의 액세스하지 않습니다 .\n총 데이터 크기가 증가하고 있으며 회사의 총 저장 용량에 가깝습니다 . 솔루션 설계자는 가장 최근에 \n액세스한 파일에 대한 저지연 액세스를 잃지 않으면서 회사의 사용 가능한 저장 공간을 늘려야 합니다 . \n솔루션 설계자는 향후 스토리지 문제를 방지하기 위해파일 수명 주기 관리도 제공해야 합니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n​",
    "보기": {
      "A": "Use AWS DataSync to copy data that is older than 7 days from the SMB file server to AWS.\nAWS DataSync 를 사용하여 SMB 파일 서버에서 AWS로 7일이 지난 데이터를 복사합니다 .\n​",
      "B": "Create an Amazon S3 File Gateway to extend the company's storage space. Create an S3 Lifecycle \npolicy to transition the data to S3 Glacier Deep Archive after 7 days.\nAmazon S3 파일 게이트웨이를 생성하여 회사의 스토리지 공간을 확장합니다 . S3 수명 주기 정책을 생\n성하여 7일 후에 데이터를 S3 Glacier Deep Archive 로 전환합니다 .\n​\n→ Amazon S3 File Gateway\n: 업계 표준 파일 프로토콜을 사용하여 파일을 원활하게 Amazon S3에 객체로 저장하고 이에 액세스할 \n수 있는 파일 인터페이스를 애플리케이션에 제공하는 AWS Storage Gateway 서비스 구성이다 .\n네트워크 파일 시스템 (NFS) 및 서버 메시지 블록(SMB) 과 같은 파일 프로토콜을 사용하여 Amazon \nSimple Storage Service(S3) 에서 객체를 저장하고 검색할 수 있습니다 . S3 File Gateway 를 통해 작성된 \n객체는 S3에서 직접 액세스할 수 있습니다 .\n게이트웨이가 생성하는 객체에 대한 초기 스토리지 클래스를 구성한 다음, 버킷 수명 주기 정책을 사용\n하여 Amazon S3에서 Amazon S3 Glacier 로 파일을 이동할 수 있습니다 . \n​\n사용 사례\n1.최근에 액세스한 데이터에 대해 빠른 로컬 액세스를 유지하면서 온프레미스 파일 데이터를 \nAmazon S3로 마이그레이션\n2.온프레미스 파일 데이터를 Amazon S3에 객체로 백업하고 (Microsoft SQL Server 와 Oracle 데\n이터베이스 및 로그 포함) 수명 주기 관리 및 교차 리전 복제와 같은 S3 기능을 사용\n3.기계 학습, 빅 데이터 분석 또는 서버리스 함수와 같은 AWS 서비스로 처리하기 위해 온프레\n미스 애플리케이션에서 생성한 데이터를 사용하는 하이브리드 클라우드 워크플로\n​\nAWS Storage Gateway\n: 사실상 무제한의 클라우드 스토리지에 대한 온프레미스 액세스 권한을 제공하는 하이브리드 클라우드 \n스토리지 서비스 . Storage Gateway 는 iSCSI, SMB 및 NFS와 같은 표준 스토리지 프로토콜 세트를 제\n공하므로 기존 애플리케이션을 다시 작성하지 않고 AWS 스토리지를 사용할 수 있다. \n​\n사용 사례\n1.백업 및 아카이브를 클라우드로 이동\n2.클라우드 기반 파일 공유를 통한 온프레미스 스토리지 감소\n3.프레미스 애플리케이션에 AWS에 저장된 데이터에 대한 짧은 대기 시간의 액세스 제공\n4.처리 전/후 워크플로에 대한 데이터 레이크 액세스인 네 가지 주요 하이브리드 클라우드 사용 \n사례를 지원",
      "C": "Create an Amazon FSx for Windows File Server file system to extend the company's storage space.\nWindows 파일 서버용 Amazon FSx 파일 시스템을 생성하여 회사의 저장 공간을 확장합니다 .\n​",
      "D": "Install a utility on each user's computer to access Amazon S3. Create an S3 Lifecycle policy to \ntransition the data to S3 Glacier Flexible Retrieval after 7 days.\n각 사용자의 컴퓨터에 유틸리티를 설치하여 Amazon S3에 액세스합니다 . S3 수명 주기 정책을 생성하\n여 7일 후 데이터를 S3 Glacier Flexible Retrieval 로 전환합니다 .\n​\n​"
    },
    "정답": "B"
  },
  {
    "문제번호": 10,
    "질문": "A company is building an ecommerce web application on AWS. The application sends information \nabout new orders to an Amazon API Gateway REST API to process. The company wants to ensure \nthat orders are processed in the order that they are received.\nWhich solution will meet these requirements?\n회사는 AWS에서 전자 상거래 웹 애플리케이션을 구축하고 있습니다 . 애플리케이션은 처리할 Amazon \nAPI Gateway REST API에 새 주문에 대한 정보를 보냅니다 . 회사는 주문이 접수된 순서대로 처리되기\n를 원합니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n​",
    "보기": {
      "A": "Use an API Gateway integration to publish a message to an Amazon Simple Notification Service \n(Amazon SNS) topic when the application receives an order. Subscribe an AWS Lambda function to \nthe topic to perform processing.\nAPI Gateway 통합을 사용하여 애플리케이션이 주문을 수신할 때 Amazon Simple Notification \nService(Amazon SNS) 주제에 메시지를 게시합니다 . AWS Lambda 함수를 주제에 구독하여 처리를 수\n행합니다 .\n​",
      "B": "Use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon \nSQS) FIFO queue when the application receives an order. Configure the SQS FIFO queue to invoke \nan AWS Lambda function for processing.\nAPI Gateway 통합을 사용하여 애플리케이션이 주문을 수신할 때 Amazon Simple Queue \nService(Amazon SQS) FIFO 대기열에 메시지를 보냅니다 . 처리를 위해 AWS Lambda 함수를 호출하도\n록 SQS FIFO 대기열을 구성합니다 .\n​\nAmazon SQS FIFO / 표준 대기열의 차이 및 장단점 ​\nAmazon SQS Standard Queue (표준 대기열 ) FIFO 대기열 (First in First Out Queue)\n장점\n•무제한에 가까운 메시지 전송 지원 (최대 \n처리량 ), 제한이 없는 TPS\n•최소 1회 전달 보장 \n(At-Least-Once-Delivery), 단 중복 수신\n이 될 수 있다.\n•Best-Effort-Ordering: 최대한 순서를 보장\n하고자 노력한다 . (하지만 신뢰할 수 없\n다.)장점\n•메시지 순서 보장 (First-In-First-Out \nDelivery)\n•Exactly-Once Processing: 1번의 전\n송, 1번의 수신 지켜짐 (중복수신 방\n지)\n•Limited Throughput: 초당 300TPS 제\n한 존재\n단점\n•메시지 순서 보장 안됨\n•반드시 1번만 읽기 보장 안됨 (중복 읽\n기 가능성 존재)단점\n•순서를 위해 느린 퍼포먼스 (초당 \n300TPS)",
      "C": "Use an API Gateway authorizer to block any requests while the application processes an order.\nAPI Gateway 권한 부여자를 사용하여 애플리케이션이 주문을 처리하는 동안 모든 요청을 차단합니다 .\n​",
      "D": "Use an API Gateway integration to send a message to an Amazon Simple Queue Service (Amazon \nSQS) standard queue when the application receives an order. Configure the SQS standard queue to \ninvoke an AWS Lambda function for processing.\nAPI Gateway 통합을 사용하여 애플리케이션이 주문을 수신할 때 Amazon Simple Queue \nService(Amazon SQS) 표준 대기열에 메시지를 보냅니다 . 처리를 위해 AWS Lambda 함수를 호출하도\n록 SQS 표준 대기열을 구성합니다 .\n​\n​"
    },
    "정답": "B"
  },
  {
    "문제번호": 11,
    "질문": "A company has an application that runs on Amazon EC2 instances and uses an Amazon Aurora \ndatabase. The EC2 instances connect to the database by using user names and passwords that are \nstored locally in a file. The company wants to minimize the operational overhead of credential \nmanagement.\nWhat should a solutions architect do to accomplish this goal?\n회사에 Amazon EC2 인스턴스에서 실행되고 Amazon Aurora 데이터베이스를 사용하는 애플리케이션이 \n있습니다 . EC2 인스턴스는 파일에 로컬로 저장된 사용자 이름과 암호를 사용하여 데이터베이스에 연결\n합니다 . 회사는 자격 증명 관리의 운영 오버헤드를 최소화 하려고 합니다 .\n솔루션 설계자는 이 목표를 달성하기 위해 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Use AWS Secrets Manager. Turn on automatic rotation.\nAWS Secrets Manager 를 사용합니다 . 자동 교체를 켭니다 .\n​\nAWS Secrets Manager 이란\n: 애플리케이션 , 서비스 및 IT 리소스에 대한 액세스를 보호하는 데 도움이 되는 보안 정보 관리 서비\n스이다 . 수명 주기 동안 데이터베이스 자격 증명, API 키 및 기타 보안 정보를 손쉽게 교체, 관리 및 검\n색할 수 있다. \n​\nAWS Secrets Manager 의 자격 증명 교체 기능\nAWS Secrets Manager 를 사용하면 예약된 일정에 따라 데이터베이스 자격 증명을 교체하도록 구성할 \n수 있다. 따라서 보안 모범 사례를 준수하고 안전하게 데이터베이스 자격 증명을 교체할 수 있다. \nAmazon RDS에서 MySQL, PostgreSQL 및 Amazon Aurora 에 대한 내장 통합을 제공하며 이러한 데이\n터베이스에 대한 자격 증명을 기본적으로 교체할 수 있다. \n​\n▶오답\nAWS Systems Manager Parameter Store이란?\n: AWS Systems Manager 의 기능인 Parameter Store는 구성 데이터 관리 및 암호 관리를 위한 안전한 \n계층적 스토리지를 제공한다 . 암호, 데이터베이스 문자열 , Amazon Machine Image(AMI) ID, 라이선스 \n코드와 같은 데이터를 파라미터 값으로 저장할 수 있다.\n•안전하고 확장 가능한 호스팅 방식 암호 관리 서비스를 사용한다 (관리할 서버가 없음).\n•데이터를 코드와 격리하여 보안 태세를 개선한다 .\n•구성 데이터 및 암호화된 문자열을 계층으로 저장하고 버전을 추적한다 .\n•세분화된 수준에서 액세스를 제어하고 감사한다 .\n•Parameter Store는 AWS 리전의 여러 가용 영역에서 호스팅되기 때문에 파라미터를 안정적으\n로 저장한다 .",
      "B": "Use AWS Systems Manager Parameter Store. Turn on automatic rotation.\nAWS Systems Manager Parameter Store를 사용합니다 . 자동 교체를 켭니다 .\n​",
      "C": "Create an Amazon S3 bucket to store objects that are encrypted with an AWS Key Management \nService (AWS KMS) encryption key. Migrate the credential file to the S3 bucket. Point the application \nto the S3 bucket.\nAWS Key Management Service(AWS KMS) 암호화 키로 암호화된 객체를 저장할 Amazon S3 버킷을 \n생성합니다 . 자격 증명 파일을 S3 버킷으로 마이그레이션합니다 . 애플리케이션이 S3 버킷을 가리키도\n록 합니다 .\n​",
      "D": "Create an encrypted Amazon Elastic Block Store (Amazon EBS) volume for each EC2 instance. \nAttach the new EBS volume to each EC2 instance. Migrate the credential file to the new EBS \nvolume. Point the application to the new EBS volume.\n각 EC2 인스턴스에 대해 암호화된 Amazon Elastic Block Store(Amazon EBS) 볼륨을 생성합니다 . 새 \nEBS 볼륨을 각 EC2 인스턴스에 연결합니다 . 자격 증명 파일을 새 EBS 볼륨으로 마이그레이션합니다 . \n애플리케이션이 새 EBS 볼륨을 가리키도록 합니다 .\n​\n​"
    },
    "정답": "A"
  },
  {
    "문제번호": 12,
    "질문": "A global company hosts its web application on Amazon EC2 instances behind an Application Load \nBalancer (ALB). The web application has static data and dynamic data. The company stores its static \ndata in an Amazon S3 bucket. The company wants to improve performance and reduce latency for \nthe static data and dynamic data. The company is using its own domain name registered with \nAmazon Route 53.\nWhat should a solutions architect do to meet these requirements?\n글로벌 회사는 ALB(Application Load Balancer) 뒤의 Amazon EC2 인스턴스에서 웹 애플리케이션을 호\n스팅합니다 . 웹 애플리케이션에는 정적 데이터 와 동적 데이터 가 있습니다 . 회사는 정적 데이터를 \nAmazon S3 버킷에 저장합니다 . 회사는 정적 데이터 및 동적 데이터의 성능을 개선하고 대기 시간을 \n줄이기를 원합니다 . 회사는 Amazon Route 53에 등록된 자체 도메인 이름을 사용하고 있습니다 .\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Create an Amazon CloudFront distribution that has the S3 bucket and the ALB as origins. Configure \nRoute 53 to route traffic to the CloudFront distribution.\nS3 버킷과 ALB를 오리진으로 포함하는 Amazon CloudFront 배포를 생성합니다 . CloudFront 배포로 트\n래픽을 라우팅하도록 Route 53을 구성합니다 .\n​\n여러 오리진에서 다양한 유형의 요청을 제공하도록 단일 CloudFront 웹 배포를 구성할 수 있다. \n예를 들어 웹 사이트에서 Amazon Simple Storage Service(Amazon S3) 버킷의 정적 콘텐츠와 로드 밸\n런서의 동적 콘텐츠를 제공할 수 있다. \nCloudFront 웹 배포에서 두 가지 유형의 콘텐츠를 모두 제공할 수 있으며 다음 단계에 따라 S3 버킷의 \n정적 콘텐츠와 로드 밸런서의 동적 콘텐츠를 제공하도록 CloudFront 웹 배포를 구성한다 .\n​\n배포를 만들 때 CloudFront 가 파일에 대한 요청을 보내는 원본을 지정한다 . \nCloudFront 에서 여러 원본을 사용할 수 있습니다 . 예를 들어 Amazon S3 버킷, MediaStore 컨테이너 , \nMediaPackage 채널, Application Load Balancer 또는 AWS Lambda 함수 URL을 사용할 수 있습니다 .\n원본이 하나 이상의 Amazon EC2 인스턴스에서 호스트되는 하나 이상의 HTTP 서버(웹 서버)인 경우 \nApplication Load Balancer 를 사용하여 인스턴스에 트래픽을 분산할 수 있습니다 . \n​\n​\n▶오답\nAWS Global Accelerator 의 표준 액셀러레이터용 엔드포인트​: AWS Global Accelerator 의 표준 액셀러레이터에 대한 엔드포인트는 Network Load Balancer, \nApplication Load Balancer, Amazon EC2 인스턴스 또는 Elastic IP 주소일 수 있습니다 . 표준 액셀러레\n이터를 사용하면 고정 IP 주소가 클라이언트의 단일 접점 역할을 하고 Global Accelerator 는 들어오는 \n트래픽을 정상적인 엔드포인트에 분산시킵니다 . Global Accelerator 는 끝점의 끝점 그룹이 속한 수신기\n에 대해 지정한 포트(또는 포트 범위)를 사용하여 끝점으로 트래픽을 보냅니다 .\n→ AWS Accelerator 의 엔드포인트로 CloudFront 나 S3가 될 수 없다.",
      "B": "Create an Amazon CloudFront distribution that has the ALB as an origin. Create an AWS Global \nAccelerator standard accelerator that has the S3 bucket as an endpoint Configure Route 53 to route \ntraffic to the CloudFront distribution.\nALB가 오리진인 Amazon CloudFront 배포를 생성합니다 . S3 버킷을 엔드포인트로 포함하는 AWS \nGlobal Accelerator 표준 액셀러레이터를 생성합니다 . CloudFront 배포로 트래픽을 라우팅하도록 Route \n53을 구성합니다 .\n​",
      "C": "Create an Amazon CloudFront distribution that has the S3 bucket as an origin. Create an AWS \nGlobal Accelerator standard accelerator that has the ALB and the CloudFront distribution as \nendpoints. Create a custom domain name that points to the accelerator DNS name. Use the custom \ndomain name as an endpoint for the web application.\nS3 버킷을 오리진으로 포함하는 Amazon CloudFront 배포를 생성합니다 . ALB 및 CloudFront 배포를 엔\n드포인트로 포함하는 AWS Global Accelerator 표준 액셀러레이터를 생성합니다 . 가속기 DNS 이름을 \n가리키는 사용자 지정 도메인 이름을 만듭니다 . 사용자 지정 도메인 이름을 웹 애플리케이션의 끝점으\n로 사용합니다 .\n​",
      "D": "Create an Amazon CloudFront distribution that has the ALB as an origin. Create an AWS Global \nAccelerator standard accelerator that has the S3 bucket as an endpoint. Create two domain names. \nPoint one domain name to the CloudFront DNS name for dynamic content. Point the other domain \nname to the accelerator DNS name for static content. Use the domain names as endpoints for the \nweb application.\nALB가 오리진인 Amazon CloudFront 배포를 생성합니다 . S3 버킷을 엔드포인트로 포함하는 AWS \nGlobal Accelerator 표준 액셀러레이터를 생성합니다 . 두 개의 도메인 이름을 만듭니다 . 하나의 도메인 \n이름이 동적 콘텐츠의 CloudFront DNS 이름을 가리키도록 합니다 . 다른 도메인 이름이 정적 콘텐츠에 \n대한 가속기 DNS 이름을 가리키도록 합니다 . 도메인 이름을 웹 애플리케이션의 끝점으로 사용합니다 .\n​\n​"
    },
    "정답": "A"
  },
  {
    "문제번호": 13,
    "질문": "A company performs monthly maintenance on its AWS infrastructure. During these maintenance \nactivities, the company needs to rotate the credentials for its Amazon RDS for MySQL databases \nacross multiple AWS Regions.\nWhich solution will meet these requirements with the LEAST operational overhead?\n회사는 AWS 인프라에 대한 월별 유지 관리를 수행합니다 . 이러한 유지 관리 활동 중에 회사는 여러 \nAWS 리전에서 MySQL 용 Amazon RDS 데이터베이스에 대한 자격 증명을 교체해야 합니다 .\n최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까 ?\n​",
    "보기": {
      "A": "Store the credentials as secrets in AWS Secrets Manager. Use multi-Region secret replication for the \nrequired Regions. Configure Secrets Manager to rotate the secrets on a schedule.\n자격 증명을 AWS Secrets Manager 에 암호로 저장합니다 . 필요한 리전에 대해 다중 리전 비밀 복제를 \n사용합니다 . 일정에 따라 보안 암호를 교체하도록 Secrets Manager 를 구성합니다 .\n​\nAWS Secrets Manager 의 암호를 여러 리전에 복제하는 방법\n: AWS Secrets Manager 의 새로운 기능으로 여러 AWS리전에서 암호를 복제할 수 있다 . 다중 리전 애\n플리케이션에 필수 리전의 복제된 암호에 대한 액세스 권한을 부여하고 Secret Manager 를 사용하여 \n복제본이 기본 암호와 동기화된 상태를 유지할 수 있다. \n이 기능은 리전 간 암호 복제의 복잡성을 추상화하여 고객이 Secret Manager 를 활용하여 다중 리전 애\n플리케이션 및 재해 복구 전략을 지원하는 데 필요한 암호를 쉽게 관리할 수 있도록 한다. \nAWS Secrets Manager 의 기능\n런타임에 암호화된 비밀 값을 \n프로그래밍 방식으로 검색\nSecrets Manager 는 애플리케이\n션 소스 코드에서 하드 코딩된 \n자격 증명을 제거하고 어떤 방\n식으로든 애플리케이션 내에 \n자격 증명을 저장하지 않음으\n로써 보안 태세를 개선하는 데 \n도움이 됩니다 .다양한 유형의 비밀 저장\nSecrets Manager 를 사용하면 \n암호의 암호화된 암호 데이터 \n부분에 텍스트를 저장할 수 있\n습니다 . 여기에는 일반적으로 \n데이터베이스 또는 서비스의 \n연결 세부 정보가 포함됩니다 .비밀 데이터 암호화\nSecrets Manager 는 AWS Key \nManagement Service(AWS \nKMS) 를 사용하여 보안 암호의 \n보호된 텍스트를 암호화합니다 .\n자동으로 교체\n사용자 개입 없이 지정된 일정\n에 따라 보안 암호를 자동으로 \n교체하도록 Secrets Manager 를 \n구성할 수 있습니다 .완전히 구성되고 바로 사용할 \n수 있는 \n순환 지원이 있는 데이터베이\n스\n교체를 활성화하도록 선택하면 \nSecrets Manager 는 AWS에서 \n작성 및 테스트한 Lambda 교\n체 함수 템플릿과 교체 프로세\n스의 전체 구성을 사용하여 다\n음 RDS 데이터베이스를 지원\n합니다 .완전히 구성되고 바로 사용할 \n수 있는 \n순환 지원이 있는 기타 서비\n스\nAWS에서 작성 및 테스트한 \nLambda 교체 함수 템플릿과 \n교체 프로세스의 전체 구성으로 \n완전히 지원되는 다음 서비스에\n서 교체를 활성화하도록 선택할 \n수 있습니다 .",
      "B": "Store the credentials as secrets in AWS Systems Manager by creating a secure string parameter. \nUse multi-Region secret replication for the required Regions. Configure Systems Manager to rotate \nthe secrets on a schedule.\n보안 문자열 파라미터를 생성하여 AWS Systems Manager 에 자격 증명을 보안 암호로 저장합니다 . 필\n요한 리전에 대해 다중 리전 비밀 복제를 사용합니다 . 일정에 따라 암호를 교체하도록 Systems \nManager 를 구성합니다 .\n​",
      "C": "Store the credentials in an Amazon S3 bucket that has server-side encryption (SSE) enabled. Use \nAmazon EventBridge (Amazon CloudWatch Events) to invoke an AWS Lambda function to rotate the \ncredentials.\n서버 측 암호화 (SSE) 가 활성화된 Amazon S3 버킷에 자격 증명을 저장합니다 . Amazon \nEventBridge(Amazon CloudWatch Events) 를 사용하여 AWS Lambda 함수를 호출하여 자격 증명을 교\n체합니다 .\n​",
      "D": "Encrypt the credentials as secrets by using AWS Key Management Service (AWS KMS) multi-Region \ncustomer managed keys. Store the secrets in an Amazon DynamoDB global table. Use an AWS \nLambda function to retrieve the secrets from DynamoDB. Use the RDS API to rotate the secrets.\nAWS Key Management Service(AWS KMS) 다중 리전 고객 관리형 키를 사용하여 자격 증명을 비밀로 \n암호화합니다 . Amazon DynamoDB 전역 테이블에 암호를 저장합니다 . AWS Lambda 함수를 사용하여 \nDynamoDB 에서 암호를 검색합니다 . RDS API를 사용하여 비밀을 교체합니다 .\n​\n​"
    },
    "정답": "A"
  },
  {
    "문제번호": 14,
    "질문": "A company runs an ecommerce application on Amazon EC2 instances behind an Application Load \nBalancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability \nZones. The Auto Scaling group scales based on CPU utilization metrics. The ecommerce application \nstores the transaction data in a MySQL 8.0 database that is hosted on a large EC2 instance.\nThe database's performance degrades quickly as application load increases. The application handles \nmore read requests than write transactions. The company wants a solution that will automatically \nscale the database to meet the demand of unpredictable read workloads while maintaining high \navailability.\nWhich solution will meet these requirements?\n회사는 Application Load Balancer 뒤의 Amazon EC2 인스턴스에서 전자 상거래 애플리케이션을 실행\n합니다 . 인스턴스는 여러 가용 영역에 걸쳐 Amazon EC2 Auto Scaling 그룹에서 실행됩니다 . Auto \nScaling 그룹은 CPU 사용률 메트릭을 기반으로 확장됩니다 . 전자 상거래 애플리케이션은 대규모 EC2 \n인스턴스에서 호스팅되는 MySQL 8.0 데이터베이스에 트랜잭션 데이터를 저장합니다 .\n애플리케이션 로드가 증가하면 데이터베이스의 성능이 빠르게 저하됩니다 . 애플리케이션은 쓰기 트랜잭\n션보다 더 많은 읽기 요청을 처리합니다 . 이 회사는 고가용성을 유지하면서 예측할 수 없는 읽기 워크\n로드의 수요를 충족하도록 데이터베이스를 자동으로 확장하는 솔루션 을 원합니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n​",
    "보기": {
      "A": "Use Amazon Redshift with a single node for leader and compute functionality.\n리더 및 컴퓨팅 기능을 위해 단일 노드와 함께 Amazon Redshift 를 사용하십시오 .\n​",
      "B": "Use Amazon RDS with a Single-AZ deployment Configure Amazon RDS to add reader instances in \na different Availability Zone.\n단일 AZ 배포와 함께 Amazon RDS 사용 다른 가용 영역에 리더 인스턴스를 추가하도록 Amazon RDS\n를 구성합니다 .비밀에 대한 액세스 제어\nIAM 권한 정책을 특정 암호에 \n대한 액세스 권한을 부여하거\n나 거부하는 사용자 , 그룹 및 \n역할에 연결하고 해당 암호의 \n관리를 제한할 수 있습니다 . 예\n를 들어 비밀을 완전히 관리하\n고 구성하는 기능이 필요한 구\n성원이 있는 그룹에 하나의 정\n책을 연결할 수 있습니다 .​ ​\n​",
      "C": "Use Amazon Aurora with a Multi-AZ deployment. Configure Aurora Auto Scaling with Aurora \nReplicas.\n다중 AZ 배포와 함께 Amazon Aurora 를 사용합니다 . Aurora 복제본을 사용하여 Aurora Auto Scaling 을 \n구성합니다 .\n​\nAmazon Aurora\n: Amazon Aurora(Aurora) 는 MySQL 및 PostgreSQL 과 호환되는 완전 관리형 관계형 데이터베이스 엔진\n이다. Aurora 은 기존 애플리케이션을 거의 변경하지 않고도 MySQL 의 처리량을 최대 5배, PostgreSQL\n의 처리량을 최대 3배 제공할 수 있다.\n기본 스토리지는 필요에 따라 자동으로 커지며 , Aurora 클러스터 볼륨 크기는 최대 128 tebibytes (TiB)\n까지 증가할 수 있다.",
      "D": "Use Amazon ElastiCache for Memcached with EC2 Spot Instances.\nEC2 스팟 인스턴스와 함께 Memcached 용 Amazon ElastiCache 를 사용합니다 .\n​\n​"
    },
    "정답": "C"
  },
  {
    "문제번호": 15,
    "질문": "A company recently migrated to AWS and wants to implement a solution to protect the traffic that \nflows in and out of the production VPC. The company had an inspection server in its on-premises \ndata center. The inspection server performed specific operations such as traffic flow inspection and \ntraffic filtering. The company wants to have the same functionalities in the AWS Cloud.\nWhich solution will meet these requirements?\n최근에 AWS로 마이그레이션한 회사가 프로덕션 VPC로 들어오고 나가는 트래픽을 보호하는 솔루션 을 \n구현하려고 합니다 . 이 회사는 사내 데이터 센터에 검사 서버를 가지고 있었습니다 . 검사 서버는 트래\n픽 흐름 검사 및 트래픽 필터링과 같은 특정 작업을 수행했습니다 . 회사는 AWS 클라우드에서 동일한 \n기능을 갖기를 원합니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n​",
    "보기": {
      "C": "Use AWS Network Firewall to create the required rules for traffic inspection and traffic filtering for \nthe production VPC.\nAWS 네트워크 방화벽을 사용하여 프로덕션 VPC에 대한 트래픽 검사 및 트래픽 필터링에 필요한 규칙\n을 생성합니다 .\n​\nAWS Network Firewall\n: Amazon Virtual Private Cloud(Amazon VPC)에서 생성한 Virtual Private Cloud(VPC) 를 위한 상태 저\n장 관리형 네트워크 방화벽 및 침입 탐지 및 방지 서비스이다 . 네트워크 방화벽을 사용하면 VPC 경계\n에서 트래픽을 필터링할 수 있습니다 . 여기에는 인터넷 게이트웨이 , NAT 게이트웨이 또는 VPN 또는 \nAWS Direct Connect 를 통해 들어오고 나가는 트래픽 필터링이 포함된다 .\n​\n​\n▶오답\n트래픽 미러링 작동 방식\n: 트래픽 미러링은 인스턴스에 연결된 네트워크 인터페이스의 인바운드 및 아웃바운드 트래픽을 복사한\n다. 미러링된 트래픽을 다른 인스턴스의 네트워크 인터페이스 , UDP 리스너가 있는 Network Load \nBalancer 또는 UDP 리스너가 있는 게이트웨이 로드 밸런서로 보낼 수 있다. \n​",
      "A": "Use Amazon GuardDuty for traffic inspection and traffic filtering in the production VPC.\n프로덕션 VPC에서 트래픽 검사 및 트래픽 필터링에 Amazon GuardDuty 를 사용합니다 .\n​",
      "B": "Use Traffic Mirroring to mirror traffic from the production VPC for traffic inspection and filtering.\n트래픽 미러링을 사용하여 트래픽 검사 및 필터링을 위해 프로덕션 VPC의 트래픽을 미러링합니다 .\n​",
      "D": "Use AWS Firewall Manager to create the required rules for traffic inspection and traffic filtering for \nthe production VPC.\nAWS Firewall Manager 를 사용하여 프로덕션 VPC에 대한 트래픽 검사 및 트래픽 필터링에 필요한 규\n칙을 생성합니다 .\n​\n​"
    },
    "정답": "C"
  },
  {
    "문제번호": 16,
    "질문": "A company hosts a data lake on AWS. The data lake consists of data in Amazon S3 and Amazon \nRDS for PostgreSQL. The company needs a reporting solution that provides data visualization and \nincludes all the data sources within the data lake. Only the company's management team should \nhave full access to all the visualizations. The rest of the company should have only limited access.\nWhich solution will meet these requirements?\n회사는 AWS에서 데이터 레이크를 호스팅합니다 . 데이터 레이크는 Amazon S3 및 PostgreSQL 용 \nAmazon RDS의 데이터로 구성됩니다 . 이 회사는 데이터 시각화를 제공하고 데이터 레이크 내의 모든 \n데이터 소스를 포함하는 보고 솔루션 이 필요합니다 . 회사의 관리 팀만 모든 시각화에 대한 전체 액세스 \n권한을 가져야 합니다 . 나머지 회사는 제한된 액세스 권한만 가져야 합니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n​",
    "보기": {
      "A": "Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. \nPublish dashboards to visualize the data. Share the dashboards with the appropriate IAM roles.\nAmazon QuickSight 에서 분석을 생성합니다 . 모든 데이터 소스를 연결하고 새 데이터 세트를 만듭니다 . \n대시보드를 게시하여 데이터를 시각화합니다 . 적절한 IAM 역할과 대시보드를 공유합니다 .\n​",
      "B": "Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. \nPublish dashboards to visualize the data. Share the dashboards with the appropriate users and \ngroups.\nAmazon QuickSight 에서 분석을 생성합니다 . 모든 데이터 소스를 연결하고 새 데이터 세트를 만듭니다 . \n대시보드를 게시하여 데이터를 시각화합니다 . 적절한 사용자 및 그룹과 대시보드를 공유합니다 .\n​\nAmazon QuickSight\n•조직 내 모든 구성원에게 통찰력을 손쉽게 제공할 수 있는 빠른 클라우드 기반 데이터 시각화 \n도구이다 .\n•어떠한 데이터를 사용할지에 대해 데이터셋을 설정해주면 간단히 데이터를 통해서 차트나 그\n래프 형태로 데이터를 시각화하고 분석한다 .\n•비용 측면에서 세션별 지불 결제 모델을 사용하여 사용량에 대해서만 요금을 지불하면 된다.\n​\n​\n개별 Amazon QuickSight 사용자 및 그룹에게 Amazon QuickSight 의 대시보드에 대한 액세스 권한 부\n여 방법\n1.게시된 대시보드의 오른쪽 상단에서 대시보드 공유를 선택한다 .\n2.사용자 및 그룹 초대의 경우 검색 상자에 사용자 이메일 또는 그룹 이름을 입력한다 .\n3.대시보드에 대한 액세스 권한을 부여하려는 사용자 또는 그룹에 대한 추가를 선택하고 , 권한 \n수준을 선택한다\n​\n▶오답\nAWS Glue\n: 분석 사용자가 여러 소스의 데이터를 쉽게 검색, 준비, 이동, 통합할 수 있도록 하는 서버리스 데이터 \n통합 서비스이다 .\nAWS Glue를 사용하면 70개 이상의 다양한 데이터 소스를 검색하여 연결하고 중앙 집중식 데이터 카\n탈로그에서 데이터를 관리할 수 있다. \n추출, 변환, 로드(ETL) 파이프라인을 시각적으로 생성, 실행, 모니터링하여 데이터 레이크에 데이터를 \n로드할 수 있으며 , \nAmazon Athena, Amazon EMR, Amazon Redshift Spectrum 을 사용하여 카탈로그화된 데이터를 즉시 \n검색하고 쿼리할 수 있다.\n​\nAWS Glue 기능\n•데이터 검색 및 구성\n•분석을 위한 데이터 변환, 준비, 정리\n•데이터 파이프라인 구축 및 모니터링\n​",
      "C": "Create an AWS Glue table and crawler for the data in Amazon S3. Create an AWS Glue extract, \ntransform, and load (ETL) job to produce reports. Publish the reports to Amazon S3. Use S3 bucket \npolicies to limit access to the reports.\nAmazon S3의 데이터에 대한 AWS Glue 테이블 및 *크롤러를 생성합니다 . AWS Glue 추출, 변환 및 \n로드(ETL) 작업을 생성하여 보고서를 생성합니다 . 보고서를 Amazon S3에 게시합니다 . S3 버킷 정책을 \n사용하여 보고서에 대한 액세스를 제한합니다 .\n* 크롤러 \n: 크롤러란 Amazon S3 및 관계형 데이터베이스의 데이터를 스캔해 스키마를 추출하고 AWS Glue \nData 카탈로그 (여기에서 메타데이터를 최신 상태로 유지함 )를 자동으로 채워 데이터 집합 검색을 간소\n화해주는 기능이다 .\n​\n​",
      "D": "Create an AWS Glue table and crawler for the data in Amazon S3. Use Amazon Athena Federated \nQuery to access data within Amazon RDS for PostgreSQL. Generate reports by using Amazon \nAthena. Publish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports.\nAmazon S3의 데이터에 대한 AWS Glue 테이블과 크롤러를 생성합니다 . Amazon Athena 연합 쿼리를 \n사용하여 PostgreSQL 용 Amazon RDS 내의 데이터에 액세스합니다 . Amazon Athena 를 사용하여 보고\n서를 생성합니다 . 보고서를 Amazon S3에 게시합니다 . S3 버킷 정책을 사용하여 보고서에 대한 액세스\n를 제한합니다 .\n​\n​"
    },
    "정답": "B"
  },
  {
    "문제번호": 17,
    "질문": "A company is implementing a new business application. The application runs on two Amazon EC2 \ninstances and uses an Amazon S3 bucket for document storage. A solutions architect needs to \nensure that the EC2 instances can access the S3 bucket.\nWhat should the solutions architect do to meet this requirement?\n회사에서 새로운 비즈니스 애플리케이션을 구현하고 있습니다 . 애플리케이션은 2개의 Amazon EC2 인\n스턴스에서 실행되며 문서 저장을 위해 Amazon S3 버킷을 사용합니다 . 솔루션 설계자는 EC2 인스턴\n스가 S3 버킷에 액세스할 수 있는지 확인해야 합니다 .\n솔루션 설계자는 이 요구 사항을 충족하기 위해 무엇을 해야 합니까 ?퀵 사이트 특징\n다양한 데이터 지원 신속한 결과 제공빠르고 직관적인 시각\n화임베디드 분석\n​",
    "보기": {
      "A": "Create an IAM role that grants access to the S3 bucket. Attach the role to the EC2 instances.\nS3 버킷에 대한 액세스 권한을 부여하는 IAM 역할을 생성합니다 . 역할을 EC2 인스턴스에 연결합니다 .\n​\n→ Amazon S3버킷에 대한 액세스 권한을 Amazon EC2에 부여하는 방법\n1. Amazon S3에 대한 액세스 권한을 부여하는 AWS Identity and Access Management(IAM) 프로파일 \n역할을 생성합니다 .\n2. 인스턴스에 IAM 인스턴스 프로파일을 연결합니다 .\n3. S3 버킷에 대한 권한을 확인합니다 .\n4. EC2 인스턴스에서 Amazon S3로의 네트워크 연결을 확인합니다 .\n5. S3 버킷에 대한 액세스를 확인합니다 .",
      "B": "Create an IAM policy that grants access to the S3 bucket. Attach the policy to the EC2 instances.\nS3 버킷에 대한 액세스 권한을 부여하는 IAM 정책을 생성합니다 . 정책을 EC2 인스턴스에 연결합니다 .\n​",
      "C": "Create an IAM group that grants access to the S3 bucket. Attach the group to the EC2 instances.\nS3 버킷에 대한 액세스 권한을 부여하는 IAM 그룹을 생성합니다 . 그룹을 EC2 인스턴스에 연결합니다 .\n​",
      "D": "Create an IAM user that grants access to the S3 bucket. Attach the user account to the EC2 \ninstances.\nS3 버킷에 대한 액세스 권한을 부여하는 IAM 사용자를 생성합니다 . 사용자 계정을 EC2 인스턴스에 연\n결합니다 .\n​\n​"
    },
    "정답": "A"
  },
  {
    "문제번호": 18,
    "질문": "An application development team is designing a microservice that will convert large images to \nsmaller, compressed images. When a user uploads an image through the web interface, the \nmicroservice should store the image in an Amazon S3 bucket, process and compress the image with \nan AWS Lambda function, and store the image in its compressed form in a different S3 bucket.\nA solutions architect needs to design a solution that uses durable, stateless components to process \nthe images automatically.\nWhich combination of actions will meet these requirements? (Choose two.)\n애플리케이션 개발 팀은 큰 이미지를 더 작은 압축 이미지로 변환하는 마이크로서비스를 설계하고 있\n습니다 . 사용자가 웹 인터페이스를 통해 이미지를 업로드하면 마이크로 서비스는 이미지를 Amazon S3 \n버킷에 저장하고 , AWS Lambda 함수로 이미지를 처리 ​​및 압축하고 , 다른 S3 버킷에 압축된 형태로 이\n미지를 저장해야 합니다 .\n솔루션 설계자는 내구성이 있는 상태 비저장 구성 요소를 사용하여 이미지를 자동으로 처리하는 솔루\n션을 설계해야 합니다 .\n이러한 요구 사항을 충족하는 작업 조합은 무엇입니까 ? (2개를 선택하세요 .)\n​",
    "보기": {
      "A": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure the S3 bucket to send a \nnotification to the SQS queue when an image is uploaded to the S3 bucket.\nAmazon Simple Queue Service(Amazon SQS) 대기열을 생성합니다 . 이미지가 S3 버킷에 업로드될 때 \nSQS 대기열에 알림을 보내도록 S3 버킷을 구성합니다 .\n​",
      "B": "Configure the Lambda function to use the Amazon Simple Queue Service (Amazon SQS) queue as \nthe invocation source. When the SQS message is successfully processed, delete the message in the \nqueue.\nAmazon Simple Queue Service(Amazon SQS) 대기열을 호출 소스로 사용하도록 Lambda 함수를 구성\n합니다 . SQS 메시지가 성공적으로 처리되면 대기열에서 메시지를 삭제합니다 .",
      "C": "Configure the Lambda function to monitor the S3 bucket for new uploads. When an uploaded image \nis detected, write the file name to a text file in memory and use the text file to keep track of the \nimages that were processed.\n새 업로드에 대해 S3 버킷을 모니터링하도록 Lambda 함수를 구성합니다 . 업로드된 이미지가 감지되면 \n메모리의 텍스트 파일에 파일 이름을 쓰고 텍스트 파일을 사용하여 처리된 이미지를 추적합니다 .\n​",
      "D": "Launch an Amazon EC2 instance to monitor an Amazon Simple Queue Service (Amazon SQS) \nqueue. When items are added to the queue, log the file name in a text file on the EC2 instance \nand invoke the Lambda function.\nAmazon EC2 인스턴스를 시작하여 Amazon Simple Queue Service(Amazon SQS) 대기열을 모니터링\n합니다 . 항목이 대기열에 추가되면 EC2 인스턴스의 텍스트 파일에 파일 이름을 기록하고 Lambda 함수\n를 호출합니다 .\n​\nE.\nConfigure an Amazon EventBridge (Amazon CloudWatch Events) event to monitor the S3 bucket. \nWhen an image is uploaded, send an alert to an Amazon ample Notification Service (Amazon SNS) \ntopic with the application owner's email address for further processing.\nAmazon EventBridge(Amazon CloudWatch Events) 이벤트를 구성하여 S3 버킷을 모니터링합니다 . 이\n미지가 업로드되면 추가 처리를 위해 애플리케이션 소유자의 이메일 주소와 함께 Amazon ample \nNotification Service(Amazon SNS) 주제에 알림을 보냅니다 .\n​"
    },
    "정답": "A"
  },
  {
    "문제번호": 19,
    "질문": "A company has a three-tier web application that is deployed on AWS. The web servers are deployed \nin a public subnet in a VPC. The application servers and database servers are deployed in private \nsubnets in the same VPC. The company has deployed a third-party virtual firewall appliance from \nAWS Marketplace in an inspection VPC. The appliance is configured with an IP interface that can \naccept IP packets.\nA solutions architect needs to integrate the web application with the appliance to inspect all traffic to \nthe application before the traffic reaches the web server.\nWhich solution will meet these requirements with the LEAST operational overhead?\n회사에 AWS에 배포된 3계층 웹 애플리케이션이 있습니다 . 웹 서버는 VPC의 퍼블릭 서브넷에 배포됩\n니다. 애플리케이션 서버와 데이터베이스 서버는 동일한 VPC의 프라이빗 서브넷에 배포됩니다 . 이 회\n사는 AWS Marketplace 의 타사 가상 방화벽 어플라이언스를 검사 VPC에 배포했습니다 .어플라이언스는 \nIP 패킷을 수락할 수 있는 IP 인터페이스 로 구성됩니다 .\n솔루션 설계자는 트래픽이 웹 서버에 도달하기 전에 애플리케이션에 대한 모든 트래픽을 검사하기 위\n해 웹 애플리케이션을 어플라이언스와 통합해야 합니다 .\n최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까 ?\n​",
    "보기": {
      "C": "Deploy a transit gateway in the inspection VPConfigure route tables to route the incoming packets \nthrough the transit gateway.\n전송 게이트웨이를 통해 들어오는 패킷을 라우팅하도록 라우팅 테이블을 구성하는 검사 VPC에 전송 \n게이트웨이를 배포합니다 .\n​",
      "A": "Create a Network Load Balancer in the public subnet of the application's VPC to route the traffic to \nthe appliance for packet inspection.\n애플리케이션 VPC의 퍼블릭 서브넷에 Network Load Balancer 를 생성하여 패킷 검사를 위해 어플라이\n언스로 트래픽을 라우팅합니다 .\n​",
      "B": "Create an Application Load Balancer in the public subnet of the application's VPC to route the traffic \nto the appliance for packet inspection.\n애플리케이션 VPC의 퍼블릭 서브넷에 Application Load Balancer 를 생성하여 패킷 검사를 위해 어플라\n이언스로 트래픽을 라우팅합니다 .\n​",
      "D": "Deploy a Gateway Load Balancer in the inspection VPC. Create a Gateway Load Balancer endpoint \nto receive the incoming packets and forward the packets to the appliance.\n검사 VPC에 게이트웨이 로드 밸런서를 배포합니다 . 게이트웨이 로드 밸런서 엔드포인트를 생성하여 수\n신 패킷을 수신하고 패킷을 어플라이언스로 전달합니다 .\n→ 어플라이언스 (기기)는 IP 패킷을 수락할 수 있는 IP 인터페이스로 구성 됨\n: Layer 3인 네트워크 계층을 뜻함\n​\nGateway Load Balancer\n: 게이트웨이 로드 밸런서를 사용하면 방화벽 , 침입 탐지 및 방지 시스템 , 심층 패킷 검사 시스템과 같\n은 가상 어플라이언스를 배포, 확장 및 관리할 수 있다.\n게이트웨이 로드 밸런서는 OSI(Open Systems Interconnection) 모델의 세 번째 계층인 네트워크 계층\n에서 작동한다 . \n모든 포트에서 모든 IP 패킷을 수신 대기하고 리스너 규칙에 지정된 대상 그룹으로 트래픽을 전달한다 . \nOSI 7 레이어 모델\n1 계층 - 물리 계층(Physical \nLayer)2 계층 - 링크 계층(Link \nLayer)3 계층 - 네트워크 계층\n(Network Layer)\n물리 계층은 전기적 , 기계적 , \n기능적인 특성을 이용해서 통신 \n장비로 데이터를 전송하게 된\n다.\n물리 계층에서는 단지 데이터를 \n전달만 한다. 데이터를 전기적\n인 신호로 변환해서 주고받는 \n기능만 할 뿐이다 .링크 계층은 네트워크 기기들 \n사이의 데이터를 전송하는 역할\n을 한다. 물리 계층을 통해 송\n수신되는 정보의 오류와 흐름을 \n관리하여 안전한 정보의 전달을 \n수행할 수 있도록 도와주는 역\n할을 한다.\n링크 계층은 에러검출 / 재전송 \n/ 흐름제어역할을 한다.중요한 기능 중 하나는 라우팅\n이다. 이는 데이터를 목적지까\n지 안전하고 빠르게 전달하는 \n기능을 말한다 . 경로를 선택하\n고 주소를 정하고 경로에 따라 \n패킷을 전달해주는 것이 네트워\n크 계층의 역할이다 .\n•PDU : 비트(Bit)\n•프로토콜 데이터 단위\n(PDU, Protocol Data \nUnit)는 데이터 통신에\n서 상위 계층이 전달\n한 데이터에 붙이는 \n제어정보를 뜻한다 .\n•프로토콜 : Modem, \nCable, Fiber, \nRS-232C\n•대표장비 : 허브, 리피\n터​\n•PDU : 프레임 (Frame)\n•프로토콜 : 이더넷\n(Ethernet), MAC, \nPPP, ATM, LAN, Wifi\n•대표장비 : 브릿지 , 스\n위치​\n•PDU : 패킷(Packet)\n•프로토콜 : IP, ICMP \n등\n•대표장비 : 라우터 , L3 \n스위치\n4 계층 - 전송 계층(Transport \nLayer)5 계층 - 세션 계층(Session \nLayer)6 계층 - 표현 계층\n(Presentation Layer)\n전송 계층은 통신을 할성화하기 \n위한 계층이다 . 양 끝단의 사용\n자들이 신뢰성있는 데이터를 주\n고 받게 해주는 역할을 한다.\n보통 TCP 프로토콜을 이용하\n며, 포트를 열어서 응용프로그\n램이 전송을 할 수 있게 한다.\n중요한 것은 데이터 전송을 위\n해서 Port 번호가 사용된다는 \n점이다 . 대표적인 프로토콜로 통신 세션을 구성하는 계층으\n로, 포트(Port)번호를 기반으로 \n연결한다 . 통신장치 간의 상호\n작용을 설정하고 유지하며 동기\n화한다 .표현 계층(Presentation layer) 은 \n코드 간의 번역을 담당하여 사\n용자 시스템에서 데이터의 형식\n상 차이를 다루는 부담을 응용 \n계층으로부터 덜어 준다. MIME \n인코딩이나 암호화 등의 동작이 \n표현 계층에서 이루어지는 것이\n다.\n​\n▶오답\nTransit Gateway (전송 게이트웨이 )\n: Transit Gateway 는 가상 사설 클라우드 (VPC) 와 온프레미스 네트워크를 상호 연결하는 데 사용할 수 \n있는 네트워크 전송 허브이다 . \n클라우드 인프라가 전 세계적으로 확장됨에 따라 리전 간 피어링은 AWS 글로벌 인프라를 사용하여 \nTransit Gateway 를 함께 연결한다 . \n​\n연결 가능\n•하나 이상의 VPC\n•Connect SD-WAN/ 서드 파티 네트워크 어플라이언스TCP와 UDP가 있다. 이 계층에\n서 사용하는 데이터 단위는 세\n그먼트이다 .\n•PDU : 세그먼트\n(Segment)\n•프로토콜 : TCP, UDP \n, ARP, RTP\n•대표장비 : 게이트웨\n이, L4 스위치•PDU : 데이터 (Data)\n•프로토콜 : NetBIOS, \nSSH, TLS•PDU : 데이터 (Data)\n•프로토콜 : JPG, \nMPEG, SMB, AFP\n7 계층 - 응용 계층\n(Application Layer)\n​응용 계층은 사용자와 바로 연\n결되어 있으며 응용 SW를 도\n와주는 계층이다 . 사용자로부터 \n정보를 입력받아 하위 계층으로 \n전달하고 하위 계층에서 전송한 \n데이터를 사용자에게 전달한다 .\n파일 전송, DB, 메일 전송 등 \n여러가지 응용 서비스를 네트워\n크에 연결해주는 역할을 한다.\n결국 응용 계층은 응용 프로세\n스와 직접 관계하여 일반적인 \n응용 서비스를 수행한다 .\n•​\n•PDU : 데이터 (Data)\n•프로토콜 : DHCP, \nDNS, FTP, HTTP\n•AWS Direct Connect 게이트웨이\n•다른 Transit Gateway 와의 피어링 연결\n•Transit Gateway 에 대한 VPN 연결"
    },
    "정답": "D"
  },
  {
    "문제번호": 20,
    "질문": "A company wants to improve its ability to clone large amounts of production data into a test \nenvironment in the same AWS Region. The data is stored in Amazon EC2 instances on Amazon \nElastic Block Store (Amazon EBS) volumes. Modifications to the cloned data must not affect the \nproduction environment. The software that accesses this data requires consistently high I/O \nperformance.\nA solutions architect needs to minimize the time that is required to clone the production data into the \ntest environment.\nWhich solution will meet these requirements?\n회사에서 동일한 AWS 리전의 테스트 환경에 대량의 프로덕션 데이터를 복제하는 기능을 개선하려고 \n합니다 . 데이터는 Amazon Elastic Block Store(Amazon EBS) 볼륨의 Amazon EC2 인스턴스에 저장됩\n니다. 복제된 데이터를 수정해도 프로덕션 환경에 영향을 주지 않아야 합니다 . 이 데이터에 액세스하는 \n소프트웨어는 일관되게 높은 I/O 성능을 요구합니다 .\n솔루션 설계자는 프로덕션 데이터를 테스트 환경에 복제하는 데 필요한 시간을 최소화 해야 합니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n​",
    "보기": {
      "A": "Take EBS snapshots of the production EBS volumes. Restore the snapshots onto EC2 instance \nstore volumes in the test environment.\n프로덕션 EBS 볼륨의 EBS 스냅샷을 만듭니다 . 테스트 환경의 EC2 인스턴스 스토어 볼륨에 스냅샷을 \n복원합니다 .\n​",
      "B": "Configure the production EBS volumes to use the EBS Multi-Attach feature. Take EBS snapshots of \nthe production EBS volumes. Attach the production EBS volumes to the EC2 instances in the test \nenvironment.\nEBS 다중 연결 기능을 사용하도록 프로덕션 EBS 볼륨을 구성합니다 . 프로덕션 EBS 볼륨의 EBS 스냅\n샷을 만듭니다 . 테스트 환경의 EC2 인스턴스에 프로덕션 EBS 볼륨을 연결합니다 .\n​",
      "C": "Take EBS snapshots of the production EBS volumes. Create and initialize new EBS volumes. Attach \nthe new EBS volumes to EC2 instances in the test environment before restoring the volumes from \nthe production EBS snapshots.\n프로덕션 EBS 볼륨의 EBS 스냅샷을 만듭니다 . 새 EBS 볼륨을 생성하고 초기화합니다 . 프로덕션 EBS \n스냅샷에서 볼륨을 복원하기 전에 테스트 환경의 EC2 인스턴스에 새 EBS 볼륨을 연결합니다 .\n​",
      "D": "Take EBS snapshots of the production EBS volumes. Turn on the EBS fast snapshot restore feature \non the EBS snapshots. Restore the snapshots into new EBS volumes. Attach the new EBS volumes \nto EC2 instances in the test environment.\n프로덕션 EBS 볼륨의 EBS 스냅샷을 만듭니다 . EBS 스냅샷에서 EBS 빠른 스냅샷 복원 기능을 켭니\n다. 스냅샷을 새 EBS 볼륨으로 복원합니다 . 테스트 환경의 EC2 인스턴스에 새 EBS 볼륨을 연결합니\n다.\n→ Amazon EBS Fast Snapshot Restore (FSR)\n: Amazon EBS FSR(빠른 스냅샷 복원)을 사용하면 생성 시 완전히 초기화되는 스냅샷에서 볼륨을 생\n성할 수 있다. 이렇게 하면 블록에 처음 액세스할 때 블록에 대한 I/O 작업의 대기 시간이 제거된다 . 빠\n른 스냅샷 복원을 사용하여 생성된 볼륨은 프로비저닝된 모든 성능을 즉시 제공한다 .\n특정 가용 영역의 특정 스냅샷에 대한 빠른 스냅샷 복원을 활성화 하여 시작할 수 있다.\n활성화된 가용 영역 중 하나에서 이러한 스냅샷 중 하나에서 볼륨을 생성하면 빠른 스냅샷 복원을 사\n용하여 볼륨이 복원된다 .\n​\n​\n▶오답\nEBS 다중 연결\n: Amazon EBS 다중 연결을 사용하면 단일 프로비저닝된 IOPS SSD( io1또는 io2) 볼륨을 동일한 가용 \n영역에 있는 여러 인스턴스에 연결할 수 있다. 여러 다중 연결 지원 볼륨을 인스턴스 또는 인스턴스 집\n합에 연결할 수 있으며 , 다중 연결을 사용하면 동시 쓰기 작업을 관리하는 클러스터링된 Linux 애플리\n케이션에서 더 쉽게 더 높은 애플리케이션 가용성을 얻을 수 있다.\n​"
    },
    "정답": "D"
  },
  {
    "문제번호": 21,
    "질문": "An ecommerce company wants to launch a one-deal-a-day website on AWS. Each day will feature \nexactly one product on sale for a period of 24 hours. The company wants to be able to handle \nmillions of requests each hour with millisecond latency during peak hours.\nWhich solution will meet these requirements with the LEAST operational overhead?\n전자 상거래 회사는 AWS에서 하루 1회 웹 사이트를 시작하려고 합니다 . 매일 24시간 동안 정확히 하\n나의 제품을 판매합니다 . 회사는 피크 시간 동안 밀리초 지연 시간으로 시간당 수백만 개의 요청을 처\n리할 수 있기를 원합니다 .\n최소한의 운영 오버헤드 로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까 ?\n​",
    "보기": {
      "A": "Use Amazon S3 to host the full website in different S3 buckets. Add Amazon CloudFront \ndistributions. Set the S3 buckets as origins for the distributions. Store the order data in Amazon S3.\nAmazon S3를 사용하여 다른 S3 버킷에 전체 웹 사이트를 호스팅합니다 . Amazon CloudFront 배포를 \n추가합니다 . S3 버킷을 배포의 오리진으로 설정합니다 . Amazon S3에 주문 데이터를 저장합니다 .\n​",
      "B": "Deploy the full website on Amazon EC2 instances that run in Auto Scaling groups across multiple \nAvailability Zones. Add an Application Load Balancer (ALB) to distribute the website traffic. Add \nanother ALB for the backend APIs. Store the data in Amazon RDS for MySQL.\n여러 가용 영역의 Auto Scaling 그룹에서 실행되는 Amazon EC2 인스턴스에 전체 웹 사이트를 배포합\n니다. ALB(Application Load Balancer) 를 추가하여 웹 사이트 트래픽을 분산합니다 . 백엔드 API에 대해 \n다른 ALB를 추가하십시오 . MySQL 용 Amazon RDS에 데이터를 저장합니다 .\n​",
      "C": "Migrate the full application to run in containers. Host the containers on Amazon Elastic Kubernetes \nService (Amazon EKS). Use the Kubernetes Cluster Autoscaler to increase and decrease the number \nof pods to process bursts in traffic. Store the data in Amazon RDS for MySQL.\n컨테이너에서 실행되도록 전체 애플리케이션을 마이그레이션합니다 . Amazon Elastic Kubernetes \nService(Amazon EKS)에서 컨테이너를 호스팅합니다 . Kubernetes 클러스터 자동 확장 처리를 사용하여 \n트래픽 버스트를 처리할 포드 수를 늘리거나 줄입니다 . MySQL 용 Amazon RDS에 데이터를 저장합니\n다.\n​",
      "D": "Use an Amazon S3 bucket to host the website's static content. Deploy an Amazon CloudFront \ndistribution. Set the S3 bucket as the origin. Use Amazon API Gateway and AWS Lambda functions \nfor the backend APIs. Store the data in Amazon DynamoDB.\nAmazon S3 버킷을 사용하여 웹 사이트의 정적 콘텐츠를 호스팅합니다 . Amazon CloudFront 배포를 배\n포합니다 . S3 버킷을 오리진으로 설정합니다 . 백엔드 API에 Amazon API Gateway 및 AWS Lambda \n함수를 사용합니다 . Amazon DynamoDB 에 데이터를 저장합니다 .\n→ 확장성이 가장 뛰어난 서비스를 고르는 문제\n:정답 D의 모든 구성 요소인 dynamoDB, API Gateway, Lambda, 그리고 s3+cloudfront 까지 무한 확장 \n가능하기 때문"
    },
    "정답": "D"
  },
  {
    "문제번호": 22,
    "질문": "A solutions architect is using Amazon S3 to design the storage architecture of a new digital media \napplication. The media files must be resilient to the loss of an Availability Zone. Some files are \naccessed frequently while other files are rarely accessed in an unpredictable pattern. The solutions \narchitect must minimize the costs of storing and retrieving the media files.\nWhich storage option meets these requirements?\n솔루션 설계자는 Amazon S3를 사용하여 새로운 디지털 미디어 애플리케이션의 스토리지 아키텍처를 \n설계하고 있습니다 . 미디어 파일은 가용 영역 손실에 대한 복원력이 있어야 합니다 . 일부 파일은 자주 \n액세스되는 반면 다른 파일은 예측할 수 없는 패턴으로 거의 액세스되지 않습니다 .솔루션 설계자는 미\n디어 파일을 저장하고 검색하는 비용을 최소화해야 합니다 .\n이러한 요구 사항을 충족하는 스토리지 옵션은 무엇입니까 ?\n​",
    "보기": {
      "A": "S3 Standard\n​",
      "B": "S3 Intelligent-Tiering\n​\n→ Amazon S3의 데이터 보호\nAmazon S3에서는 미션 크리티컬 및 기본 데이터 스토리지에 적합하게 설계된 , 내구성이 뛰어난 스토\n리지 인프라를 제공한다 . \nS3 Standard, S3 Intelligent-Tiering, S3 Standard-IA, S3 Glacier Instant Retrieval, S3 Glacier Flexible \nRetrieval 및 S3 Glacier Deep Archive 는 AWS 리전의 최소 3개 가용 영역에 걸쳐 여러 디바이스에 객\n체를 중복 저장한다 .\n​\nAmazon S3 Intelligent Tiering\n: 액세스 패턴이 변화할 경우 성능 저하나 운영 오버헤드 없이 가장 비용 효율적인 액세스 티어로 데이\n터를 자동으로 이동하여 스토리지 비용을 최적화하도록 설계된 Amazon S3 스토리지 클래스이다 . \n객체 크기 또는 보존 기간에 상관없이 액세스 패턴을 알 수 없거나 예측할 수 없는, 또는 액세스 패턴\n이 변화하는 데이터의 경우 S3 Intelligent-Tiering 이 이상적인 스토리지 클래스이다 . \nS3 Intelligent Tiering 은 빈번한 액세스에 최적화된 티어, 빈번하지 않은 액세스에 최적화된 저비용 티\n어, 거의 액세스하지 않는 데이터에 최적화된 초저가 티어, 이렇게 3개의 액세스 티어에 객체를 자동으\n로 저장한다 .",
      "C": "S3 Standard-Infrequent Access (S3 Standard-IA)\n​",
      "D": "S3 One Zone-Infrequent Access (S3 One Zone-IA)\n​"
    },
    "정답": "B"
  },
  {
    "문제번호": 23,
    "질문": "A company is storing backup files by using Amazon S3 Standard storage. The files are accessed \nfrequently for 1 month. However, the files are not accessed after 1 month. The company must keep \nthe files indefinitely.\nWhich storage solution will meet these requirements MOST cost-effectively?\n회사에서 Amazon S3 Standard 스토리지 를 사용하여 백업 파일을 저장하고 있습니다 .1개월 동안 파일\n에 자주 액세스합니다 . 단, 1개월 이후에는 파일에 접근하지 않습니다 . 회사는 파일을 무기한 보관해야 \n합니다 .\n이러한 요구 사항을 가장비용 효율적 으로 충족하는 스토리지 솔루션은 무엇입니까 ?\n​",
    "보기": {
      "A": "Configure S3 Intelligent-Tiering to automatically migrate objects.\n객체를 자동으로 마이그레이션하도록 S3 Intelligent-Tiering 을 구성합니다 .\n​",
      "B": "Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Glacier Deep \nArchive after 1 month.\nS3 수명 주기 구성을 생성하여 1개월 후에 S3 Standard 에서 S3 Glacier Deep Archive 로 객체를 전환\n합니다 .\n​\n→ 객체 수명 주기 관리\n: S3는 버킷에 저장된 객체의 수명 주기 (Life Cycle) 를 관리할 수 있는 기능이 있다.\n이 기능은 일정 시간이 지났을 때, 사용 되지 않는 파일들을 삭제하거나 다른 곳에 백업하여 S3 저장 \n공간을 절약할 수 있는 비용 효울적인 방법이다 .\n​\n•전환 작업 \n객체가 다른 스토리지 클래스로 전환할 시기를 정의한다 .\n예를 들어, 생성 후 30일이 지나면 객체를 S3 Standard-IA 스토리지 클래스로 전환하거나 , 생성 후 1년\n이 지나면 객체를 S3 Glacier 스토리지 클래스에 아카이브하도록 선택할 수 있다.\n​\n•만료 작업\n객체가 만료되는 시기를 정의한다 . Amazon S3는 만료된 객체를 자동으로 삭제한다 .",
      "C": "Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Standard-Infrequent \nAccess (S3 Standard-IA) after 1 month.\nS3 수명 주기 구성을 생성하여 1개월 후에 객체를 S3 Standard 에서 S3 Standard-Infrequent \nAccess(S3 Standard-IA) 로 전환합니다 .\n​",
      "D": "Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 One \nZone-Infrequent Access (S3 One Zone-IA) after 1 month.\nS3 수명 주기 구성을 생성하여 1개월 후에 객체를 S3 Standard 에서 S3 One Zone-Infrequent \nAccess(S3 One Zone-IA) 로 전환합니다 .\n​\n​"
    },
    "정답": "B"
  },
  {
    "문제번호": 24,
    "질문": "A company observes an increase in Amazon EC2 costs in its most recent bill. The billing team \nnotices unwanted vertical scaling of instance types for a couple of EC2 instances. A solutions \narchitect needs to create a graph comparing the last 2 months of EC2 costs and perform an \nin-depth analysis to identify the root cause of the vertical scaling.\nHow should the solutions architect generate the information with the LEAST operational overhead?\n회사는 가장 최근 청구서에서 Amazon EC2 비용 증가를 관찰했습니다 . 청구 팀은 몇 개의 EC2 인스턴\n스에 대한 인스턴스 유형의 원치 않는 수직적 확장을 발견했습니다 . 솔루션 설계자는 지난 2개월간의 \nEC2 비용을 비교하는 그래프를 생성하고 심층 분석을 수행하여 수직적 확장의 근본 원인을 식별해야 \n합니다 .\n솔루션 설계자는 운영 오버헤드가 가장 적은 정보를 어떻게 생성해야 합니까 ?\n​",
    "보기": {
      "A": "Use AWS Budgets to create a budget report and compare EC2 costs based on instance types.\nAWS 예산을 사용하여 예산 보고서를 생성하고 인스턴스 유형에 따라 EC2 비용을 비교합니다 .\n​",
      "B": "Use Cost Explorer's granular filtering feature to perform an in-depth analysis of EC2 costs based on \ninstance types.\nCost Explorer 의 세분화된 필터링 기능을 사용하여 인스턴스 유형을 기반으로 EC2 비용에 대한 심층 \n분석을 수행합니다 .\n​\nAWS Cost Explorer\n: 비용 및 사용량을 보고 분석할 수 있는 도구입니다 . 기본 그래프 , 비용 탐색기 비용 및 사용 보고서 \n또는 비용 탐색기 RI 보고서를 사용하여 사용량 및 비용을 탐색할 수 있다. \n최대 지난 12개월 동안의 데이터를 보고 향후 12개월 동안 지출할 가능성이 있는 금액을 예측하고 구\n매할 예약 인스턴스에 대한 권장 사항을 얻을 수 있다. 비용 탐색기를 사용하여 추가 조회가 필요한 영\n역을 식별하고 비용을 이해하는 데 사용할 수 있는 추세를 확인할 수 있다.",
      "C": "Use graphs from the AWS Billing and Cost Management dashboard to compare EC2 costs based on \ninstance types for the last 2 months.\nAWS Billing and Cost Management 대시보드의 그래프를 사용하여 지난 2개월 동안의 인스턴스 유형\n을 기준으로 EC2 비용을 비교합니다 .\n​",
      "D": "Use AWS Cost and Usage Reports to create a report and send it to an Amazon S3 bucket. Use \nAmazon QuickSight with Amazon S3 as a source to generate an interactive graph based on instance \ntypes.\nAWS 비용 및 사용 보고서를 사용하여 보고서를 생성하고 Amazon S3 버킷으로 보냅니다 . Amazon S3\n와 함께 Amazon QuickSight 를 소스로 사용하여 인스턴스 유형을 기반으로 대화형 그래프를 생성합니\n다.\n​\n​"
    },
    "정답": "B"
  },
  {
    "문제번호": 25,
    "질문": "A company is designing an application. The application uses an AWS Lambda function to receive \ninformation through Amazon API Gateway and to store the information in an Amazon Aurora \nPostgreSQL database.\nDuring the proof-of-concept stage, the company has to increase the Lambda quotas significantly to \nhandle the high volumes of data that the company needs to load into the database. A solutions \narchitect must recommend a new design to improve scalability and minimize the configuration effort.\nWhich solution will meet these requirements?\n회사에서 응용 프로그램을 설계하고 있습니다 . 애플리케이션은 AWS Lambda 함수를 사용하여 Amazon \nAPI Gateway 를 통해 정보를 수신하고 Amazon Aurora PostgreSQL 데이터베이스에 정보를 저장합니\n다.\n개념 증명 단계에서 회사는 데이터베이스에 로드해야 하는 대용량 데이터를 처리하기 위해 Lambda 할\n당량을 크게 늘려야 합니다 . 솔루션 설계자는 확장성을 개선하고 구성 노력을 최소화하기 위해 새로운 \n설계를 권장해야 합니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n​",
    "보기": {
      "A": "Refactor the Lambda function code to Apache Tomcat code that runs on Amazon EC2 instances. \nConnect the database by using native Java Database Connectivity (JDBC) drivers.\nLambda 함수 코드를 Amazon EC2 인스턴스에서 실행되는 Apache Tomcat 코드로 *리팩터링합니다 . \n네이티브 JDBC(Java Database Connectivity) 드라이버를 사용하여 데이터베이스를 연결합니다 .\n* 리팩토링 : 이미 작성한 소스코드에서 구현된 일련의 행위들을 변경없이 , 코드의 가독성과 유지보수\n성을 높이기 위해 내부구조를 변경하는 것.\n​",
      "B": "Change the platform from Aurora to Amazon DynamoDProvision a DynamoDB Accelerator (DAX) \ncluster. Use the DAX client SDK to point the existing DynamoDB API calls at the DAX cluster.\n플랫폼을 Aurora 에서 Amazon DynamoDProvision a DynamoDB Accelerator(DAX) 클러스터로 변경합니\n다. DAX 클라이언트 SDK를 사용하여 DAX 클러스터에서 기존 DynamoDB API 호출을 가리킵니다 .\n​",
      "C": "Set up two Lambda functions. Configure one function to receive the information. Configure the other \nfunction to load the information into the database. Integrate the Lambda functions by using Amazon \nSimple Notification Service (Amazon SNS).\n두 개의 Lambda 함수를 설정합니다 . 정보를 수신할 하나의 기능을 구성하십시오 . 정보를 데이터베이스\n에 로드하도록 다른 기능을 구성하십시오 . Amazon Simple Notification Service(Amazon SNS)를 사용하\n여 Lambda 함수를 통합합니다 .\n​",
      "D": "Set up two Lambda functions. Configure one function to receive the information. Configure the other \nfunction to load the information into the database. Integrate the Lambda functions by using an \nAmazon Simple Queue Service (Amazon SQS) queue.\n두 개의 Lambda 함수를 설정합니다 . 정보를 수신할 하나의 기능을 구성하십시오 . 정보를 데이터베이스\n에 로드하도록 다른 기능을 구성하십시오 . Amazon Simple Queue Service(Amazon SQS) 대기열을 사\n용하여 Lambda 함수를 통합합니다 .\n​\n→ Amazon SQS에서 Lambda 사용\n: Lambda 함수를 사용하여 Amazon Simple Queue Service(Amazon SQS) 대기열의 메시지를 처리할 \n수 있습니다 . \nLambda 함수가 SQS의 대기열로 이동할 수 있으며 , 디커플링으로 부하를 분산함으로서 성능 향상에 \n도움이된다 .\n​\nAmazon SQS 이벤트 소스를 사용하여 Lambda 함수를 최적의 상태로 크키 조정\n→ 최적의 실행 시 이벤트 소스로 구성된 Amazon SQS 대기열을 사용하는 Lambda 함수는 분당 최대 \n60개의 인스턴스로 확장할 수 있다. \n최대 동시 호출 수는 1,000개이다 . FIFO 이벤트 소스 매핑을 사용하는 경우 함수는 활성 메세지그룹 \n수에 따라 동시성을 조정할 수 있다.\n​"
    },
    "정답": "D"
  },
  {
    "문제번호": 26,
    "질문": "A company needs to review its AWS Cloud deployment to ensure that its Amazon S3 buckets do \nnot have unauthorized configuration changes.\nWhat should a solutions architect do to accomplish this goal?\n회사는 AWS 클라우드 배포를 검토하여 Amazon S3 버킷에 무단 구성 변경이 없는지 확인해야 합니다 .\n솔루션 설계자는 이 목표를 달성하기 위해 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Turn on AWS Config with the appropriate rules.\n적절한 규칙으로 AWS Config 를 켭니다 .\n​\n→ AWS Config 의 작동 방식\n: AWS Config 를 켜면 먼저 계정에 있는 지원되는 AWS 리소스를 찾고, 각 리소스에 대한 구성 항목을 \n생성한다 . 또한 AWS Config 는 리소스의 구성이 변경되면 구성 항목을 생성하고 , 구성 레코더를 시작할 \n때부터 리소스의 구성 항목에 대한 기록 내역을 유지한다 .\n​\nAmazon S3 버킷\n: AWS Config 구성 변경 사항을 추적한다 .\nAWS리소스를 지정하고 지정한 Amazon S3 버킷으로 업데이트된 구성 세부 정보를 정기적으로 보낸\n다.",
      "B": "Turn on AWS Trusted Advisor with the appropriate checks.\n적절한 검사를 통해 AWS Trusted Advisor 를 켭니다 .\n​",
      "C": "Turn on Amazon Inspector with the appropriate assessment template.\n적절한 평가 템플릿으로 Amazon Inspector 를 켭니다 .\n​",
      "D": "Turn on Amazon S3 server access logging. Configure Amazon EventBridge (Amazon Cloud Watch \nEvents).\nAmazon S3 서버 액세스 로깅을 켭니다 . Amazon EventBridge(Amazon Cloud Watch Events) 를 구성합\n니다.\n​\n​"
    },
    "정답": "A"
  },
  {
    "문제번호": 27,
    "질문": "A company is launching a new application and will display application metrics on an Amazon \nCloudWatch dashboard. The company's product manager needs to access this dashboard periodically. \nThe product manager does not have an AWS account. A solutions architect must provide access to \nthe product manager by following the principle of least privilege.\nWhich solution will meet these requirements?\n회사에서 새 애플리케이션을 시작하고 Amazon CloudWatch 대시보드에 애플리케이션 지표를 표시합니\n다. 회사의 제품 관리자는 이 대시보드에 주기적으로 액세스해야 합니다 . 제품 관리자에게 AWS 계정이 \n없습니다 . 솔루션 설계자는 최소 권한 원칙에 따라 제품 관리자에 대한 액세스를 제공해야 합니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n​",
    "보기": {
      "A": "Share the dashboard from the CloudWatch console. Enter the product manager's email address, and \ncomplete the sharing steps. Provide a shareable link for the dashboard to the product manager.\nCloudWatch 콘솔에서 대시보드를 공유합니다 . 제품 관리자의 이메일 주소를 입력하고 공유 단계를 완\n료합니다 . 대시보드에 대한 공유 가능한 링크를 제품 관리자에게 제공하십시오 .\n​\n→ CloudWatch 대시보드 공유\n: AWS 계정에 직접 액세스할 수 없는 사용자와 CloudWatch 대시보드를 공유할 수 있다. 이렇게 하면 \n팀 간에 그리고 이해 관계자 및 조직 외부의 사람들과 대시보드를 공유할 수 있다. \n​\n​",
      "B": "Create an IAM user specifically for the product manager. Attach the CloudWatchReadOnlyAccess \nAWS managed policy to the user. Share the new login credentials with the product manager. Share \nthe browser URL of the correct dashboard with the product manager.\n특히 제품 관리자를 위한 IAM 사용자를 생성합니다 . CloudWatchReadOnlyAccess AWS 관리형 정책을 \n사용자에게 연결합니다 . 새 로그인 자격 증명을 제품 관리자와 공유하십시오 . 올바른 대시보드의 브라\n우저 URL을 제품 관리자와 공유하십시오 .\n​",
      "C": "Create an IAM user for the company's employees. Attach the ViewOnlyAccess AWS managed policy \nto the IAM user. Share the new login credentials with the product manager. Ask the product \nmanager to navigate to the CloudWatch console and locate the dashboard by name in the \nDashboards section.\n회사 직원을 위한 IAM 사용자를 생성합니다 . ViewOnlyAccess AWS 관리형 정책을 IAM 사용자에게 연\n결합니다 . 새 로그인 자격 증명을 제품 관리자와 공유하십시오 . 제품 관리자에게 CloudWatch 콘솔로 \n이동하여 대시보드 섹션에서 이름으로 대시보드를 찾으라고 요청합니다 .\n​",
      "D": "Deploy a bastion server in a public subnet. When the product manager requires access to the \ndashboard, start the server and share the RDP credentials. On the bastion server, ensure that the \nbrowser is configured to open the dashboard URL with cached AWS credentials that have \nappropriate permissions to view the dashboard.\n퍼블릭 서브넷에 배스천 서버를 배포합니다 . 제품 관리자가 대시보드에 액세스해야 하는 경우 서버를 \n시작하고 RDP 자격 증명을 공유합니다 . 배스천 서버에서 대시보드를 볼 수 있는 적절한 권한이 있는 \n캐시된 AWS 자격 증명으로 대시보드 URL을 열도록 브라우저가 구성되어 있는지 확인합니다 .\n​\n​"
    },
    "정답": "A"
  },
  {
    "문제번호": 28,
    "질문": "A company is migrating applications to AWS. The applications are deployed in different accounts. \nThe company manages the accounts centrally by using AWS Organizations. The company's security \nteam needs a single sign-on (SSO) solution across all the company's accounts. The company must \ncontinue managing the users and groups in its on-premises self-managed Microsoft Active Directory.\nWhich solution will meet these requirements?\n회사에서 애플리케이션을 AWS로 마이그레이션하고 있습니다 . 응용 프로그램은 다른 계정에 배포됩니\n다. 회사는 AWS Organizations 를 사용하여 중앙에서 계정을 관리합니다 . 회사의 보안 팀은 회사의 모\n든 계정에 *SSO(Single Sign-On) 솔루션 이 필요합니다 . 회사는 사내 자체 관리 Microsoft Active \nDirectory 에서 사용자 및 그룹을 계속 관리해야 합니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n​",
    "보기": {
      "A": "Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console. Create a one-way forest trust \nor a one-way domain trust to connect the company's self-managed Microsoft Active Directory with \nAWS SSO by using AWS Directory Service for Microsoft Active Directory.\nAWS SSO 콘솔에서 AWS Single Sign-On(AWS SSO)을 활성화합니다 . 단방향 포리스트 트러스트 또는 \n단방향 도메인 트러스트를 생성하여 Microsoft Active Directory 용 AWS Directory Service 를 사용하여 회\n사의 자체 관리형 Microsoft Active Directory 를 AWS SSO와 연결합니다 .\n​\nAWS Single Sign-On(AWS SSO) = AWS IAM Identity Center\n: 직원 사용자를 한 번 생성하거나 연결하고 여러 AWS 계정 및 애플리케이션에 대한 직원 사용자 액\n세스를 중앙에서 관리한다 .\nIAM Identity Center 에서 직접 사용자 아이덴티티를 생성하거나 Microsoft Active Directory 및 Okta \nUniversal Directory 또는 Azure AD와 같은 표준 기반의 ID 제공업체를 비롯하여 기존의 아이덴티티 소\n스를 연결할 수 있다. \n​\n→ AWS Managed Microsoft AD 신뢰에 대해 알고 싶었던 모든 것\n: 많은 AWS 고객은 Active Directory 를 사용하여 다양한 애플리케이션 및 서비스에 대한 사용자 인증 \n및 권한 부여를 중앙 집중화합니다 . AWS는 가용성이 높고 탄력적인 Active Directory 서비스를 제공하\n기 위해 AWS Managed Microsoft AD라고도 하는 Microsoft Active Directory 용 AWS Directory Service\n를 제공한다 . \n​\n​",
      "B": "Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console. Create a two-way forest trust \nto connect the company's self-managed Microsoft Active Directory with AWS SSO by using AWS \nDirectory Service for Microsoft Active Directory.\nAWS SSO 콘솔에서 AWS Single Sign-On(AWS SSO)을 활성화합니다 . Microsoft Active Directory 용 \nAWS Directory Service 를 사용하여 회사의 자체 관리형 Microsoft Active Directory 를 AWS SSO와 연결\n하는 양방향 포리스트 트러스트를 생성합니다 .\n​",
      "C": "Use AWS Directory Service. Create a two-way trust relationship with the company's self-managed \nMicrosoft Active Directory.\nAWS 디렉터리 서비스를 사용합니다 . 회사의 자체 관리 Microsoft Active Directory 와 양방향 신뢰 관계\n를 만드십시오 .\n​",
      "D": "Deploy an identity provider (IdP) on premises. Enable AWS Single Sign-On (AWS SSO) from the \nAWS SSO console.\n온프레미스에 ID 공급자 (IdP)를 배포합니다 . AWS SSO 콘솔에서 AWS Single Sign-On(AWS SSO)을 \n활성화합니다 .\n​\n​"
    },
    "정답": "A"
  },
  {
    "문제번호": 29,
    "질문": "A company provides a Voice over Internet Protocol (VoIP) service that uses UDP connections. The \nservice consists of Amazon EC2 instances that run in an Auto Scaling group. The company has \ndeployments across multiple AWS Regions.\nThe company needs to route users to the Region with the lowest latency. The company also needs \nautomated failover between Regions.\nWhich solution will meet these requirements?\n회사는 UDP 연결을 사용하는 VoIP(Voice over Internet Protocol) 서비스 를 제공합니다 . 이 서비스는 \nAuto Scaling 그룹에서 실행되는 Amazon EC2 인스턴스로 구성됩니다 . 회사는 여러 AWS 리전에 배포\n하고 있습니다 .\n회사는 지연 시간이 가장 짧은 리전으로 사용자를 라우팅해야 합니다 . 이 회사는 또한지역 간 자동 장\n애 조치가 필요합니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n​",
    "보기": {
      "A": "Deploy a Network Load Balancer (NLB) and an associated target group. Associate the target group \nwith the Auto Scaling group. Use the NLB as an AWS Global Accelerator endpoint in each Region.\nNLB(Network Load Balancer) 및 연결된 대상 그룹을 배포합니다 . 대상 그룹을 Auto Scaling 그룹과 연\n결합니다 . 각 리전에서 NLB를 AWS Global Accelerator 엔드포인트로 사용합니다 .\n​\n→ AWS Global Accelerator\n: Global Accelerator 는 엣지에서 패킷을 단일 또는 여러 AWS 리전에서 실행되는 애플리케이션으로 프\n록시하여 TCP 또는 UDP를 통해 광범위한 애플리케이션의 성능을 개선한다 .\nGlobal Accelerator 는 게임(UDP), IoT(MQTT) 또는 VoIP와 같은 HTTP 외 사용 사례는 물론, 특별히 정\n적 IP 주소 또는 결정적 빠른 지역 장애 복구를 요구하는 HTTP 사용 사례에 적합하다 . \n두 서비스 모두 DDoS 공격을 막기 위해 AWS Shield 와 통합되어 있다.\n​\n​",
      "B": "Deploy an Application Load Balancer (ALB) and an associated target group. Associate the target \ngroup with the Auto Scaling group. Use the ALB as an AWS Global Accelerator endpoint in each \nRegion.\nALB(Application Load Balancer) 및 연결된 대상 그룹을 배포합니다 . 대상 그룹을 Auto Scaling 그룹과 \n연결합니다 . 각 리전에서 ALB를 AWS Global Accelerator 엔드포인트로 사용합니다 .\n​",
      "C": "Deploy a Network Load Balancer (NLB) and an associated target group. Associate the target group \nwith the Auto Scaling group. Create an Amazon Route 53 latency record that points to aliases for \neach NLB. Create an Amazon CloudFront distribution that uses the latency record as an origin.\nNLB(Network Load Balancer) 및 연결된 대상 그룹을 배포합니다 . 대상 그룹을 Auto Scaling 그룹과 연\n결합니다 . 각 NLB의 별칭을 가리키는 Amazon Route 53 지연 시간 레코드를 생성합니다 . 지연 시간 레\n코드를 오리진으로 사용하는 Amazon CloudFront 배포를 생성합니다 .\n​",
      "D": "Deploy an Application Load Balancer (ALB) and an associated target group. Associate the target \ngroup with the Auto Scaling group. Create an Amazon Route 53 weighted record that points to \naliases for each ALB. Deploy an Amazon CloudFront distribution that uses the weighted record as an \norigin.\nALB(Application Load Balancer) 및 연결된 대상 그룹을 배포합니다 . 대상 그룹을 Auto Scaling 그룹과 \n연결합니다 . 각 ALB의 별칭을 가리키는 Amazon Route 53 가중치 레코드를 생성합니다 . 가중 레코드를 \n오리진으로 사용하는 Amazon CloudFront 배포를 배포합니다 .\n​\n​"
    },
    "정답": "A"
  },
  {
    "문제번호": 30,
    "질문": "A development team runs monthly resource-intensive tests on its general purpose Amazon RDS for \nMySQL DB instance with Performance Insights enabled. The testing lasts for 48 hours once a month \nand is the only process that uses the database. The team wants to reduce the cost of running the \ntests without reducing the compute and memory attributes of the DB instance.\nWhich solution meets these requirements MOST cost-effectively?\n개발 팀은 성능 개선 도우미가 활성화된 MySQL DB 인스턴스용 범용 Amazon RDS에서 매월 리소스 \n집약적 테스트를 실행합니다 . 테스트는 한 달에 한 번 48시간 동안 지속되며 데이터베이스를 사용하는 \n유일한 프로세스입니다 . 팀은 DB 인스턴스의 컴퓨팅 및 메모리 속성을 줄이지 않고 테스트 실행 비용\n을 줄이려고 합니다 .\n어떤 솔루션이 이러한 요구 사항을 가장 비용 효율적으로 충족합니까 ?\n​",
    "보기": {
      "A": "Stop the DB instance when tests are completed. Restart the DB instance when required.\n테스트가 완료되면 DB 인스턴스를 중지합니다 . 필요한 경우 DB 인스턴스를 다시 시작합니다 .\n→ RDS 데이터베이스가 중지된 경우에도 스토리지 비용은 지불된다 .",
      "B": "Use an Auto Scaling policy with the DB instance to automatically scale when tests are completed.\nDB 인스턴스와 함께 Auto Scaling 정책을 사용하여 테스트가 완료되면 자동으로 확장합니다 .\n​",
      "C": "Create a snapshot when tests are completed. Terminate the DB instance and restore the snapshot \nwhen required.\n테스트가 완료되면 스냅샷을 만듭니다 . DB 인스턴스를 종료하고 필요한 경우 스냅샷을 복원합니다 .\n→ DB의 수동 스냅샷을 생성하고 S3 Standard 로 이동하고 필요한 경우 수동 스냅샷에서 복원에서 복\n원한다 .\n​\n▶오답",
      "D": "Modify the DB instance to a low-capacity instance when tests are completed. Modify the DB instance \nagain when required.\n테스트가 완료되면 DB 인스턴스를 저용량 인스턴스로 수정합니다 . 필요한 경우 DB 인스턴스를 다시 \n수정합니다 .\n​\n​"
    },
    "정답": "C"
  },
  {
    "문제번호": 31,
    "질문": "A company that hosts its web application on AWS wants to ensure all Amazon EC2 \ninstances. Amazon RDS DB instances. and Amazon Redshift clusters are configured with \ntags. The company wants to minimize the effort of configuring and operating this check.\nWhat should a solutions architect do to accomplish this?\nAWS에서 웹 애플리케이션을 호스팅하는 회사는 모든 Amazon EC2 인스턴스를 보장하기를 원합\n니다. Amazon RDS DB 인스턴스 . Amazon Redshift 클러스터는 태그로 구성됩니다 . 회사는 이 \n검사를 구성하고 운영하는 노력을 최소화하기를 원합니다 .\n솔루션 설계자는 이를 달성하기 위해 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Use AWS Config rules to define and detect resources that are not properly tagged.\nAWS Config 규칙을 사용하여 적절하게 태그가 지정되지 않은 리소스를 정의하고 감지합니다 .\n​\n→ AWS Config\n: AWS Config 는 AWS 리소스 구성을 측정, 감사 및 평가할 수 있는 서비스 . \n1. AWS 리소스 구성을 지속적으로 모니터링 및 기록하고 , 원하는 구성을 기준으로 기록된 구성을 자동\n으로 평가해 준다. \n2. 리소스 구성 및 리소스 변경 사항을 자동으로 평가하여 AWS 인프라 전반의 지속적인 규정 준수 및 \n자체 거버넌스를 보장할 수 있다.\n3. 규정 준수 감사, 보안 분석, 변경 관리 및 운영 문제 해결 작업을 간소화할 수 있다.",
      "B": "Use Cost Explorer to display resources that are not properly tagged. Tag those resources \nmanually.\n비용 탐색기를 사용하여 제대로 태그가 지정되지 않은 리소스를 표시합니다 . 해당 리소스에 수동\n으로 태그를 지정합니다 .\n​",
      "C": "Write API calls to check all resources for proper tag allocation. Periodically run the code \non an EC2 instance.\n적절한 태그 할당을 위해 모든 리소스를 확인하는 API 호출을 작성합니다 . EC2 인스턴스에서 \n주기적으로 코드를 실행합니다 .\n​",
      "D": "Write API calls to check all resources for proper tag allocation. Schedule an AWS Lambda \nfunction through Amazon CloudWatch to periodically run the code.\n적절한 태그 할당을 위해 모든 리소스를 확인하는 API 호출을 작성합니다 . Amazon CloudWatch\n를 통해 AWS Lambda 함수를 예약하여 코드를 주기적으로 실행합니다 .\n​\n​"
    },
    "정답": "A"
  },
  {
    "문제번호": 32,
    "질문": "A development team needs to host a website that will be accessed by other teams. The \nwebsite contents consist of HTML, CSS, client-side JavaScript, and images.\nWhich method is the MOST cost-effective for hosting the website?\n개발 팀은 다른 팀이 액세스할 웹사이트를 호스팅해야 합니다 . 웹사이트 콘텐츠는 HTML, CSS, \n클라이언트 측 JavaScript 및 이미지로 구성됩니다 .\n웹 사이트 호스팅에 가장 비용 효율적인 방법은 무엇입니까 ?\n​",
    "보기": {
      "A": "Containerize the website and host it in AWS Fargate.\n웹 사이트를 컨테이너화하고 AWS Fargate 에서 호스팅합니다 .\n​",
      "B": "Create an Amazon S3 bucket and host the website there.\nAmazon S3 버킷을 생성하고 거기에서 웹 사이트를 호스팅합니다 .\n​\n→ HTML, CSS, 클라이언트 측 JavaScript 및 이미지는 모두 정적 리소스\nAmazon S3을 사용하여 정적 웹 사이트를 호스팅할 수 있다. 정적 웹 사이트에서 개별 웹 페이\n지는 정적 콘텐츠를 포함한다 . 클라이언트 측 스크립트를 포함할 수도 있다.",
      "C": "Deploy a web server on an Amazon EC2 instance to host the website.\nAmazon EC2 인스턴스에 웹 서버를 배포하여 웹 사이트를 호스팅합니다 .\n​",
      "D": "Configure an Application Load Balancer with an AWS Lambda target that uses the \nExpress.js framework.\nExpress.js 프레임워크를 사용하는 AWS Lambda 대상으로 Application Load Balancer 를 구성합\n니다.\n​\n​"
    },
    "정답": "B"
  },
  {
    "문제번호": 33,
    "질문": "A company runs an online marketplace web application on AWS. The application serves \nhundreds of thousands of users during peak hours. The company needs a scalable, \nnear-real-time solution to share the details of millions of financial transactions with \nseveral other internal applications. Transactions also need to be processed to remove \nsensitive data before being stored in a document database for low-latency retrieval.\nWhat should a solutions architect recommend to meet these requirements?\n회사는 AWS에서 온라인 마켓플레이스 웹 애플리케이션을 실행합니다 . 이 애플리케이션은 피크 \n시간에 수십만 명의 사용자에게 서비스를 제공합니다 . 이 회사는 수백만 건의 금융 거래 세부 정\n보를 다른 여러 내부 애플리케이션과 공유할 수 있는 확장 가능한 거의 실시간 솔루션이 필요합\n니다. 또한 지연 시간이 짧은 검색을 위해 문서 데이터베이스에 저장하기 전에 민감한 데이터를 \n제거하기 위해 트랜잭션을 처리해야 합니다 .\n이러한 요구 사항을 충족하기 위해 솔루션 설계자는 무엇을 권장해야 합니까 ?\n​",
    "보기": {
      "A": "Store the transactions data into Amazon DynamoDB. Set up a rule in DynamoDB to remove \nsensitive data from every transaction upon write. Use DynamoDB Streams to share the \ntransactions data with other applications.\n트랜잭션 데이터를 Amazon DynamoDB 에 저장합니다 . 쓰기 시 모든 트랜잭션에서 민감한 데이\n터를 제거하도록 DynamoDB 에서 규칙을 설정합니다 . DynamoDB 스트림을 사용하여 다른 애플\n리케이션과 트랜잭션 데이터를 공유합니다 .\n​",
      "B": "Stream the transactions data into Amazon Kinesis Data Firehose to store data in Amazon \nDynamoDB and Amazon S3. Use AWS Lambda integration with Kinesis Data Firehose to \nremove sensitive data. Other applications can consume the data stored in Amazon S3.\n트랜잭션 데이터를 Amazon Kinesis Data Firehose 로 스트리밍하여 Amazon DynamoDB 및 \nAmazon S3에 데이터를 저장합니다 . Kinesis Data Firehose 와 AWS Lambda 통합을 사용하여 \n민감한 데이터를 제거하십시오 . 다른 애플리케이션은 Amazon S3에 저장된 데이터를 사용할 수 \n있습니다 .\n​",
      "C": "Stream the transactions data into Amazon Kinesis Data Streams. Use AWS Lambda \nintegration to remove sensitive data from every transaction and then store the transactions \ndata in Amazon DynamoDB. Other applications can consume the transactions data off the \nKinesis data stream.\n트랜잭션 데이터를 Amazon Kinesis Data Streams 로 스트리밍합니다 . AWS Lambda 통합을 사\n용하여 모든 트랜잭션에서 민감한 데이터를 제거한 다음 Amazon DynamoDB 에 트랜잭션 데이터\n를 저장합니다 . 다른 애플리케이션은 Kinesis 데이터 스트림의 트랜잭션 데이터를 사용할 수 있\n습니다 .\n→ DynamoDB 용 Amazon Kinesis Data Streams 를 사용하면 DynamoDB 테이블에서 Kinesis 데이터 스트림으로 \n데이터를 직접 보낼 수 있다. 또한 스트림에서 더 긴 데이터 보존 및 여러 동시 스트림 판독기로 팬아웃해야 하는 사용 \n사례에 이 기능을 활용할 수 있다.\n​\n▶오답\nAmazon Kinesis Data Firehose\n: * 스트리밍 ETL 솔루션입니다 . 스트리밍 데이터를 데이터 스토어와 분석 도구에 로드하는 가\n장 쉬운 방법이다 . 스트리밍 데이터를 캡처하고 변환한 후 Amazon S3, Amazon Redshift, \nAmazon OpenSearch Service 및 Splunk 로 로드하여 이미 사용하고 있는 기존 비즈니스 인텔리\n전스 도구 및 대시보드를 통해 거의 실시간으로 분석할 수 있다. \nAmazon Kinesis Data Firehose 는 데이터 처리량에 맞춰 자동으로 크기가 조정되며 지속적인 관\n리가 필요 없는 완전관리형 서비스입니다 . 또한, 데이터를 로드하기 전에 배치, 압축 및 암호화하\n여 대상 스토리지의 사용량을 최소화하고 보안을 강화할 수 있습니다 .\nKinesis Data Firehose 는 Kinesis Data Firehose 는 현재 Amazon S3, Amazon Redshift, Amazon \nOpenSearch Service, Splunk, Datadog, NewRelic, Dynatrace, Sumologic, LogicMonitor, MongoDB 및 \nHTTP End Point를 대상으로 지원하며 ,\nDynamoDB 에 데이터를 직접 저장할 수 없다.\n​\n* 스트리밍 ETL : 스트리밍 ETL이란 한 장소에서 다른 장소로 실제 데이터를 처리 및 이동하\n는 것.\n​",
      "D": "Store the batched transactions data in Amazon S3 as files. Use AWS Lambda to process \nevery file and remove sensitive data before updating the files in Amazon S3. The Lambda \nfunction then stores the data in Amazon DynamoDB. Other applications can consume \ntransaction files stored in Amazon S3.\n일괄 처리된 트랜잭션 데이터를 Amazon S3에 파일로 저장합니다 . Amazon S3에서 파일을 업데\n이트하기 전에 AWS Lambda 를 사용하여 모든 파일을 처리하고 민감한 데이터를 제거하십시오 . \n그러면 Lambda 함수가 Amazon DynamoDB 에 데이터를 저장합니다 . 다른 애플리케이션은 \nAmazon S3에 저장된 트랜잭션 파일을 사용할 수 있습니다 .\n​\n​"
    },
    "정답": "C"
  },
  {
    "문제번호": 34,
    "질문": "A company hosts its multi-tier applications on AWS. For compliance, governance, auditing, \nand security, the company must track configuration changes on its AWS resources and \nrecord a history of API calls made to these resources.\nWhat should a solutions architect do to meet these requirements?\n회사는 AWS에서 다중 계층 애플리케이션을 호스팅합니다 . 규정 준수, 거버넌스 , 감사 및 보안을 \n위해 회사는 AWS 리소스의 구성 변경 사항을 추적하고 이러한 리소스에 대한 API 호출 기록을 \n기록해야 합니다 .\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Use AWS CloudTrail to track configuration changes and AWS Config to record API calls.\nAWS CloudTrail 을 사용하여 구성 변경을 추적하고 AWS Config 를 사용하여 API 호출을 기록하\n십시오 .\n​",
      "B": "Use AWS Config to track configuration changes and AWS CloudTrail to record API calls.\nAWS Config 를 사용하여 구성 변경을 추적하고 AWS CloudTrail 을 사용하여 API 호출을 기록합\n니다.\n​\n→ CloudTrail - 사용자 활동 및 API 호출 기록을 추적. \nConfig - 태그 리소스의 구성 및 관계를 평가, 감사 및 평가.",
      "C": "Use AWS Config to track configuration changes and Amazon CloudWatch to record API \ncalls.\nAWS Config 를 사용하여 구성 변경을 추적하고 Amazon CloudWatch 를 사용하여 API 호출을 기\n록합니다 .\n​",
      "D": "Use AWS CloudTrail to track configuration changes and Amazon CloudWatch to record API \ncalls.\nAWS CloudTrail 을 사용하여 구성 변경을 추적하고 Amazon CloudWatch 를 사용하여 API 호출\n을 기록합니다 .\n​\n​"
    },
    "정답": "B"
  },
  {
    "문제번호": 35,
    "질문": "A company is preparing to launch a public-facing web application in the AWS Cloud. The \narchitecture consists of Amazon EC2 instances within a VPC behind an Elastic Load \nBalancer (ELB). A third-party service is used for the DNS. The company's solutions \narchitect must recommend a solution to detect and protect against large-scale DDoS \nattacks.\nWhich solution meets these requirements?\n한 회사가 AWS 클라우드에서 공개 웹 애플리케이션 출시를 준비하고 있습니다 . 아키텍처는 \nElastic Load Balancer(ELB) 뒤의 VPC 내 Amazon EC2 인스턴스로 구성됩니다 . DNS에는 타사 \n서비스가 사용됩니다 . 회사의 솔루션 설계자는 대규모 DDoS 공격을 감지하고 보호하기 위한 솔\n루션을 권장해야 합니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n​",
    "보기": {
      "A": "Enable Amazon GuardDuty on the account.\n계정에서 Amazon GuardDuty 를 활성화합니다 .\n​",
      "B": "Enable Amazon Inspector on the EC2 instances.\nEC2 인스턴스에서 Amazon Inspector 를 활성화합니다 .\n​",
      "C": "Enable AWS Shield and assign Amazon Route 53 to it.\nAWS Shield 를 활성화하고 여기에 Amazon Route 53을 할당합니다 .\n​",
      "D": "Enable AWS Shield Advanced and assign the ELB to it.\nAWS Shield Advanced 를 활성화하고 ELB를 할당합니다 .\n→ AWS Shield Advanced 는 Amazon EC2 인스턴스 , Elastic Load Balancing 로드 밸런서 , CloudFront \n배포, Route 53 호스팅 영역 및 AWS Global Accelerator 표준 액셀러레이터에 대한 확장된 DDoS 공\n격 보호 기능을 제공한다 .\n​\n​"
    },
    "정답": "D"
  },
  {
    "문제번호": 36,
    "질문": "A company is building an application in the AWS Cloud. The application will store data in \nAmazon S3 buckets in two AWS Regions. The company must use an AWS Key \nManagement Service (AWS KMS) customer managed key to encrypt all data that is stored \nin the S3 buckets. The data in both S3 buckets must be encrypted and decrypted with the \nsame KMS key. The data and the key must be stored in each of the two Regions.\nWhich solution will meet these requirements with the LEAST operational overhead?\n회사는 AWS 클라우드에서 애플리케이션을 구축하고 있습니다 . 애플리케이션은 두 AWS 리전의 \nAmazon S3 버킷에 데이터를 저장합니다 . 회사는 AWS Key Management Service(AWS KMS) \n고객 관리형 키를 사용하여 S3 버킷에 저장된 모든 데이터를 암호화해야 합니다 . 두 S3 버킷의 \n데이터는 동일한 KMS 키로 암호화 및 복호화해야 합니다 . 데이터와 키는 두 지역 각각에 저장AWS Shield 기능\nAWS Shield Standard AWS Shield Advanced\n모든 AWS 고객은 추가 비용 없이 AWS Shield \nStandard 에 의한 자동 보호를 받을 수 있습니다 . \nAWS Shield Standard 는 가장 일반적이고 빈번하게 발\n생하며 웹 사이트 또는 애플리케이션을 목표로 하는 네\n트워크 및 전송 계층 DDoS 공격으로부터 보호합니다 .\nAmazon CloudFront 및 Amazon Route 53과 함께 AWS \nShield Standard 를 사용하면 알려진 모든 인프라 (계층 3 \n및 4) 공격에 대해 포괄적인 가용성 보호를 받을 수 있\n습니다 .Amazon Elastic Compute Cloud(EC2), Elastic Load \nBalancing(ELB), Amazon CloudFront, AWS Global \nAccelerator 및 Amazon Route 53 리소스에서 실행되는 \n애플리케이션을 대상으로 하는 공격에 대해 더 높은 수\n준의 보호를 구현하려면 AWS Shield Advanced 를 구독\n하면 됩니다 . \nAWS Shield Standard 가 제공하는 네트워크 및 전송 계\n층 보호 이외에 , Shield Advanced 는 정교한 대규모 \nDDoS 공격에 대한 추가 보호 및 완화, 실시간에 가까운 \n공격에 대한 가시성 , 웹 애플리케이션 방화벽 AWS \nWAF와의 통합을 제공합니다 . \nShield Advanced 를 사용하면 AWS Shield Response \nTeam(SRT) 에 연중무휴로 액세스할 수 있으며 EC2, \nELB, CloudFront, Global Accelerator 및 Route 53 요\n금의 DDoS 관련 급증으로부터 보호할 수 있습니다 .\n되어야 합니다 .\n최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까 ?\n​",
    "보기": {
      "A": "Create an S3 bucket in each Region. Configure the S3 buckets to use server-side \nencryption with Amazon S3 managed encryption keys (SSE-S3). Configure replication \nbetween the S3 buckets.\n각 리전에서 S3 버킷을 생성합니다 . Amazon S3 관리형 암호화 키(SSE-S3) 와 함께 서버 측 암\n호화를 사용하도록 S3 버킷을 구성합니다 . S3 버킷 간의 복제를 구성합니다 .\n​",
      "B": "Create a customer managed multi-Region KMS key. Create an S3 bucket in each Region. \nConfigure replication between the S3 buckets. Configure the application to use the KMS key \nwith client-side encryption.\n고객 관리형 다중 리전 KMS 키를 생성합니다 . 각 리전에서 S3 버킷을 생성합니다 . S3 버킷 간\n의 복제를 구성합니다 . 클라이언트 측 암호화와 함께 KMS 키를 사용하도록 애플리케이션을 구\n성합니다 .\n​\n→ Amazon S3는 AWS Key Management Service(AWS KMS) 와 통합되어 Amazon S3 객체의 \n서버 측 암호화를 제공한다 . Amazon S3는 AWS KMS 키를 사용하여 Amazon S3 객체를 암호\n화한다 . 객체를 보호하는 암호화 키는 AWS KMS를 암호화되지 않은 상태로 두지 않는다 . 또한 \n이 통합을 통해 AWS KMS 키에 대한 권한을 설정하고 비밀을 보호하는 데이터 키를 생성, 암호\n화 및 해독하는 작업을 감사할 수 있다.\n​\n→ AWS KMS는 다중 리전 키 를 지원한다 . 이는 여러 리전에서 동일한 키를 가지고 있는 것처\n럼 서로 바꿔서 사용할 수 있는 서로 다른 AWS 리전의 AWS KMS 키이다 . 각 관련 다중 리전 \n키 세트에는 동일한 키 구성 요소 와 키 ID 가 있으므로 AWS KMS를 다시 암호화하거나 리전 \n간 호출을 수행하지 않고도 한 AWS 리전에서 데이터를 암호화하고 다른 AWS 리전에서 해독할 \n수 있다.\n다중 리전 키는 AWS 리전 간에 암호화된 데이터를 이동하거나 리전 간 액세스가 필요한 워크로\n드에 유연하고 확장 가능한 솔루션을 제공한다\n리전 간에 보호된 데이터를 공유, 이동 또는 백업해야 하거나 다른 리전에서 작동하는 애플리케\n이션의 동일한 디지털 서명을 생성해야 하는 경우 다중 리전 키를 사용할 수 있다.\n​\n​S3 버킷 데이터 암호화 방법 - 서버 측 암호화 (SSE)\nSSE-S3 SSE-KMS SSE-C\nSSE-S3 는 AWS에서 제공 / \n관리되는 키를 통해 데이터를 \n암호화하는 방법.\n버킷에 객체를 업로드할 때 헤\n더에 \n\"x-amz-server-side-encryption\": \n\"AES256\" 키-값 쌍을 추가해 업\n로드하면 자동으로 S3에서 제\n공되는 키를 통해 암호화 .SSE-KMS 는 AWS KMS(Key \nManagement Service) 를 통해 \n암호화용 키를 적용하는 방법.\nSSE-S3 와 유사하게 헤더에 \n\"x-amz-server-side-encryption\": \n\"aws:kms\" 키-값 쌍을 추가해 \n적용할 수 있으며 , KMS에서 \n관리하는 키는 사용자가 설정\n할 수 있고 누가 키를 사용했\n는지 등 기록 조회가 가능하다\n는장점이 존재.SSE-C 는 회사 또는 개인이 온\n프레미스 환경에서 자체적으로 \n관리하는 키를 통해 암호화하는 \n방법.\n특이한 점으로는 SSE-KMS 나 \nSSE-S3 와는 다르게 반드시 \nHTTPS 프로토콜을 통해 요청\n을 보내야 하며, 요청에는 사용\n자가 사용할 임의의 데이터 키\n를 포함해야함 .",
      "C": "Create a customer managed KMS key and an S3 bucket in each Region. Configure the S3 \nbuckets to use server-side encryption with Amazon S3 managed encryption keys \n(SSE-S3). Configure replication between the S3 buckets.\n각 리전에서 고객 관리형 KMS 키와 S3 버킷을 생성합니다 . Amazon S3 관리형 암호화 키\n(SSE-S3) 와 함께 서버 측 암호화를 사용하도록 S3 버킷을 구성합니다 . S3 버킷 간의 복제를 \n구성합니다 .\n​",
      "D": "Create a customer managed KMS key and an S3 bucket in each Region. Configure the S3 \nbuckets to use server-side encryption with AWS KMS keys (SSE-KMS). Configure \nreplication between the S3 buckets.\n각 리전에서 고객 관리형 KMS 키와 S3 버킷을 생성합니다 . AWS KMS 키(SSE-KMS) 로 서버 \n측 암호화를 사용하도록 S3 버킷을 구성합니다 . S3 버킷 간의 복제를 구성합니다 .\n​\n​"
    },
    "정답": "B"
  },
  {
    "문제번호": 37,
    "질문": "A company recently launched a variety of new workloads on Amazon EC2 instances in its \nAWS account. The company needs to create a strategy to access and administer the \ninstances remotely and securely. The company needs to implement a repeatable process \nthat works with native AWS services and follows the AWS Well-Architected Framework.\nWhich solution will meet these requirements with the LEAST operational overhead?\n한 회사는 최근 AWS 계정의 Amazon EC2 인스턴스에서 다양한 새로운 워크로드를 출시했습니\n다. 회사는 인스턴스에 원격으로 안전하게 액세스하고 관리하는 전략을 수립해야 합니다 . 회사는 \n기본 AWS 서비스와 함께 작동하고 AWS Well-Architected 프레임워크를 따르는 반복 가능한 \n프로세스를 구현해야 합니다 .\n최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까 ?\n​",
    "보기": {
      "A": "Use the EC2 serial console to directly access the terminal interface of each instance for \nadministration.\nEC2 직렬 콘솔을 사용하여 관리를 위해 각 인스턴스의 터미널 인터페이스에 직접 액세스합니다 .\n​",
      "B": "Attach the appropriate IAM role to each existing instance and new instance. Use AWS \nSystems Manager Session Manager to establish a remote SSH session.\n각 기존 인스턴스와 새 인스턴스에 적절한 IAM 역할을 연결합니다 . AWS Systems Manager \nSession Manager 를 사용하여 원격 SSH 세션을 설정합니다 .\n​\n→ IAM역할 연결은 기존 AWS 서비스와 함께 작동되며 운영 오버헤드 최소화 할 수 있어 정답\n에 더 적합함 .\n​\n▶오답\nBastion host (배스천 호스트 )\n: 침입 차단 소프트웨어가 설치되어 내부와 외부 네트워크 사이에서 일종의 게이트 역할을 수행\n하는 호스트 .\n기존 사용하던 PC에서 private subnet 안의 서버에 들어오게되면 보안에 취약하므로 Bastion \nHost에서 private subnet 으로 \n접속하는편이 더 안전하다 .\n​\nBastion Host에서 SSH pem키 인증 방법\n기본적으로 개인 인스턴스 및 Bastion 서버의 SSH는 인증에 개인 키를 사용하도록 구성됩니다 . 배스 \n천 호스트 인스턴스에서 다른 케이스에 연결할 때 여전히 개인 키가 필요합니다 . \n​",
      "C": "Create an administrative SSH key pair. Load the public key into each EC2 instance. Deploy \na *bastion host in a public subnet to provide a tunnel for administration of each instance.\n관리 SSH 키 쌍을 만듭니다 . 공개 키를 각 EC2 인스턴스에 로드합니다 . 퍼블릭 서브넷에 *배스\n천 호스트를 배포하여 각 인스턴스의 관리를 위한 터널을 제공합니다 .\n​",
      "D": "Establish an AWS Site-to-Site VPN connection. Instruct administrators to use their local \non-premises machines to connect directly to the instances by using SSH keys across the \nVPN tunnel.\nAWS Site-to-Site VPN 연결을 설정합니다 . 관리자에게 로컬 온프레미스 머신을 사용하여 \nVPN 터널에서 SSH 키를 사용하여 인스턴스에 직접 연결하도록 지시합니다 .\n​\n​"
    },
    "정답": "B"
  },
  {
    "문제번호": 38,
    "질문": "A company is hosting a static website on Amazon S3 and is using Amazon Route 53 for \nDNS. The website is experiencing increased demand from around the world. The company \nmust decrease latency for users who access the website.\nWhich solution meets these requirements MOST cost-effectively?\n회사는 Amazon S3에서 정적 웹 사이트를 호스팅하고 DNS에 Amazon Route 53을 사용하고 있\n습니다 . 웹 사이트는 전 세계적으로 수요가 증가하고 있습니다 . 회사는 웹 사이트에 액세스하는 \n사용자의 대기 시간을 줄여야합니다 .\n어떤 솔루션이 이러한 요구 사항을 가장 비용 효율적으로 충족합니까 ?\n​",
    "보기": {
      "A": "Replicate the S3 bucket that contains the website to all AWS Regions. Add Route 53 \ngeolocation routing entries.\n웹 사이트가 포함된 S3 버킷을 모든 AWS 리전에 복제합니다 . Route 53 지리적 위치 라우팅 항\n목을 추가합니다 .\n​",
      "B": "Provision accelerators in AWS Global Accelerator. Associate the supplied IP addresses with \nthe S3 bucket. Edit the Route 53 entries to point to the IP addresses of the accelerators.\nAWS Global Accelerator 에서 액셀러레이터를 프로비저닝합니다 . 제공된 IP 주소를 S3 버킷과 \n연결합니다 . 액셀러레이터의 IP 주소를 가리키도록 Route 53 항목을 편집합니다 .\n​",
      "C": "Add an Amazon CloudFront distribution in front of the S3 bucket. Edit the Route 53 entries \nto point to the CloudFront distribution.\nS3 버킷 앞에 Amazon CloudFront 배포를 추가합니다 . CloudFront 배포를 가리키도록 Route 53 \n항목을 편집합니다 .\n​\nAmazon CloudFront\n: CloudFront 는 AWS에서 제공하는 CDN(Content Delivery Network) 서비스이다 . CDN 서비스를 이용하면 서비스 대\n기 시간과 성능이 개선되어 이미지 , 오디오 , 비디오 및 일반 웹 페이지 등을 최종 사용자에게 빠르게 제공할 수 있다.\n​\nEC2나 S3의 데이터에 접근을 했을 때 CloudFront 서비스를 사용하지 않는다면 해당 리전에서 데이터를 직접 가져오므\n로 해당 리전이 멀리 떨어져 있다면 아무래도 비교적 지연 시간이 있을 수 밖에 없다.\nCloudFront 는 오리진 서버에 위치한 원본 파일을 전세계에 위치한 에지 로케이션으로 배포하고 에지 로케이션은 이 데\n이터를 캐싱하며 사용자는 자신의 위치와 가까운 에지 로케이션에서 캐싱된 데이터를 제공 받으므로 이런 문제를 어느 \n정도 해결할 수 있다. \n오리진 서버는 S3 버킷, EC2 혹은 ELB(Elastic Load Balancer) 와 같은 다른 AWS이거나 자체 오리진 서버일 수 있다.",
      "D": "Enable S3 Transfer Acceleration on the bucket. Edit the Route 53 entries to point to the \nnew endpoint.\n버킷에서 S3 Transfer Acceleration 을 활성화합니다 . 새 엔드포인트를 가리키도록 Route 53 항\n목을 편집합니다 .\n​\n​"
    },
    "정답": "C"
  },
  {
    "문제번호": 39,
    "질문": "A company maintains a searchable repository of items on its website. The data is stored in \nan Amazon RDS for MySQL database table that contains more than 10 million rows. The \ndatabase has 2 TB of General Purpose SSD storage. There are millions of updates against \nthis data every day through the company's website.\nThe company has noticed that some insert operations are taking 10 seconds or longer. The \ncompany has determined that the database storage performance is the problem.\nWhich solution addresses this performance issue?\n회사는 웹 사이트에서 검색 가능한 항목 저장소를 유지 관리합니다 . 데이터는 천만 개 이상의 행\n이 포함된 Amazon RDS for MySQL 데이터베이스 테이블에 저장됩니다 . 데이터베이스에는 2TB\n의 범용 SSD 스토리지가 있습니다 . 회사 웹 사이트를 통해 이 데이터에 대한 수백만 건의 업데\n이트가 매일 있습니다 .\n이 회사는 일부 삽입 작업이 10초 이상 걸리는 것을 확인했습니다 . 회사는 데이터베이스 스토리\n지 성능이 문제라고 판단했습니다 .\n이 성능 문제를 해결하는 솔루션은 무엇입니까 ?\n​",
    "보기": {
      "A": "Change the storage type to Provisioned IOPS SSD.\n스토리지 유형을 프로비저닝된 IOPS SSD로 변경합니다 .\n프로비저닝된 IOP SSD\n:프로비저닝된 IOPS 스토리지는 낮은 I/O 대기 시간과 일관된 I/O 처리량이 필요한 I/O 집약적 워크로\n드, 특히 데이터베이스 워크로드의 요구 사항을 충족하도록 설계되었다 .",
      "B": "Change the DB instance to a memory optimized instance class.\nDB 인스턴스를 메모리 최적화 인스턴스 클래스로 변경합니다 .\n​",
      "C": "Change the DB instance to a burstable performance instance class.\nDB 인스턴스를 버스트 가능한 성능 인스턴스 클래스로 변경합니다 .\n​",
      "D": "Enable Multi-AZ RDS read replicas with MySQL native asynchronous replication.\nMySQL 기본 비동기 복제로 다중 AZ RDS 읽기 전용 복제본을 활성화합니다 .\n​\n​"
    },
    "정답": "A"
  },
  {
    "문제번호": 40,
    "질문": "A company has thousands of edge devices that collectively generate 1 TB of status alerts \neach day. Each alert is approximately 2 KB in size. A solutions architect needs to \nimplement a solution to ingest and store the alerts for future analysis.\nThe company wants a highly available solution. However, the company needs to minimize Amazon RDS 스토리지 유형\n: 최대 64테비바이트 (TiB)의 스토리지로 MySQL, MariaDB, Oracle 및 PostgreSQL RDS DB 인\n스턴스를 생성할 수 있습니다 . SQL Server RDS DB 인스턴스는 스토리지의 최대 16TiB 까지 \n생성할 수 있다. \n범용 SSD 프로비저닝된 IOPS 마그네틱\n범용 SSD 스토리지는 비용 \n효율적이어서 대부분의 데이\n터베이스 워크로드에 적합하\n다. \n범용 SSD DB 인스턴스의 스\n토리지 크기 범위이다 .\n•MariaDB, MySQL, \nOracle 및 \nPostgreSQL 데이터\n베이스 인스턴스 : \n20GiB –64TiB\n•SQL Server \nEnterprise, Standard, \nWeb 및 Express \nEditions: \n20GiB~16TiB빠르고 일관적인 I/O 성능이 \n필요한 프로덕션 애플리케이\n션의 경우에는 프로비저닝된 \nIOPS( 초당 입/출력 연산 수) \n스토리지를 권장한다 . \n프로비저닝된 IOPS 스토리지\n는 성능이 예측 가능하며 , 일\n관적으로 지연 시간이 짧은 \n스토리지 유형이다 . \n프로비저닝된 IOPS 스토리지\n는 일관적인 성능이 필요한 \n온라인 트랜잭션 프로세싱\n(OLTP) 워크로드에 이상적일 \n뿐만 아니라 프로비저닝된 \nIOPS는 이런 워크로드의 성\n능 튜닝에도 효과적이다 .Amazon RDS는 이전 버전과\n의 호환성을 위해 마그네틱 \n스토리지를 지원한다 . \n새 스토리지가 필요할 경우 \n범용 SSD 또는 프로비저닝된 \nIOPS SSD를 사용하는 것이 \n좋다. \n마그네틱 스토리지의 제한 항\n목\n•SQL 서버 데이터베\n이스 엔진을 사용할 \n경우 스토리지를 확\n장할 수 없다.\n•스토리지 자동 조정\n을 지원하지 않는다 .\n•탄력적 볼륨을 지원\n하지 않는다 .\n•최대 3TiB 크기로 제\n한된다 .\n•최대 1,000IOPS 로 제\n한된다 .\ncosts and does not want to manage additional infrastructure. Additionally, the company \nwants to keep 14 days of data available for immediate analysis and archive any data older \nthan 14 days.\nWhat is the MOST operationally efficient solution that meets these requirements?\n회사에는 매일 1TB의 상태 알림을 집합적으로 생성하는 수천 개의 에지 장치가 있습니다 . 각 경\n고의 크기는 약 2KB입니다 . 솔루션 설계자는 향후 분석을 위해 경고를 수집하고 저장하는 솔루\n션을 구현해야 합니다 .\n회사는 고가용성 솔루션을 원합니다 . 그러나 회사는 비용을 최소화 ​해야 하며 추가 인프라 관리를 \n원하지 않습니다 . 또한 회사는 즉각적인 분석을 위해14일 동안의 데이터를 유지하고 14일이 지\n난 데이터를 보관하기를 원합니다 .\n이러한 요구 사항을 충족하는 가장 운영 효율성이 높은 솔루션은 무엇입니까 ?\n​",
    "보기": {
      "A": "Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the \nKinesis Data Firehose stream to deliver the alerts to an Amazon S3 bucket. Set up an S3 \nLifecycle configuration to transition data to Amazon S3 Glacier after 14 days.\nAmazon Kinesis Data Firehose 전송 스트림을 생성하여 알림을 수집합니다 . Amazon S3 버킷에 \n알림을 전달하도록 Kinesis Data Firehose 스트림을 구성합니다 . 14일 후에 데이터를 Amazon \nS3 Glacier 로 전환하도록 S3 수명 주기 구성을 설정합니다 .\n​\n→ Amazon Kinesis Data Firehose Firehose\n: 실시간 전송을 위한 완전관리형 서비스이다 . \n스트리밍 데이터 Amazon S3(Amazon S3), Amazon Redshift, Amazon OpenSearch Service, \nSplunk, Amazon OpenSearch Service, Splunk 및 모든 사용자 지정 HTTP 엔드포인트 또는 \nDatadog, Dynatrace, LogicMongoDB, New Relic 및 Sumo Logic을 포함하여 지원되는 서드 파티 \n서비스 제공업체가 소유한 HTTP 엔드포인트 Kinesis Data Firehose 파이어호스는 Kinesis 스트\n리밍 데이터 플랫폼의 일부이며 Kinesis Data Streams ,Kinesis Video Streams , 및 Amazon \nKinesis Data Analytics. \nKinesis Data Firehose Firehose 를 사용하면 애플리케이션을 쓰거나 리소스를 관리할 필요가 없\n다.",
      "B": "Launch Amazon EC2 instances across two Availability Zones and place them behind an \nElastic Load Balancer to ingest the alerts. Create a script on the EC2 instances that will \nstore the alerts in an Amazon S3 bucket. Set up an S3 Lifecycle configuration to transition \ndata to Amazon S3 Glacier after 14 days.\n두 가용 영역에서 Amazon EC2 인스턴스를 시작하고 Elastic Load Balancer 뒤에 배치하여 알림\n을 수집합니다 . Amazon S3 버킷에 경고를 저장할 EC2 인스턴스에 대한 스크립트를 생성합니다 . \n14일 후에 데이터를 Amazon S3 Glacier 로 전환하도록 S3 수명 주기 구성을 설정합니다 .\n​",
      "C": "Create an Amazon Kinesis Data Firehose delivery stream to ingest the alerts. Configure the \nKinesis Data Firehose stream to deliver the alerts to an Amazon OpenSearch Service \n(Amazon Elasticsearch Service) cluster. Set up the Amazon OpenSearch Service (Amazon \nElasticsearch Service) cluster to take manual snapshots every day and delete data from the \ncluster that is older than 14 days.\nAmazon Kinesis Data Firehose 전송 스트림을 생성하여 알림을 수집합니다 . Amazon \nOpenSearch Service(Amazon Elasticsearch Service) 클러스터에 알림을 전달하도록 Kinesis \nData Firehose 스트림을 구성합니다 . Amazon OpenSearch Service(Amazon Elasticsearch \nService) 클러스터를 설정하여 매일 수동 스냅샷을 만들고 클러스터에서 14일이 지난 데이터를 \n삭제합니다 .\n​",
      "D": "Create an Amazon Simple Queue Service (Amazon SQS) standard queue to ingest the \nalerts, and set the message retention period to 14 days. Configure consumers to poll the \nSQS queue, check the age of the message, and analyze the message data as needed. If the \nmessage is 14 days old, the consumer should copy the message to an Amazon S3 bucket \nand delete the message from the SQS queue.\nAmazon Simple Queue Service(Amazon SQS) 표준 대기열을 생성하여 알림을 수집하고 메시지 \n보존 기간을 14일로 설정합니다 . SQS 대기열을 폴링하고 , 메시지의 수명을 확인하고 , 필요에 따\n라 메시지 데이터를 분석하도록 소비자를 구성합니다 . 메시지가 14일이 지난 경우 소비자는 메\n시지를 Amazon S3 버킷에 복사하고 SQS 대기열에서 메시지를 삭제해야 합니다 .\n​\n​"
    },
    "정답": "A"
  },
  {
    "문제번호": 41,
    "질문": "A company's application integrates with multiple software-as-a-service (SaaS) sources for data \ncollection. The company runs Amazon EC2 instances to receive the data and to upload the data to \nan Amazon S3 bucket for analysis. The same EC2 instance that receives and uploads the data also \nsends a notification to the user when an upload is complete. The company has noticed slow \napplication performance and wants to improve the performance as much as possible.\nWhich solution will meet these requirements with the LEAST operational overhead?\n회사의 애플리케이션은 데이터 수집을 위해 여러 SaaS(Software-as-a-Service) 소스와 통합됩니다 . 이 \n회사는 Amazon EC2 인스턴스를 실행하여 데이터를 수신하고 분석을 위해 데이터를 Amazon S3 버킷\n에 업로드합니다 . 데이터를 수신하고 업로드하는 동일한 EC2 인스턴스도 업로드가 완료되면 사용자에\n게 알림을 보냅니다 . 회사는 느린 응용 프로그램 성능을 발견했으며 가능한 한 성능을 개선하려고 합니\n다.\n최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까 ?\n​",
    "보기": {
      "A": "Create an Auto Scaling group so that EC2 instances can scale out. Configure an S3 event \nnotification to send events to an Amazon Simple Notification Service (Amazon SNS) topic when the \nupload to the S3 bucket is complete.\nEC2 인스턴스가 확장할 수 있도록 Auto Scaling 그룹을 생성합니다 . S3 버킷에 대한 업로드가 완료되\n면 Amazon Simple Notification Service(Amazon SNS) 주제에 이벤트를 보내도록 S3 이벤트 알림을 구\n성합니다 .\n​",
      "B": "Create an Amazon AppFlow flow to transfer data between each SaaS source and the S3 bucket. \nConfigure an S3 event notification to send events to an Amazon Simple Notification Service (Amazon \nSNS) topic when the upload to the S3 bucket is complete.\nAmazon AppFlow 흐름을 생성하여 각 SaaS 소스와 S3 버킷 간에 데이터를 전송합니다 . S3 버킷에 대\n한 업로드가 완료되면 Amazon Simple Notification Service(Amazon SNS) 주제에 이벤트를 보내도록 \nS3 이벤트 알림을 구성합니다 .\n​\n→ Amazon AppFlow \n: Amazon AppFlow 는 클릭 몇 번으로 Salesforce, SAP, Zendesk, Slack 및 ServiceNow 와 같은 서비스\n형 소프트웨어 (SaaS) 애플리케이션과 Amazon S3 및 Amazon Redshift 와 같은 AWS 서비스 간에 데이\n터를 안전하게 전송할 수 있게 해 주는 완전관리형 통합 서비스이다 .\n​\nAmazon AppFlow 기능\n•AppFlow 를 사용하면 엔터프라이즈급 규모에 원하는 빈도로 즉 일정에 따라, 비즈니스 이벤트\n에 대한 응답으로 또는 온디맨드로 데이터 플로우를 실행할 수 있다.\n•필터링 및 검증과 같은 데이터 변환 기능을 구성하여 추가 단계 없이 플로우 자체의 일부로 \n바로 사용 가능한 풍부한 데이터를 생성할 수 있다.\n•AppFlow 는 이동 중의 데이터를 자동으로 암호화하며 사용자가 AWS PrivateLink 와 통합된 \nSaaS 애플리케이션을 위해 퍼블릭 인터넷상의 데이터 플로우를 제한할 수 있게 해 주므로 보\n안 위협에 대한 노출이 감소시킬 수 있다.",
      "C": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule for each SaaS source to send \noutput data. Configure the S3 bucket as the rule's target. Create a second EventBridge (Cloud \nWatch Events) rule to send events when the upload to the S3 bucket is complete. Configure an \nAmazon Simple Notification Service (Amazon SNS) topic as the second rule's target.\n각 SaaS 소스에 대해 Amazon EventBridge(Amazon CloudWatch Events) 규칙을 생성하여 출력 데이터\n를 보냅니다 . S3 버킷을 규칙의 대상으로 구성합니다 . S3 버킷에 업로드가 완료되면 이벤트를 전송하\n는 두 번째 EventBridge(Cloud Watch Events) 규칙을 생성합니다 . Amazon Simple Notification \nService(Amazon SNS) 주제를 두 번째 규칙의 대상으로 구성합니다 .\n​",
      "D": "Create a Docker container to use instead of an EC2 instance. Host the containerized application on \nAmazon Elastic Container Service (Amazon ECS). Configure Amazon CloudWatch Container Insights \nto send events to an Amazon Simple Notification Service (Amazon SNS) topic when the upload to \nthe S3 bucket is complete.\nEC2 인스턴스 대신 사용할 Docker 컨테이너를 생성합니다 . Amazon Elastic Container Service(Amazon \nECS)에서 컨테이너화된 애플리케이션을 호스팅합니다 . S3 버킷에 업로드가 완료되면 Amazon Simple \nNotification Service(Amazon SNS) 주제에 이벤트를 보내도록 Amazon CloudWatch Container Insights\n를 구성합니다 .\n​\n​"
    },
    "정답": "B"
  },
  {
    "문제번호": 42,
    "질문": "A company runs a highly available image-processing application on Amazon EC2 instances in a \nsingle VPC. The EC2 instances run inside several subnets across multiple Availability Zones. The \nEC2 instances do not communicate with each other. However, the EC2 instances download images \nfrom Amazon S3 and upload images to Amazon S3 through a single NAT gateway. The company is \nconcerned about data transfer charges.\nWhat is the MOST cost-effective way for the company to avoid Regional data transfer charges?\n회사는 단일 VPC의 Amazon EC2 인스턴스에서 고가용성 이미지 처리 애플리케이션을 실행합니다 . \nEC2 인스턴스는 여러 가용 영역의 여러 서브넷 내에서 실행됩니다 . EC2 인스턴스는 서로 통신하지 않\n습니다 . 그러나 EC2 인스턴스는 Amazon S3에서 이미지를 다운로드하고 단일 NAT 게이트웨이를 통해 \nAmazon S3에 이미지를 업로드 합니다 . 회사는 데이터 전송 요금에 대해 우려하고 있습니다 .\n회사가 지역 데이터 전송 요금을 피할 수 있는 가장 비용 효율적인 방법은 무엇입니까 ?\n​",
    "보기": {
      "C": "Deploy a gateway VPC endpoint for Amazon S3.\nAmazon S3용 게이트웨이 VPC 엔드포인트를 배포합니다 .\n​\n→ VPC 게이트웨이 엔드 포인트를 배포하면 데이터는 AWS 네트워크를 통해서만 전송된다 .\n​",
      "A": "Launch the NAT gateway in each Availability Zone.\n각 가용 영역에서 NAT 게이트웨이를 시작합니다 .\n​",
      "B": "Replace the NAT gateway with a NAT instance.\nNAT 게이트웨이를 NAT 인스턴스로 교체합니다 .\n​",
      "D": "Provision an EC2 Dedicated Host to run the EC2 instances.\nEC2 인스턴스를 실행할 EC2 전용 호스트를 프로비저닝합니다 .\n​\n​"
    },
    "정답": "C"
  },
  {
    "문제번호": 43,
    "질문": "A company has an on-premises application that generates a large amount of time-sensitive data that \nis backed up to Amazon S3. The application has grown and there are user complaints about internet \nbandwidth limitations. A solutions architect needs to design a long-term solution that allows for both \ntimely backups to Amazon S3 and with minimal impact on internet connectivity for internal users.\nWhich solution meets these requirements?\n회사에 Amazon S3에 백업되는 시간에 민감한 대량의 데이터를 생성하는 온프레미스 애플리케이션이 \n있습니다 . 애플리케이션이 성장했고 인터넷 대역폭 제한에 대한 사용자 불만이 있습니다 . 솔루션 설계\n자는 Amazon S3에 대한 적시 백업을 허용하고 내부 사용자의 인터넷 연결에 미치는 영향을 최소화 하\n는 장기 솔루션 을 설계해야 합니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n​",
    "보기": {
      "A": "Establish AWS VPN connections and proxy all traffic through a VPC gateway endpoint.\nAWS VPN 연결을 설정하고 VPC 게이트웨이 엔드포인트를 통해 모든 트래픽을 프록시합니다 .\n→ VPN도 인터넷을 통과하고 대역폭을 사용한다 .\n​",
      "B": "Establish a new AWS Direct Connect connection and direct backup traffic through this new \nconnection.\n새 AWS Direct Connect 연결을 설정하고 이 새 연결을 통해 백업 트래픽을 직접 연결합니다 .\n→ 직접연결은 온프레미스와 AWS간의 전용 연결이며 , 이는 인터넷 연결처럼 늘어나거나 줄어들지 않\n는\n안정적인 네트워크 연결을 보장하는 방법이다 .\n​\n▶오답",
      "C": "Order daily AWS Snowball devices. Load the data onto the Snowball devices and return the devices \nto AWS each day.\n매일 AWS Snowball 디바이스를 주문합니다 . Snowball 디바이스에 데이터를 로드하고 디바이스를 매일 \nAWS로 반환합니다 .\n→ 매일 Snowball 전송은 비용과 효율성 측면에서 장기적인 솔루션이라 볼 수 없다.\n​",
      "D": "Submit a support ticket through the AWS Management Console. Request the removal of S3 service \nlimits from the account.\nAWS Management 콘솔을 통해 지원 티켓을 제출합니다 . 계정에서 S3 서비스 제한 제거를 요청합니다 .\n→ S3 제한은 여기에서 아무것도 변경하지 않는것이다 .\n​"
    },
    "정답": "B"
  },
  {
    "문제번호": 44,
    "질문": "A company has an Amazon S3 bucket that contains critical data. The company must protect the \ndata from accidental deletion.\nWhich combination of steps should a solutions architect take to meet these requirements? (Choose \ntwo.)\n회사에 중요한 데이터가 포함된 Amazon S3 버킷이 있습니다 . 회사는 우발적인 삭제로부터 데이터를 \n보호해야 합니다 .\n이러한 요구 사항을 충족하기 위해 솔루션 설계자는 어떤 단계 조합을 취해야 합니까 ? (2개를 선택하\n세요.)\n​",
    "보기": {
      "A": "Enable versioning on the S3 bucket.\nS3 버킷에서 버전 관리를 활성화합니다 .\n→ S3 버전 관리를 사용하면 한 버킷 내에 여러 개의 객체 버전을 유지할 수 있다. \n​",
      "B": "Enable MFA Delete on the S3 bucket.\nS3 버킷에서 MFA 삭제를 활성화합니다 .\n→ Amazon S3 버킷에서 S3 버전 관리를 사용하는 경우 선택적으로 MFA(멀티 팩터 인증) Delete 를 사\n용 설정하도록 버킷을 구성하여 \n다른 보안 계층을 추가할 수 있다. \n​\nS3 버킷에서 버전 관리 사용\n: Amazon S3의 버전 관리는 동일 버킷 내에 여러 개의 객체 변형을 보유하는 수단이다 . \n•버킷에 저장된 모든 버전의 객체를 모두 보존, 검색 및 복원\n•의도치 않은 사용자 작업 및 애플리케이션 장애로부터 더 쉽게 복구\n•Amazon S3가 동일한 객체에 대해 여러 쓰기 요청을 동시에 수신하는 경우 모든 객체가 저장\n​\nAmazon S3 버킷에서 삭제되었거나 누락된 객체를 감사하는 방법\n→ S3 객체가 어떻게 삭제되었는지 알아보는 방법 - 서버 액세스 로그 , AWS CloudTrail 로그 검토\n​\nAmazon S3버킷에서 실수로 인한 객체 삭제 방지 및 완화 기능\n•버전 관리를 활성화하여 객체의 이전 버전을 유지한다 .\n•객체의 교차 리전 복제를 활성화한다 .\n•객체 버전을 삭제할 때, Multi-Factor Authentication(MFA) 을 요구하려면 MFA 삭제를 활성화 \n한다.",
      "C": "Create a bucket policy on the S3 bucket.\nS3 버킷에 버킷 정책을 생성합니다 .\n​",
      "D": "Enable default encryption on the S3 bucket.\nS3 버킷에서 기본 암호화를 활성화합니다 .\n​\nE.\nCreate a lifecycle policy for the objects in the S3 bucket.\nS3 버킷의 객체에 대한 수명 주기 정책을 생성합니다 .\n​\n​"
    },
    "정답": "A"
  },
  {
    "문제번호": 45,
    "질문": "A company has a data ingestion workflow that consists of the following:\n• An Amazon Simple Notification Service (Amazon SNS) topic for notifications about new data \ndeliveries\n• An AWS Lambda function to process the data and record metadata\nThe company observes that the ingestion workflow fails occasionally because of network connectivity \nissues. When such a failure occurs, the Lambda function does not ingest the corresponding data \nunless the company manually reruns the job.\nWhich combination of actions should a solutions architect take to ensure that the Lambda function \ningests all data in the future? (Choose two.)\n회사에 다음으로 구성된 데이터 수집 워크플로가 있습니다 .\n• 새 데이터 전송에 대한 알림을 위한 Amazon Simple Notification Service(Amazon SNS) 주제\n• 데이터를 처리하고 메타데이터를 기록하는 AWS Lambda 함수\n회사에서 수집 워크플로가 실패하는 것을 관찰합니다 . 때때로 네트워크 연결 문제로 인해. 이러한 장애\n가 발생하면 회사에서 수동으로 작업을 다시 실행하지 않는 한 Lambda 함수는 해당 데이터를 수집하\n지 않습니다 .\nLambda 함수가 향후 모든 데이터를 수집하도록 하려면 솔루션 설계자가 취해야 하는 작업 조합은 무\n엇입니까 ? (2개를 선택하세요 .)\n​",
    "보기": {
      "A": "Deploy the Lambda function in multiple Availability Zones.\n여러 가용 영역에 Lambda 함수를 배포합니다 .\n→ Lambda 는 서버리스 서비스이므로 다중AZ일 필요가 없다.",
      "B": "Create an Amazon Simple Queue Service (Amazon SQS) queue, and subscribe it to the SNS topic.\nAmazon Simple Queue Service(Amazon SQS) 대기열을 생성하고 SNS 주제를 구독합니다 .\n​\nE.\nModify the Lambda function to read from an Amazon Simple Queue Service (Amazon SQS) queue.\nAmazon Simple Queue Service(Amazon SQS) 대기열에서 읽도록 Lambda 함수를 수정합니다 .\n​\n→ Lambda 는 자체적으로 고가용성과 확장성을 제공하는 완전 관리형 서비스이며 , SQS 대기열을 읽고 \n손실이 없게한다 .\n​\n▶오답",
      "C": "Increase the CPU and memory that are allocated to the Lambda function.\nLambda 함수에 할당된 CPU와 메모리를 늘립니다 .\n​",
      "D": "Increase provisioned throughput for the Lambda function.\nLambda 함수에 대해 프로비저닝된 처리량을 늘립니다 .\n​\nE.\nModify the Lambda function to read from an Amazon Simple Queue Service (Amazon SQS) queue.\nAmazon Simple Queue Service(Amazon SQS) 대기열에서 읽도록 Lambda 함수를 수정합니다 .\n​\n​"
    },
    "정답": "B"
  },
  {
    "문제번호": 46,
    "질문": "A company has an application that provides marketing services to stores. The services are based on \nprevious purchases by store customers. The stores upload transaction data to the company through \nSFTP, and the data is processed and analyzed to generate new marketing offers. Some of the files \ncan exceed 200 GB in size.\nRecently, the company discovered that some of the stores have uploaded files that contain \npersonally identifiable information (PII) that should not have been included. The company wants \nadministrators to be alerted if PII is shared again. The company also wants to automate remediation.\nWhat should a solutions architect do to meet these requirements with the LEAST development effort?\n회사에 매장에 마케팅 서비스를 제공하는 애플리케이션이 있습니다 . 서비스는 매장 고객의 이전 구매를 \n기반으로 합니다 . 상점은 SFTP 를 통해 거래 데이터를 회사에 업로드하고 데이터를 처리 및 분석하여 \n새로운 마케팅 제안을 생성합니다 . 일부 파일의 크기는 200GB 를 초과할 수 있습니다 .\n최근에 회사는 일부 상점에서 포함되어서는 안 되는 개인 식별 정보(PII)가 포함된 파일을 업로드 했음을 \n발견했습니다 . 회사는 PII가 다시 공유될 경우 관리자에게 경고를 주기를 원합니다 . 회사는 또한 문제 \n해결을 자동화 하기를 원합니다 .\n최소한의 개발 노력으로 이러한 요구 사항을 충족하기 위해 솔루션 설계자는 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Use an Amazon S3 bucket as a secure transfer point. Use Amazon Inspector to scan the objects in \nthe bucket. If objects contain PII, trigger an S3 Lifecycle policy to remove the objects that contain \nPII.\nAmazon S3 버킷을 보안 전송 지점으로 사용하십시오 . Amazon Inspector 를 사용하여 버킷의 객체를 \n스캔합니다 . 객체에 PII가 포함된 경우 S3 수명 주기 정책을 트리거하여 PII가 포함된 객체를 제거합니\n다.\n​",
      "B": "Use an Amazon S3 bucket as a secure transfer point. Use Amazon Macie to scan the objects in the \nbucket. If objects contain PII, use Amazon Simple Notification Service (Amazon SNS) to trigger a \nnotification to the administrators to remove the objects that contain PII.\nAmazon S3 버킷을 보안 전송 지점으로 사용합니다 . Amazon Macie 를 사용하여 버킷의 객체를 스캔합\n니다. 객체에 PII가 포함된 경우 Amazon Simple Notification Service(Amazon SNS)를 사용하여 관리자\n에게 PII가 포함된 객체를 제거하라는 알림을 트리거합니다 .\n​\nAmazon Macie\n: Amazon Macie 는 완전관리형 데이터 보안 및 데이터 프라이버시 서비스로 , 기계 학습 및 패턴 일치를 \n활용하여 AWS에서 민감한 데이터를 검색하고 보호한다 .\n​\nAmazon Macie 주요 이점\n•기계 학습 및 패턴 일치를 사용하여 대규모의 민감한 데이터를 비용 효율적으로 검색\n•이름, 주소 및 신용 카드 번호와 같은 개인 식별 정보(PII)를 포함하여 대규모의 점점 증가하는 \n민감한 데이터 유형 목록을 자동으로 감지\n•Amazon S3에 저장된 데이터에 대한 데이터 보안 및 데이터 프라이버시의 지속적인 가시성을 \n제공\n•AWS Management Console 에서 한 번의 클릭 또는 단일 API 호출로 쉽게 설정\n•AWS Organizations 를 사용하여 다중 계정 지원을 제공하므로 몇 번의 클릭만으로 모든 계정\n에서 Macie 를 활성화할 수 있음\n​",
      "C": "Implement custom scanning algorithms in an AWS Lambda function. Trigger the function when \nobjects are loaded into the bucket. If objects contain PII, use Amazon Simple Notification Service \n(Amazon SNS) to trigger a notification to the administrators to remove the objects that contain PII.\nAWS Lambda 함수에서 사용자 지정 스캔 알고리즘을 구현합니다 . 객체가 버킷에 로드될 때 함수를 트\n리거합니다 . 객체에 PII가 포함된 경우 Amazon Simple Notification Service(Amazon SNS)를 사용하여 \n관리자에게 PII가 포함된 객체를 제거하라는 알림을 트리거합니다 .\n​",
      "D": "Implement custom scanning algorithms in an AWS Lambda function. Trigger the function when \nobjects are loaded into the bucket. If objects contain PII, use Amazon Simple Email Service (Amazon \nSES) to trigger a notification to the administrators and trigger an S3 Lifecycle policy to remove the \nmeats that contain PII.\nAWS Lambda 함수에서 사용자 지정 스캔 알고리즘을 구현합니다 . 객체가 버킷에 로드될 때 함수를 트\n리거합니다 . 객체에 PII가 포함된 경우 Amazon Simple Email Service(Amazon SES)를 사용하여 관리자\n에게 알림을 트리거하고 S3 수명 주기 정책을 트리거하여 PII가 포함된 고기를 제거합니다 .\n​\n​"
    },
    "정답": "B"
  },
  {
    "문제번호": 47,
    "질문": "A company needs guaranteed Amazon EC2 capacity in three specific Availability Zones in a specific \nAWS Region for an upcoming event that will last 1 week.\nWhat should the company do to guarantee the EC2 capacity?\n회사는 1주일 동안 진행될 예정된 이벤트를 위해 특정 AWS 리전의 3개의 특정 가용 영역에서 보장된 \nAmazon EC2 용량이 필요합니다 .\nEC2 용량을 보장하기 위해 회사는 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Purchase Reserved Instances that specify the Region needed.\n필요한 리전을 지정하는 예약 인스턴스를 구매합니다 .\n​",
      "B": "Create an On-Demand Capacity Reservation that specifies the Region needed.\n필요한 지역을 지정하는 온디맨드 용량 예약을 생성합니다 .\n​",
      "C": "Purchase Reserved Instances that specify the Region and three Availability Zones needed.\n필요한 리전과 3개의 가용 영역을 지정하는 예약 인스턴스를 구매합니다 .\n​",
      "D": "Create an On-Demand Capacity Reservation that specifies the Region and three Availability Zones \nneeded.\n필요한 지역과 3개의 가용 영역을 지정하는 온디맨드 용량 예약을 생성합니다 .\n→ 이벤트 동안 특정 AZ에 대한 주문형 용량 예약이 올바른 선택이다 .\n​\n▶오답\n: 예약 인스턴스는 장기용이므로 온디맨드가 정답이다 ."
    },
    "정답": "D"
  },
  {
    "문제번호": 48,
    "질문": "A company's website uses an Amazon EC2 instance store for its catalog of items. The company \nwants to make sure that the catalog is highly available and that the catalog is stored in a durable \nlocation.\nWhat should a solutions architect do to meet these requirements?\n회사 웹 사이트는 항목 카탈로그에 Amazon EC2 인스턴스 스토어를 사용합니다 . 회사는 카탈로그의 가\n용성이 높고 카탈로그가 내구성 있는 위치에 저장되기를 원합니다 .\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Move the catalog to Amazon ElastiCache for Redis.\n카탈로그를 Redis 용 Amazon ElastiCache 로 이동합니다 .\n​",
      "B": "Deploy a larger EC2 instance with a larger instance store.\n더 큰 인스턴스 스토어로 더 큰 EC2 인스턴스를 배포합니다 .\n→ 임시 스토리지\n​",
      "C": "Move the catalog from the instance store to Amazon S3 Glacier Deep Archive.\n인스턴스 스토어에서 Amazon S3 Glacier Deep Archive 로 카탈로그를 이동합니다 .\n→ 영구적 사용",
      "D": "Move the catalog to an Amazon Elastic File System (Amazon EFS) file system.\n카탈로그를 Amazon Elastic File System(Amazon EFS) 파일 시스템으로 이동합니다 .\n​\nAmazon Elastic File System(Amazon EFS) \n: 지정된 연도 동안 99.999999999%( 엘레븐 나인)의 내구성을 제공하도록 설계되었다 . 또한 EFS \nStandard 및 EFS Standard-IA 스토리지 클래스는 전체 가용 영역이 손실되는 경우에도 데이터를 보존\n할 수 있도록 설계되었다 . \n​\n▶오답"
    },
    "정답": "D"
  },
  {
    "문제번호": 49,
    "질문": "A company stores call transcript files on a monthly basis. Users access the files randomly within 1 \nyear of the call, but users access the files infrequently after 1 year. The company wants to optimize \nits solution by giving users the ability to query and retrieve files that are less than 1-year-old as \nquickly as possible. A delay in retrieving older files is acceptable.\nWhich solution will meet these requirements MOST cost-effectively?\n회사는 매월 통화 기록 파일을 저장합니다 . 사용자는 통화 후 1년 이내에 파일에 무작위로 액세스하지\n만 1년 이후에는 파일에 자주 액세스하지 않습니다 . 이 회사는 사용자에게 1년 미만의 파일을 가능한 \n한 빨리 쿼리하고 검색할 수 있는 기능을 제공하여 솔루션을 최적화하려고 합니다 . 오래된 파일을 검색\n하는 데 있어 지연은 허용됩니다 .\n어떤 솔루션이 이러한 요구 사항을 가장 비용 효율적 으로 충족합니까 ?\n​",
    "보기": {
      "A": "Store individual files with tags in Amazon S3 Glacier Instant Retrieval. Query the tags to retrieve the \nfiles from S3 Glacier Instant Retrieval.\nAmazon S3 Glacier Instant Retrieval 에 태그가 있는 개별 파일을 저장합니다 . 태그를 쿼리하여 S3 \nGlacier Instant Retrieval 에서 파일을 검색합니다 .\n​",
      "B": "Store individual files in Amazon S3 Intelligent-Tiering. Use S3 Lifecycle policies to move the files to \nS3 Glacier Flexible Retrieval after 1 year. Query and retrieve the files that are in Amazon S3 by \nusing Amazon Athena. Query and retrieve the files that are in S3 Glacier by using S3 Glacier \nSelect.\nAmazon S3 Intelligent-Tiering 에 개별 파일을 저장합니다 . S3 수명 주기 정책을 사용하여 1년 후 파일\n을 S3 Glacier Flexible Retrieval 로 이동합니다 . Amazon Athena 를 사용하여 Amazon S3에 있는 파일을 \n쿼리하고 검색합니다 . S3 Glacier Select 를 사용하여 S3 Glacier 에 있는 파일을 쿼리하고 검색합니다 .\n​\nS3 Intelligent-Tiering\n: 개체 크기나 보존 기간에 관계없이 알 수 없거나 변경되거나 예측할 수 없는 액세스 패턴이 있는 데\n이터에 이상적인 스토리지 클래스이다 . S3 Intelligent-Tiering 을 거의 모든 워크로드 , 특히 데이터 레이\n크, 데이터 분석, 새로운 애플리케이션 및 사용자 생성 콘텐츠에 대한 기본 스토리지 클래스로 사용할 \n수 있다. \n​\nAmazon S3 Glacier Flexible Retrieval(Amazon S3 Glacier 스토리지 클래스 )\n: 비동기로 검색되는 아카이브 데이터에 대한 저렴한 비용의 스토리지를 제공한다 . \n몇 분 정도에서 몇 시간까지 다양한 액세스 시간에서 비용의 균형을 조정하는 대부분의 검색 속도 옵\n션과 무료 대량 검색 기능을 제공한다 .\n이는 가끔 몇 분 안에 일부 데이터를 검색해야 하는 경우, 그리고 백업, 재해 복구, 오프사이트 데이터 \n스토리지 요구 사항에 대해 적합한 솔루션이다 .\nS3 Glacier Flexible Retrieval 은 AWS 리전에 있는 최소 3개의 AWS 가용 영역에서 여러 디바이스에 객\n체를 중복 저장하여 99.999999999% 의 데이터 내구성을 지원하도록 설계되었다 .",
      "C": "Store individual files with tags in Amazon S3 Standard storage. Store search metadata for each \narchive in Amazon S3 Standard storage. Use S3 Lifecycle policies to move the files to S3 Glacier \nInstant Retrieval after 1 year. Query and retrieve the files by searching for metadata from Amazon \nS3.\nAmazon S3 Standard 스토리지에 태그가 있는 개별 파일을 저장합니다 . Amazon S3 Standard 스토리\n지의 각 아카이브에 대한 검색 메타데이터를 저장합니다 . S3 수명 주기 정책을 사용하여 1년 후에 파\n일을 S3 Glacier Instant Retrieval 로 이동합니다 . Amazon S3에서 메타데이터를 검색하여 파일을 쿼리\n하고 검색합니다 .\n​",
      "D": "Store individual files in Amazon S3 Standard storage. Use S3 Lifecycle policies to move the files to \nS3 Glacier Deep Archive after 1 year. Store search metadata in Amazon RDS. Query the files from \nAmazon RDS. Retrieve the files from S3 Glacier Deep Archive.\nAmazon S3 Standard 스토리지에 개별 파일을 저장합니다 . S3 수명 주기 정책을 사용하여 1년 후에 \n파일을 S3 Glacier Deep Archive 로 이동합니다 . Amazon RDS에 검색 메타데이터를 저장합니다 . \nAmazon RDS에서 파일을 쿼리합니다 . S3 Glacier Deep Archive 에서 파일을 검색합니다 .\n​\n​"
    },
    "정답": "B"
  },
  {
    "문제번호": 50,
    "질문": "A company has a production workload that runs on 1,000 Amazon EC2 Linux instances. The \nworkload is powered by third-party software. The company needs to patch the third-party software on \nall EC2 instances as quickly as possible to remediate a critical security vulnerability.\nWhat should a solutions architect do to meet these requirements?\n회사에 1,000개의 Amazon EC2 Linux 인스턴스에서 실행되는 프로덕션 워크로드가 있습니다 . 워크로드\n는 타사 소프트웨어에 의해 구동됩니다 . 회사는 중요한 보안 취약성을 수정하기 위해 가능한 한 빨리 \n모든 EC2 인스턴스에서 타사 소프트웨어를 패치해야 합니다 .\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Create an AWS Lambda function to apply the patch to all EC2 instances.\nAWS Lambda 함수를 생성하여 모든 EC2 인스턴스에 패치를 적용합니다 .\n​",
      "B": "Configure AWS Systems Manager Patch Manager to apply the patch to all EC2 instances.\n모든 EC2 인스턴스에 패치를 적용하도록 AWS Systems Manager Patch Manager 를 구성합니다 .\n​",
      "C": "Schedule an AWS Systems Manager maintenance window to apply the patch to all EC2 instances.\nAWS Systems Manager 유지 관리 기간을 예약하여 모든 EC2 인스턴스에 패치를 적용합니다 .\n→ 가능한 빨리라는 조건이 있으므로 최적이 될 수 없다.\n​\nAWS Systems Manager Patch Manager\n: AWS Systems Manager 의 기능인 패치 관리자는 패치 설치 후의 재부팅을 나중으로 연기하는 옵션을 \n제공한다 . 패치 적용 작업 중 인스턴스에서 실행 중인 애플리케이션 또는 프로세스를 중단할 수 없는 \n경우 이 옵션을 선택할 수 있다. \n→ 타사 소프트웨어를 경로 지정해야 하고 Systems Manager Patch Manager 는 OS 소유 소프트웨어에\n서만 작동하기 때문에 \n정답이 아니다 .\n​",
      "D": "Use AWS Systems Manager Run Command to run a custom command that applies the patch to all \nEC2 instances.\nAWS Systems Manager Run Command 를 사용하여 모든 EC2 인스턴스에 패치를 적용하는 사용자 지\n정 명령을 실행합니다 .\n​\n→ AWS Systems Manager\n: 대규모 인프라 관리 작업을 단순화할 수 있는 플랫폼 서비스 .\n​\nAWS Systems Manager Run Command\n: AWS Systems Manager 의 기능인 Run Command 를 사용하여 관리형 노드의 구성을 원격으로 안전하\n게 관리할 수 있다. \nRun Command 를 사용하면 일반적인 관리 태스크를 자동화하고 대규모로 일회성 구성 변경을 수행할 \n수 있다. AWS Management Console, AWS Command Line Interface(AWS CLI), AWS Tools for \nWindows PowerShell 또는 AWS SDK에서 Run Command 를 사용할 수 있다.\n​\n​\n▶오답"
    },
    "정답": "D"
  },
  {
    "문제번호": 51,
    "질문": "A company is developing an application that provides order shipping statistics for retrieval by a \nREST API. The company wants to extract the shipping statistics, organize the data into an \neasy-to-read HTML format, and send the report to several email addresses at the same time every \nmorning.\nWhich combination of steps should a solutions architect take to meet these requirements? (Choose \ntwo.)\n회사는 REST API로 검색하기 위해 주문 배송 통계를 제공하는 애플리케이션을 개발 중입니다 . 이 회\n사는 배송 통계를 추출하고 데이터를 읽기 쉬운 HTML 형식으로 구성하고 매일 아침 여러 이메일 주소\n로 보고서를 보내려고 합니다 .\n이러한 요구 사항을 충족하기 위해 솔루션 설계자는 어떤 단계 조합을 취해야 합니까 ? (2개를 선택하\n세요.)\n​",
    "보기": {
      "A": "Configure the application to send the data to Amazon Kinesis Data Firehose.\n데이터를 Amazon Kinesis Data Firehose 로 보내도록 애플리케이션을 구성합니다 .\n​",
      "B": "Use Amazon Simple Email Service (Amazon SES) to format the data and to send the report by \nemail.\nAmazon Simple Email Service(Amazon SES)를 사용하여 데이터 형식을 지정하고 보고서를 이메일로 \n보냅니다 .\n​\n→ AWS Management Console 을(를) 사용하거나 직접 애플리케이션을 통해 또는 AWS SDK, AWS \nCommand Line Interface 또는 AWS Tools for Windows PowerShell 을(를) 통해 간접적으로 Amazon \nSES API를 호출하여 서식이 지정된 이메일을 보낼 수 있다.\nAmazon SES API는 SendEmail 작업을 통해 서식이 지정된 이메일을 작성하고 전송할 수 있다. \nSendEmail 에는 보낸 사람: 주소, 받는 사람: 주소, 메시지 제목 및 메시지 본문(텍스트 , HTML 또는 둘 \n다)이 필요하다 . \n​",
      "C": "Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS \nGlue job to query the application's API for the data.\nAWS Glue 작업을 호출하여 데이터에 대한 애플리케이션의 API를 쿼리하는 Amazon \nEventBridge(Amazon CloudWatch Events) 예약 이벤트를 생성합니다 .\n​",
      "D": "Create an Amazon EventBridge (Amazon CloudWatch Events) scheduled event that invokes an AWS \nLambda function to query the application's API for the data.\nAWS Lambda 함수를 호출하여 데이터에 대한 애플리케이션의 API를 쿼리하는 Amazon \nEventBridge(Amazon CloudWatch Events) 예약 이벤트를 생성합니다 .\n​\n→ Amazon EventBridge\n: Amazon EventBridge 는 자체 애플리케이션 , Software-as-a-Service(SaaS) 애플리케이션 , AWS 서비스\n의 데이터를 사용하여 애플리케이션을 쉽게 연결할 수 있게 지원하는 서버리스 이벤트 버스이다 .\nEventBridge 는 Zendesk, Datadog 또는 Pagerduty 와 같은 이벤트 소스의 실시간 데이터 스트림을 전송\n하고 해당 데이터를 AWS Lambda 와 같은 대상으로 라우팅한다 . \n​\nAmazon CloudWatch Events 기능\n•데이터를 전송할 대상을 결정하는 라우팅 규칙을 설정하여 모든 데이터 소스에 실시간으로 대\n응하는 애플리케이션 아키텍처를 구축할 수 있다\n•EventBridge 를 사용하면 느슨하게 결합된 분산형의 이벤트 기반 아키텍처를 구축할 수 있어, \n개발자 민첩성과 애플리케이션의 복원력을 향상 시킨다 .\n•이벤트 수집 및 전송, 보안, 권한 부여, 오류 처리를 담당하기 때문에 사용자는 이벤트 기반 \n애플리케이션을 쉽게 구축할 수 있다\n•CloudWatch Events API를 활용하므로 CloudWatch Events 사용자는 새로운 EventBridge 콘솔\n과 CloudWatch Events 콘솔에서 기존의 기본 버스, 규칙 및 이벤트에 액세스할 수 있다.\n​\n▶오답\n→ AWS Glue\n: 분석 사용자가 여러 소스의 데이터를 쉽게 검색, 준비, 이동, 통합할 수 있도록 하는 서버리스 데이터 \n통합 서비스이다 .\n주요 데이터 통합 기능을 단일 서비스로 통합한다 . \n여기에는 데이터 검색, 최신 ETL, 정제, 변환, 중앙 집중식 카탈로그화가 포함된다 . 또한 서버리스이므\n로 관리할 인프라가 없다. \nETL, ELT, 스트리밍과 같은 모든 워크로드를 하나의 서비스에서 유연하게 지원하므로 AWS Glue는 다\n양한 워크로드 및 사용자 유형에 \n걸쳐 사용자를 지원한다 .\n​\nAWS Glue의 기능\n•70개 이상의 다양한 데이터 소스를 검색하여 연결하고 중앙 집중식 데이터 카탈로그에서 데이\n터를 관리할 수 있다\n•추출, 변환, 로드(ETL) 파이프라인을 시각적으로 생성, 실행, 모니터링하여 데이터 레이크에 데\n이터를 로드할 수 있다\n•Amazon Athena, Amazon EMR, Amazon Redshift Spectrum 을 사용하여 카탈로그화된 데이터\n를 즉시 검색하고 쿼리할 수 있다.\n•아키텍처 전반에 걸쳐 데이터를 쉽게 통합할 수 있다. AWS 분석 서비스 및 Amazon S3 데이\n터 레이크와 통합된다 ."
    },
    "정답": "B"
  },
  {
    "문제번호": 52,
    "질문": "A company wants to migrate its on-premises application to AWS. The application produces output \nfiles that vary in size from tens of gigabytes to hundreds of terabytes. The application data must be \nstored in a standard file system structure. The company wants a solution that scales automatically. is \nhighly available, and requires minimum operational overhead.\nWhich solution will meet these requirements?\n회사에서 온프레미스 애플리케이션을 AWS로 마이그레이션하려고 합니다 . 애플리케이션은 수십 기가바\n이트에서 수백 테라바이트까지 다양한 크기의 출력 파일을 생성합니다 . 애플리케이션 데이터는 표준 파\n일 시스템 구조로 저장되어야 합니다 . 회사는 자동으로 확장되는 솔루션을 원합니다 . 고가용성이며 최\n소한의 운영 오버헤드 가 필요합니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n​",
    "보기": {
      "A": "Migrate the application to run as containers on Amazon Elastic Container Service (Amazon ECS). \nUse Amazon S3 for storage.\nAmazon Elastic Container Service(Amazon ECS)에서 컨테이너로 실행되도록 애플리케이션을 마이그레\n이션합니다 . 스토리지에 Amazon S3를 사용합니다 .\n​",
      "B": "Migrate the application to run as containers on Amazon Elastic Kubernetes Service (Amazon EKS). \nUse Amazon Elastic Block Store (Amazon EBS) for storage.\nAmazon Elastic Kubernetes Service(Amazon EKS)에서 컨테이너로 실행되도록 애플리케이션을 마이그\n레이션합니다 . 스토리지에 Amazon Elastic Block Store(Amazon EBS)를 사용합니다 .\n​",
      "C": "Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon \nElastic File System (Amazon EFS) for storage.\n다중 AZ Auto Scaling 그룹의 Amazon EC2 인스턴스로 애플리케이션을 마이그레이션합니다 . 스토리지\n에 Amazon Elastic File System(Amazon EFS)을 사용합니다 .\n​\n→ Amazon Elastic File System(Amazon EFS)\n: AWS에서 파일 스토리지를 쉽게 설정, 확장 및 비용 최적화할 수 있게 해주는 간단한 방식의 탄력적 \n서버리스 파일 시스템 .\n​\n▶오답\nAmazon EBS\n: AWS 클라우드의 EC2 인스턴스에 사용할 영구 블록 스토리지 볼륨을 제공, 각 EBS 볼륨은 가용 영\n역 내에 자동으로 복제되어 구성요소 장애로부터 보호해주고 고가용성 및 내구성을 제공한다 . \nAmazon EBS 볼륨은 워크로드 실행에 필요한 지연시간이 짧고 일관된 성능을 제공합니다 .\nAmazon EBS를 사용하면 단 몇 분 내에 사용량을 많게 또는 적게 확장할 수 있으며 , 프로비저닝한 부\n분에 대해서만 저렴한 비용을 지불합니다 .\n​\nEBS를 효율적으로 저장하는 방법\n: Snapshot - 특정 시간에 EBS 상태의 저장본 . 특정시간의 EBS 복구 가능. S3에 보관(변화된 부분만 \n저장)\n​",
      "D": "Migrate the application to Amazon EC2 instances in a Multi-AZ Auto Scaling group. Use Amazon \nElastic Block Store (Amazon EBS) for storage.\n다중 AZ Auto Scaling 그룹의 Amazon EC2 인스턴스로 애플리케이션을 마이그레이션합니다 . 스토리지\n에 Amazon Elastic Block Store(Amazon EBS)를 사용합니다 .\n​\n​"
    },
    "정답": "C"
  },
  {
    "문제번호": 53,
    "질문": "A company needs to store its accounting records in Amazon S3. The records must be immediately \naccessible for 1 year and then must be archived for an additional 9 years. No one at the company, \nincluding administrative users and root users, can be able to delete the records during the entire \n10-year period. The records must be stored with maximum resiliency.\nWhich solution will meet these requirements?\n​회사는 Amazon S3에 회계 기록을 저장해야 합니다 . 기록은 1년 동안 즉시 액세스 할 수 있어야 하며 그 \n후 추가로 9년 동안 보관해야 합니다 . 관리자 및 루트 사용자를 포함하여 회사의 그 누구도 전체 10년 \n동안 기록을 삭제할 수 없습니다 .기록은 최대한의 복원력으로 저장해야 합니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n​",
    "보기": {
      "A": "Store the records in S3 Glacier for the entire 10-year period. Use an access control policy to deny \ndeletion of the records for a period of 10 years.\n​전체 10년 동안 S3 Glacier 에 기록을 저장합니다 . 접근통제 정책을 사용하여 10년 동안 기록 삭제를 거\n부합니다 .\n​",
      "B": "Store the records by using S3 Intelligent-Tiering. Use an IAM policy to deny deletion of the records. \nAfter 10 years, change the IAM policy to allow deletion.\n​S3 Intelligent-Tiering 을 사용하여 레코드를 저장합니다 . IAM 정책을 사용하여 레코드 삭제를 거부합니\n다. 10년 후 삭제를 허용하도록 IAM 정책을 변경합니다 .\n​",
      "C": "Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 Glacier Deep Archive \nafter 1 year. Use S3 Object Lock in compliance mode for a period of 10 years.\n​S3 수명 주기 정책을 사용하여 1년 후에 S3 Standard 에서 S3 Glacier Deep Archive 로 레코드를 전환\n합니다 . 10년 동안 규정 준수 모드에서 S3 Object Lock을 사용합니다 .\n​\nS3 Object Lock (S3 객체 잠금)\n: S3 객체 잠금을 사용하면 write-once-read-many (WORM) 모델을 사용하여 객체를 저장할 수 있다. \n객체 잠금은 고정된 시간 동안 또는 무기한으로 객체의 삭제 또는 덮어쓰기를 방지하는 데 도움이 될 \n수 있다. \n객체 잠금을 사용하면 WORM 스토리지가 필요한 규제 요구 사항을 충족하거나 객체 변경 및 삭제에 \n대한 보호 계층을 \n추가하는 데 도움이 된다.",
      "D": "Use an S3 Lifecycle policy to transition the records from S3 Standard to S3 One Zone-Infrequent \nAccess (S3 One Zone-IA) after 1 year. Use S3 Object Lock in governance mode for a period of 10 \nyears.\n​S3 수명 주기 정책을 사용하여 1년 후 레코드를 S3 Standard 에서 S3 One Zone-Infrequent Access(S3 \nOne Zone-IA) 로 전환합니다 . 10년 동안 거버넌스 모드에서 S3 Object Lock을 사용합니다 .\n​"
    },
    "정답": "C"
  },
  {
    "문제번호": 54,
    "질문": "A company runs multiple Windows workloads on AWS. The company's employees use Windows file 스토리지 클래스\nS3 Standard \n: 자주 액세스하기 위\n해 미션 크리티컬 \n프로턱션 데이터 보관S3 Standard-IA \nS3 One Zone-IA\n: 액세스 빈도가 낮은 \n데이터 보관S3 Glacier Flexible \nRetrieval S3 Glacier \nDeep Archive\n: 가장 낮은 비용으로 \n데이터 보관S3 Intelligent-Tiering\n: 액세스 패턴이 변경\n되거나 알 수 없는 액\n세스 패턴이 있는 데이\n터 보관\nshares that are hosted on two Amazon EC2 instances. The file shares synchronize data between \nthemselves and maintain duplicate copies. The company wants a highly available and durable \nstorage solution that preserves how users currently access the files.\nWhat should a solutions architect do to meet these requirements?\n​회사는 AWS에서 여러 Windows 워크로드를 실행합니다 . 회사 직원은 두 개의 Amazon EC2 인스턴스\n에서 호스팅되는 Windows 파일 공유를 사용합니다 . 파일 공유는 서로 간에 데이터를 동기화하고 중복 \n복사본을 유지합니다 . 이 회사는 사용자가 현재 파일에 액세스하는 방식을 보존하는 고가용성 및 내구\n성 스토리지 솔루션을 원합니다 .\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Migrate all the data to Amazon S3. Set up IAM authentication for users to access files.\n​모든 데이터를 Amazon S3로 마이그레이션합니다 . 사용자가 파일에 액세스할 수 있도록 IAM 인증을 \n설정합니다 .\n​",
      "B": "Set up an Amazon S3 File Gateway. Mount the S3 File Gateway on the existing EC2 instances.\n​Amazon S3 파일 게이트웨이를 설정합니다 . 기존 EC2 인스턴스에 S3 파일 게이트웨이를 탑재합니다 .\n​",
      "C": "Extend the file share environment to Amazon FSx for Windows File Server with a Multi-AZ \nconfiguration. Migrate all the data to FSx for Windows File Server.\n​다중 AZ 구성을 사용하여 파일 공유 환경을 Windows 파일 서버용 Amazon FSx로 확장합니다 . 모든 \n데이터를 Windows 파일 서버용 FSx로 마이그레이션합니다 .\n​\n→ Amazon FSx\n: Windows 파일 서버용 Amazon FSx는 완전 기본 Windows 파일 시스템이 지원하는 완전 관리형 \nMicrosoft Windows \n파일 서버를 제공한다 . \nFSx for Windows File Server 는 엔터프라이즈 애플리케이션을 AWS 클라우드로 쉽게 리프트 앤 시프트\n할 수 있는 기능, \n성능 및 호환성을 갖추고 있다.\nWindows 기본 공유 폴더 GUI 및 PowerShell 의 원격 관리용 Amazon FSx CLI를 사용하여 \nWindows 파일 서버용 FSx 파일 시스템의 파일 공유를 관리할 수 있다.",
      "D": "Extend the file share environment to Amazon Elastic File System (Amazon EFS) with a Multi-AZ \nconfiguration. Migrate all the data to Amazon EFS.\n다중 AZ 구성을 사용하여 파일 공유 환경을 Amazon Elastic File System(Amazon EFS)으로 확장합니\n다. 모든 데이터를 Amazon EFS로 마이그레이션합니다 .\n​"
    },
    "정답": "C"
  },
  {
    "문제번호": 55,
    "질문": "A solutions architect is developing a VPC architecture that includes multiple subnets. The architecture \nwill host applications that use Amazon EC2 instances and Amazon RDS DB instances. The \narchitecture consists of six subnets in two Availability Zones. Each Availability Zone includes a public \nsubnet, a private subnet, and a dedicated subnet for databases. Only EC2 instances that run in the \nprivate subnets can have access to the RDS databases.\nWhich solution will meet these requirements?\n​솔루션 설계자는 여러 서브넷을 포함하는 VPC 아키텍처 를 개발 중입니다 . 아키텍처는 Amazon EC2 \n인스턴스 및 Amazon RDS DB 인스턴스를 사용하는 애플리케이션을 호스팅합니다 . 아키텍처는 2개의 \n가용 영역에 있는 6개의 서브넷으로 구성됩니다 . 각 가용 영역에는 퍼블릭 서브넷 , 프라이빗 서브넷 및 \n데이터베이스용 전용 서브넷이 포함됩니다 . 프라이빗 서브넷에서 실행되는 EC2 인스턴스만 RDS 데이\n터베이스에 액세스할 수 있습니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n​",
    "보기": {
      "A": "Create a new route table that excludes the route to the public subnets' CIDR blocks. Associate the \nroute table with the database subnets.\n​퍼블릭 서브넷의 CIDR 블록에 대한 경로를 제외하는 새 라우팅 테이블을 생성합니다 . 라우팅 테이블을 \n데이터베이스 서브넷과 연결합니다 .\n→ 트래픽 흐름을 완전히 구성하지 않음\n​",
      "B": "Create a security group that denies inbound traffic from the security group that is assigned to \ninstances in the public subnets. Attach the security group to the DB instances.\n​퍼블릭 서브넷의 인스턴스에 할당된 보안 그룹의 인바운드 트래픽을 거부하는 보안 그룹을 생성합니다 . \n보안 그룹을 DB 인스턴스에 연결합니다 .\n→​ 보안 그룹에 거부 규칙이 없음\n​",
      "C": "Create a security group that allows inbound traffic from the security group that is assigned to \ninstances in the private subnets. Attach the security group to the DB instances.\n프라이빗 서브넷의 인스턴스에 할당된 보안 그룹의 인바운드 트래픽을 허용하는 보안 그룹을 생성합니\n다. 보안 그룹을 DB 인스턴스에 연결합니다 .\n​\n→ VPC 보안 그룹은 DB 인스턴스 내부 및 외부에서 송수신되는 트래픽에 대한 액세스를 제어합니다 . \n기본적으로 DB 인스턴스에 대한 네트워크 액세스는 해제되어 있습니다 . IP 주소 범위, 포트 또는 보안 \n그룹에서 액세스를 허용하는 보안 그룹의 규칙을 지정할 수 있습니다 .\n​\n▶오답",
      "D": "Create a new peering connection between the public subnets and the private subnets. Create a \ndifferent peering connection between the private subnets and the database subnets.\n퍼블릭 서브넷과 프라이빗 서브넷 사이에 새로운 피어링 연결을 생성합니다 . 프라이빗 서브넷과 데이터\n베이스 서브넷 간에 다른 피어링 연결을 만듭니다 .\n→ 피어링은 대부분 VPC 간에 이루어지며 여기에서는 실제로 도움이 되지 않음.​"
    },
    "정답": "C"
  },
  {
    "문제번호": 56,
    "질문": "A company has registered its domain name with Amazon Route 53. The company uses Amazon API \nGateway in the ca-central-1 Region as a public interface for its backend microservice APIs. \nThird-party services consume the APIs securely. The company wants to design its API Gateway URL \nwith the company's domain name and corresponding certificate so that the third-party services can \nuse HTTPS.\nWhich solution will meet these requirements?\n​회사는 Amazon Route 53에 도메인 이름을 등록했습니다 . 이 회사는 ca-central-1 리전의 Amazon API \nGateway 를 백엔드 마이크로서비스 API의 공용 인터페이스로 사용합니다 . 타사 서비스는 API를 안전하\n게 사용합니다 . 회사는 타사 서비스에서 HTTPS 를 사용할 수 있도록 회사의 도메인 이름 및 해당 인증\n서로 API 게이트웨이 URL을 설계하려고 합니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n​",
    "보기": {
      "A": "Create stage variables in API Gateway with Name=\"Endpoint-URL\" and Value=\"Company Domain \nName\" to overwrite the default URL. Import the public certificate associated with the company's \ndomain name into AWS Certificate Manager (ACM).\n​API Gateway 에서 Name=\"Endpoint-URL\" 및 Value=\"Company Domain Name\" 으로 단계 변수를 생성하\n여 기본 URL을 덮어씁니다 . 회사의 도메인 이름과 연결된 공인 인증서를 AWS Certificate \nManager(ACM) 로 가져옵니다 .\n​",
      "B": "Create Route 53 DNS records with the company's domain name. Point the alias record to the \nRegional API Gateway stage endpoint. Import the public certificate associated with the company's \ndomain name into AWS Certificate Manager (ACM) in the us-east-1 Region.\n​회사의 도메인 이름으로 Route 53 DNS 레코드를 생성합니다 . 별칭 레코드가 리전 API 게이트웨이 단\n계 엔드포인트를 가리키도록 합니다 . 회사의 도메인 이름과 연결된 공인 인증서를 us-east-1 리전의 \nAWS Certificate Manager(ACM) 로 가져옵니다 .\n​",
      "C": "Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company's \ndomain name. Import the public certificate associated with the company's domain name into AWS \nCertificate Manager (ACM) in the same Region. Attach the certificate to the API Gateway endpoint. \nConfigure Route 53 to route traffic to the API Gateway endpoint.\n​리전 API 게이트웨이 엔드포인트를 생성합니다 . API Gateway 엔드포인트를 회사의 도메인 이름과 연\n결합니다 . 회사의 도메인 이름과 연결된 공인 인증서를 동일한 리전의 AWS Certificate Manager(ACM)\n로 가져옵니다 . API Gateway 엔드포인트에 인증서를 연결합니다 . API Gateway 엔드포인트로 트래픽을 \n라우팅하도록 Route 53을 구성합니다 .\n​\n→ API Gateway 에서 리전 사용자 지정 도메인 이름 설정\n: AWS 리전의 경우 리전 API 엔드포인트에 대한 사용자 지정 도메인 이름을 생성할 수 있다. \n사용자 지정 도메인 이름을 생성하려면 리전별 ACM 인증서를 제공해야 한다.\nAPI Gateway 리전 사용자 지정 도메인 이름의 경우, API와 동일한 리전에서 인증서를 요청하거나 가\n져와야 한다. \n인증서는 공개적으로 신뢰할 수 있는 인증 기관에서 서명해야 하며 사용자 지정 도메인 이름을 포함해\n야 한다.",
      "D": "Create a Regional API Gateway endpoint. Associate the API Gateway endpoint with the company's \ndomain name. Import the public certificate associated with the company's domain name into AWS \nCertificate Manager (ACM) in the us-east-1 Region. Attach the certificate to the API Gateway APIs. \nCreate Route 53 DNS records with the company's domain name. Point an A record to the \ncompany's domain name.\n리전 API 게이트웨이 엔드포인트를 생성합니다 . API Gateway 엔드포인트를 회사의 도메인 이름과 연\n결합니다 . 회사의 도메인 이름과 연결된 공인 인증서를 us-east-1 리전의 AWS Certificate \nManager(ACM) 로 가져옵니다 . API Gateway API에 인증서를 연결합니다 . 회사의 도메인 이름으로 \nRoute 53 DNS 레코드를 생성합니다 . A 레코드가 회사의 도메인 이름을 가리키도록 합니다 .\n​\n​"
    },
    "정답": "C"
  },
  {
    "문제번호": 57,
    "질문": "A company is running a popular social media website. The website gives users the ability to upload \nimages to share with other users. The company wants to make sure that the images do not contain \ninappropriate content. The company needs a solution that minimizes development effort.\nWhat should a solutions architect do to meet these requirements?\n​한 회사에서 인기 있는 소셜 미디어 웹사이트를 운영하고 있습니다 . 웹사이트는 사용자에게 이미지를 \n업로드하여 다른 사용자와 공유할 수 있는 기능을 제공합니다 . 회사는 이미지에 부적절한 콘텐츠가 포\n함되지 않았는지 확인하고 싶습니다 . 회사는 개발 노력을 최소화하는 솔루션이 필요합니다 .\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Use Amazon Comprehend to detect inappropriate content. Use human review for low-confidence \npredictions.\n​Amazon Comprehend 를 사용하여 부적절한 콘텐츠를 감지합니다 . 신뢰도가 낮은 예측에는 인적 검토를 \n사용합니다 .\n​",
      "B": "Use Amazon Rekognition to detect inappropriate content. Use human review for low-confidence \npredictions.\nAmazon Rekognition 을 사용하여 부적절한 콘텐츠를 감지합니다 . 신뢰도가 낮은 예측에는 인적 검토를 \n사용합니다 .\n​\n→ 콘텐츠 검토\n: Amazon Rekognition 을 사용하여 부적절하거나 원치 않거나 불쾌감을 주는 콘텐츠를 감지할 수 있다. \n소셜 미디어 , 방송 미디어 , 광고 및 전자 상거래 상황에서 Rekognition 조정 API를 사용하여 보다 안전\n한 사용자 환경을 \n만들고 광고주에게 브랜드 안전을 보장하며 지역 및 글로벌 규정을 준수할 수 있다.\n​\n▶오답\n​Amazon Comprehend\n: Amazon Comprehend 는 기계 학습을 사용하여 텍스트 안에 있는 의미를 찾아내고 통찰을 얻는 자연\n어 처리(NLP) 서비스 .\n​\nAmazon SageMaker\n: 완전관리형 인프라 , 도구 및 워크플로를 사용하여 모든 사용 사례에 대해 기계 학습(ML) 모델을 구축, \n훈련 및 배포하는\n완전관리형 서비스 .\n​\nAWS Fargate\n: 컨테이너에 적합한 서버리스 컴퓨팅 엔진으로 , Amazon Elastic Container Service(ECS) 및 Amazon \nElastic Kubernetes Service(EKS) 와 연동된다 . \nAWS Fargate 를 사용하면 애플리케이션을 구축하는 데 집중할 수 있다. Fargate 를 사용하면 서버를 프\n로비저닝하고 관리할 필요가 없기 때문에 애플리케이션별로 리소스를 지정하고 관련 비용을 지불할 수 \n있으며 , 계획적으로 애플리케이션을 격리하여 보안을 개선할 수 있다.\n→ Fargate 를 통해 기계 학습(ML) 모델을 훈련, 테스트 및 배포하기 위해 과도하게 프로비저닝하지 않\n고도 서버 용량을 \n증대하는 데 필요한 확장성을 달성할 수 있다.",
      "C": "Use Amazon SageMaker to detect inappropriate content. Use ground truth to label low-confidence \npredictions.\n​Amazon SageMaker 를 사용하여 부적절한 콘텐츠를 감지합니다 . 신뢰도가 낮은 예측에 레이블을 지정\n하려면 정답을 사용합니다 .\n​",
      "D": "Use AWS Fargate to deploy a custom machine learning model to detect inappropriate content. Use \nground truth to label low-confidence predictions.\nAWS Fargate 를 사용하여 사용자 지정 기계 학습 모델을 배포하여 부적절한 콘텐츠를 감지합니다 . 신\n뢰도가 낮은 예측에 레이블을 지정하려면 정답을 사용합니다 .\n​"
    },
    "정답": "B"
  },
  {
    "문제번호": 58,
    "질문": "A company wants to run its critical applications in containers to meet requirements for scalability and \navailability. The company prefers to focus on maintenance of the critical applications. The company \ndoes not want to be responsible for provisioning and managing the underlying infrastructure that runs \nthe containerized workload.\nWhat should a solutions architect do to meet these requirements?\n​회사는 확장성 및 가용성에 대한 요구 사항을 충족하기 위해 컨테이너에서 중요한 응용 프로그램을 실\n행하려고 합니다 . 회사는 중요한 응용 프로그램의 유지 관리에 집중하는 것을 선호합니다 . 회사는 컨테\n이너화된 워크로드를 실행하는 기본 인프라의 프로비저닝 및 관리에 대한 책임을 원하지 않습니다 .\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Use Amazon EC2 instances, and install Docker on the instances.\n​Amazon EC2 인스턴스를 사용하고 인스턴스에 Docker 를 설치합니다 .\n​",
      "B": "Use Amazon Elastic Container Service (Amazon ECS) on Amazon EC2 worker nodes.\n​Amazon EC2 작업자 노드에서 Amazon Elastic Container Service(Amazon ECS)를 사용합니다 .\n​",
      "C": "Use Amazon Elastic Container Service (Amazon ECS) on AWS Fargate.\n​AWS Fargate 에서 Amazon Elastic Container Service(Amazon ECS)를 사용합니다 .\n​\nAWS Fargate\n: 컨테이너에 적합한 서버리스 컴퓨팅 엔진으로 , Amazon Elastic Container Service(ECS) 및 Amazon \nElastic Kubernetes Service(EKS) 와 연동된다 . \nAWS Fargate 를 사용하면 애플리케이션을 구축하는 데 집중할 수 있다. Fargate 를 사용하면 서버를 프\n로비저닝하고 관리할 필요가 없기 때문에 애플리케이션별로 리소스를 지정하고 관련 비용을 지불할 수 \n있으며 , 계획적으로 애플리케이션을 격리하여 보안을 개선할 수 있다.",
      "D": "Use Amazon EC2 instances from an Amazon Elastic Container Service (Amazon ECS)-optimized \nAmazon Machine Image (AMI).\nAmazon Elastic Container Service(Amazon ECS)에 최적화된 Amazon 머신 이미지 (AMI)의 Amazon \nEC2 인스턴스를 사용합니다 .\n​"
    },
    "정답": "C"
  },
  {
    "문제번호": 59,
    "질문": "A company hosts more than 300 global websites and applications. The company requires a platform \nto analyze more than 30 TB of clickstream data each day.\nWhat should a solutions architect do to transmit and process the clickstream data?\n​회사는 300개 이상의 글로벌 웹사이트 및 애플리케이션을 호스팅합니다 . 이 회사는 매일 30TB 이상의 \n클릭스트림 데이터를 분석할 플랫폼이 필요합니다 .\n솔루션 설계자는 클릭스트림 데이터를 전송하고 처리하기 위해 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Design an AWS Data Pipeline to archive the data to an Amazon S3 bucket and run an Amazon \nEMR cluster with the data to generate analytics.\n​AWS Data Pipeline 을 설계하여 데이터를 Amazon S3 버킷에 보관하고 데이터로 Amazon EMR 클러스\n터를 실행하여 분석을 생성합니다 .\n​",
      "B": "Create an Auto Scaling group of Amazon EC2 instances to process the data and send it to an \nAmazon S3 data lake for Amazon Redshift to use for analysis.\n​Amazon EC2 인스턴스의 Auto Scaling 그룹을 생성하여 데이터를 처리하고 Amazon Redshift 가 분석에 \n사용할 수 있도록 Amazon S3 데이터 레이크로 보냅니다 .\n​",
      "C": "Cache the data to Amazon CloudFront. Store the data in an Amazon S3 bucket. When an object is \nadded to the S3 bucket. run an AWS Lambda function to process the data for analysis.\n​데이터를 Amazon CloudFront 에 캐시합니다 . Amazon S3 버킷에 데이터를 저장합니다 . 객체가 S3 버킷\n에 추가될 때. AWS Lambda 함수를 실행하여 분석용 데이터를 처리합니다 .\n​",
      "D": "Collect the data from Amazon Kinesis Data Streams. Use Amazon Kinesis Data Firehose to transmit \nthe data to an Amazon S3 data lake. Load the data in Amazon Redshift for analysis.\nAmazon Kinesis Data Streams 에서 데이터를 수집합니다 . Amazon Kinesis Data Firehose 를 사용하여 \nAmazon S3 데이터 레이크로 데이터를 전송합니다 . 분석을 위해 Amazon Redshift 에 데이터를 로드합\n니다.\n​\n→ Amazon Redshift 스트리밍 수집을 통한 실시간 분석\n: Amazon Redshift 는 빠르고 확장 가능하며 안전한 완전 관리형 클라우드 데이터 웨어하우스이다 .\n​\nAmazon Redshift 기능\n•표준 SQL을 사용하여 모든 데이터를 간단하고 비용 효율적으로 분석\n•다른 클라우드 데이터 웨어하우스보다 최대 3배 더 나은 가격 대비 성능을 제공\n•수만 명의 고객이 하루에 엑사바이트 규모의 데이터를 처리\n•고성능 BI(비즈니스 인텔리전스 ) 보고, 애플리케이션 대시보드 , 데이터 탐색 및 실시간 분석과 \n같은 분석 워크로드를 강화\n​\nAmazon Redshift 스트리밍 수집을 사용하면 Amazon S3에서 데이터를 스테이징하고 클러스터에 로드\n하는 것과 관련된 지연 시간과 복잡성 없이 \nKinesis Data Streams 에 직접 연결할 수 있다."
    },
    "정답": "D"
  },
  {
    "문제번호": 60,
    "질문": "A company has a website hosted on AWS. The website is behind an Application Load Balancer \n(ALB) that is configured to handle HTTP and HTTPS separately. The company wants to forward all \nrequests to the website so that the requests will use HTTPS.\nWhat should a solutions architect do to meet this requirement?\n회사에 AWS에서 호스팅되는 웹 사이트가 있습니다 . 웹 사이트는 HTTP 와 HTTPS 를 별도로 처리하도\n록 구성된 ALB(Application Load Balancer) 뒤에 있습니다 . 회사는 요청이 HTTPS 를 사용하도록 모든 \n요청을 웹사이트로 전달하려고 합니다 .\n솔루션 설계자는 이 요구 사항을 충족하기 위해 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Update the ALB's network ACL to accept only HTTPS traffic.\n​HTTPS 트래픽만 허용하도록 ALB의 네트워크 ACL을 업데이트합니다 .\n​",
      "B": "Create a rule that replaces the HTTP in the URL with HTTPS.\n​URL의 HTTP 를 HTTPS 로 바꾸는 규칙을 만듭니다 .\n​",
      "C": "Create a listener rule on the ALB to redirect HTTP traffic to HTTPS.\n​ALB에서 리스너 규칙을 생성하여 HTTP 트래픽을 HTTPS 로 리디렉션합니다 .\n​\n→ Application Load Balancer 용 HTTPS 리스너 생성\n: 리스너는 연결 요청을 확인하는 프로세스입니다 . \n로드 밸런서를 생성할 때 리스너를 정의하고 언제든지 로드 밸런서에 리스너를 추가할 수 있습니다 .\n​\n리스너가 지원하는 프로토콜 및 포트\n•프로토콜 : HTTP, HTTPS\n•포트: 1-65535\n​\n애플리케이션이 비즈니스 로직에 집중할 수 있도록 HTTPS 리스너를 사용하여 암호화 및 암호 해독 작\n업을 \n로드 밸런서로 오프로드할 수 있다.",
      "D": "Replace the ALB with a Network Load Balancer configured to use Server Name Indication (SNI).\nALB를 SNI(서버 이름 표시)를 사용하도록 구성된 Network Load Balancer 로 교체합니다 .\n​"
    },
    "정답": null
  },
  {
    "문제번호": 61,
    "질문": "A company is developing a two-tier web application on AWS. The company's developers have \ndeployed the application on an Amazon EC2 instance that connects directly to a backend Amazon \nRDS database. The company must not hardcode database credentials in the application. The \ncompany must also implement a solution to automatically rotate the database credentials on a \nregular basis.\nWhich solution will meet these requirements with the LEAST operational overhead?\n한 회사가 AWS에서 2계층 웹 애플리케이션을 개발하고 있습니다 . 회사 개발자는 백엔드 Amazon RDS \n데이터베이스에 직접 연결되는 Amazon EC2 인스턴스에 애플리케이션을 배포했습니다 . 회사는 애플리\n케이션에 데이터베이스 자격 증명을 하드코딩해서는 안 됩니다 . 또한 회사는 정기적으로 데이터베이스 \n자격 증명을 자동으로 교체하는 솔루션을 구현해야 합니다 .\n최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까 ?\n​",
    "보기": {
      "A": "Store the database credentials in the instance metadata. Use Amazon EventBridge (Amazon \nCloudWatch Events) rules to run a scheduled AWS Lambda function that updates the RDS \ncredentials and instance metadata at the same time.\n인스턴스 메타데이터에 데이터베이스 자격 증명을 저장합니다 . Amazon EventBridge(Amazon \nCloudWatch Events) 규칙을 사용하여 RDS 자격 증명과 인스턴스 메타데이터를 동시에 업데이트하는 \n예약된 AWS Lambda 함수를 실행합니다 .\n​",
      "B": "Store the database credentials in a configuration file in an encrypted Amazon S3 bucket. Use \nAmazon EventBridge (Amazon CloudWatch Events) rules to run a scheduled AWS Lambda function \nthat updates the RDS credentials and the credentials in the configuration file at the same time. Use \nS3 Versioning to ensure the ability to fall back to previous values.\n암호화된 Amazon S3 버킷의 구성 파일에 데이터베이스 자격 증명을 저장합니다 . Amazon \nEventBridge(Amazon CloudWatch Events) 규칙을 사용하여 RDS 자격 증명과 구성 파일의 자격 증명을 \n동시에 업데이트하는 예약된 AWS Lambda 함수를 실행합니다 . S3 버전 관리를 사용하여 이전 값으로 \n폴백하는 기능을 보장합니다 .​\n​",
      "C": "Store the database credentials as a secret in AWS Secrets Manager. Turn on automatic rotation for \nthe secret. Attach the required permission to the EC2 role to grant access to the secret.\n데이터베이스 자격 증명을 AWS Secrets Manager 에 암호로 저장합니다 . 보안 비밀에 대한 자동 순환을 \n켭니다 . EC2 역할에 필요한 권한을 연결하여 보안 암호에 대한 액세스 권한을 부여합니다 .\n​",
      "D": "Store the database credentials as encrypted parameters in AWS Systems Manager Parameter Store. \nTurn on automatic rotation for the encrypted parameters. Attach the required permission to the EC2 \nrole to grant access to the encrypted parameters.\n데이터베이스 자격 증명을 AWS Systems Manager Parameter Store에 암호화된 파라미터로 저장합니\n다. 암호화된 매개변수에 대해 자동 교체을 켭니다 . EC2 역할에 필요한 권한을 연결하여 암호화된 파\n라미터에 대한 액세스 권한을 부여합니다 .\n​\n​"
    },
    "정답": "C"
  },
  {
    "문제번호": 62,
    "질문": "A company is deploying a new public web application to AWS. The application will run behind an \nApplication Load Balancer (ALB). The application needs to be encrypted at the edge with an \nSSL/TLS certificate that is issued by an external certificate authority (CA). The certificate must be \nrotated each year before the certificate expires.\nWhat should a solutions architect do to meet these requirements?\n회사에서 AWS에 새로운 공개 웹 애플리케이션을 배포하고 있습니다 . 애플리케이션은 ALB(Application \nLoad Balancer) 뒤에서 실행됩니다 . 애플리케이션은 외부 CA(인증 기관)에서 발급한 SSL/TLS 인증서 를 \n사용하여 엣지에서 암호화해야 합니다 . 인증서가 만료되기 전에 매년 인증서를 교체해야 합니다 .\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Use AWS Certificate Manager (ACM) to issue an SSL/TLS certificate. Apply the certificate to the \nALB. Use the managed renewal feature to automatically rotate the certificate.\nAWS Certificate Manager(ACM) 를 사용하여 SSL/TLS 인증서를 발급합니다 . 인증서를 ALB에 적용합니\n다. 관리형 갱신 기능을 사용하여 인증서를 자동으로 교체합니다 .\n​",
      "B": "Use AWS Certificate Manager (ACM) to issue an SSL/TLS certificate. Import the key material from \nthe certificate. Apply the certificate to the ALUse the managed renewal feature to automatically rotate \nthe certificate.\nAWS Certificate Manager(ACM) 를 사용하여 SSL/TLS 인증서를 발급합니다 . 인증서에서 키 자료를 가져\n옵니다 . AL에 인증서 적용 관리되는 갱신 기능을 사용하여 인증서를 자동으로 교체합니다 .\n​",
      "C": "Use AWS Certificate Manager (ACM) Private Certificate Authority to issue an SSL/TLS certificate \nfrom the root CA. Apply the certificate to the ALB. Use the managed renewal feature to automatically \nrotate the certificate.\nAWS Certificate Manager(ACM) 사설 인증 기관을 사용하여 루트 CA에서 SSL/TLS 인증서를 발급합니\n다. 인증서를 ALB에 적용합니다 . 관리형 갱신 기능을 사용하여 인증서를 자동으로 교체합니다 .\n​",
      "D": "Use AWS Certificate Manager (ACM) to import an SSL/TLS certificate. Apply the certificate to the \nALB. Use Amazon EventBridge (Amazon CloudWatch Events) to send a notification when the \ncertificate is nearing expiration. Rotate the certificate manually.\nAWS Certificate Manager(ACM) 를 사용하여 SSL/TLS 인증서를 가져옵니다 . 인증서를 ALB에 적용합니\n다. Amazon EventBridge(Amazon CloudWatch Events) 를 사용하여 인증서가 만료될 때 알림을 보냅니\n다. 인증서를 수동으로 교체합니다 .​\n​"
    },
    "정답": "D"
  },
  {
    "문제번호": 63,
    "질문": "A company runs its infrastructure on AWS and has a registered base of 700,000 users for its \ndocument management application. The company intends to create a product that converts large .pdf \nfiles to .jpg image files. The .pdf files average 5 MB in size. The company needs to store the \noriginal files and the converted files. A solutions architect must design a scalable solution to \naccommodate demand that will grow rapidly over time.\nWhich solution meets these requirements MOST cost-effectively?\n회사는 AWS에서 인프라를 실행하고 문서 관리 애플리케이션에 대해 700,000 명의 등록 기반을 보유하\n고 있습니다 . 회사는 큰 .pdf 파일을 .jpg 이미지 파일로 변환하는 제품을 만들려고 합니다 . .pdf 파일의 \n크기는 평균 5MB입니다 . 회사는 원본 파일과 변환 파일을 보관해야 합니다 . 솔루션 설계자는 시간이 \n지남에 따라 빠르게 증가할 수요를 수용할 수 있는 확장 가능한 솔루션 을 설계해야 합니다 .\n어떤 솔루션이 이러한 요구 사항을 가장 비용 효율적 으로 충족합니까 ?\n​",
    "보기": {
      "A": "Save the .pdf files to Amazon S3. Configure an S3 PUT event to invoke an AWS Lambda function \nto convert the files to .jpg format and store them back in Amazon S3.\n.pdf 파일을 Amazon S3에 저장합니다 . AWS Lambda 함수를 호출하여 파일을 .jpg 형식으로 변환하고 \nAmazon S3에 다시 저장하도록 S3 PUT 이벤트를 구성합니다 .\n​\n▶오답",
      "B": "Save the .pdf files to Amazon DynamoDUse the DynamoDB Streams feature to invoke an AWS \nLambda function to convert the files to .jpg format and store them back in DynamoDB.\n.pdf 파일을 Amazon DynamoD 에 저장 DynamoDB 스트림 기능을 사용하여 AWS Lambda 함수를 호출\n하여 파일을 .jpg 형식으로 변환하고 DynamoDB 에 다시 저장합니다 .\n→ DynamoDB 의 최대 항목 크기는 400KB 이기 때문에 정답이 될 수 없다.\n​",
      "C": "Upload the .pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2 instances, \nAmazon Elastic Block Store (Amazon EBS) storage, and an Auto Scaling group. Use a program in \nthe EC2 instances to convert the files to .jpg format. Save the .pdf files and the .jpg files in the \nEBS store.\nAmazon EC2 인스턴스 , Amazon Elastic Block Store(Amazon EBS) 스토리지 및 Auto Scaling 그룹이 \n포함된 AWS Elastic Beanstalk 애플리케이션에 .pdf 파일을 업로드합니다 . EC2 인스턴스의 프로그램을 \n사용하여 파일을 .jpg 형식으로 변환합니다 . .pdf 파일과 .jpg 파일을 EBS 스토어에 저장합니다 .\n​",
      "D": "Upload the .pdf files to an AWS Elastic Beanstalk application that includes Amazon EC2 instances, \nAmazon Elastic File System (Amazon EFS) storage, and an Auto Scaling group. Use a program in \nthe EC2 instances to convert the file to .jpg format. Save the .pdf files and the .jpg files in the EBS \nstore.\n.pdf 파일을 Amazon EC2 인스턴스 , Amazon Elastic File System(Amazon EFS) 스토리지 및 Auto \nScaling 그룹이 포함된 AWS Elastic Beanstalk 애플리케이션에 업로드합니다 . EC2 인스턴스의 프로그\n램을 사용하여 파일을 .jpg 형식으로 변환합니다 . .pdf 파일과 .jpg 파일을 EBS 스토어에 저장합니다 .\n​"
    },
    "정답": "A"
  },
  {
    "문제번호": 64,
    "질문": "A company has more than 5 TB of file data on Windows file servers that run on premises. Users \nand applications interact with the data each day.\nThe company is moving its Windows workloads to AWS. As the company continues this process, the \ncompany requires access to AWS and on-premises file storage with minimum latency. The company \nneeds a solution that minimizes operational overhead and requires no significant changes to the \nexisting file access patterns. The company uses an AWS Site-to-Site VPN connection for connectivity \nto AWS.\nWhat should a solutions architect do to meet these requirements?\n회사는 온프레미스에서 실행되는 Windows 파일 서버에 5TB 이상의 파일 데이터를 가지고 있습니다 . \n사용자와 애플리케이션은 매일 데이터와 상호 작용합니다 .\n이 회사는 Windows 워크로드를 AWS로 이전하고 있습니다 . 회사가 이 프로세스를 계속함에 따라 회사\n는 최소 지연 시간으로 AWS 및 온프레미스 파일 스토리지에 액세스할 수 있어야 합니다 . 회사는 운영 \n오버헤드를 최소화하고 기존 파일 액세스 패턴을 크게 변경할 필요가 없는 솔루션이 필요합니다 . 회사\n는 AWS 연결을 위해 AWS Site-to-Site VPN 연결을 사용합니다 .\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Deploy and configure Amazon FSx for Windows File Server on AWS. Move the on-premises file data \nto FSx for Windows File Server. Reconfigure the workloads to use FSx for Windows File Server on \nAWS.\nAWS에서 Windows 파일 서버용 Amazon FSx를 배포 및 구성합니다 . 온-프레미스 파일 데이터를 \nWindows 파일 서버용 FSx로 이동합니다 . AWS에서 Windows 파일 서버용 FSx를 사용하도록 워크로드\n를 재구성합니다 .\n​",
      "B": "Deploy and configure an Amazon S3 File Gateway on premises. Move the on-premises file data to \nthe S3 File Gateway. Reconfigure the on-premises workloads and the cloud workloads to use the S3 \nFile Gateway.\n온프레미스에 Amazon S3 파일 게이트웨이를 배포하고 구성합니다 . 온프레미스 파일 데이터를 S3 파일 \n게이트웨이로 이동합니다 . S3 파일 게이트웨이를 사용하도록 온프레미스 워크로드 및 클라우드 워크로\n드를 재구성합니다 .\n​",
      "C": "Deploy and configure an Amazon S3 File Gateway on premises. Move the on-premises file data to \nAmazon S3. Reconfigure the workloads to use either Amazon S3 directly or the S3 File Gateway. \ndepending on each workload's location.\n온프레미스에 Amazon S3 파일 게이트웨이를 배포하고 구성합니다 . 온프레미스 파일 데이터를 Amazon \nS3로 이동합니다 . Amazon S3를 직접 사용하거나 S3 파일 게이트웨이를 사용하도록 워크로드를 재구\n성합니다 . 각 워크로드의 위치에 따라 다릅니다 .\n​",
      "D": "Deploy and configure Amazon FSx for Windows File Server on AWS. Deploy and configure an \nAmazon FSx File Gateway on premises. Move the on-premises file data to the FSx File Gateway. \nConfigure the cloud workloads to use FSx for Windows File Server on AWS. Configure the \non-premises workloads to use the FSx File Gateway.\n​AWS에서 Windows 파일 서버용 Amazon FSx를 배포 및 구성합니다 . 온프레미스에 Amazon FSx 파일 \n게이트웨이를 배포하고 구성합니다 . 온프레미스 파일 데이터를 FSx 파일 게이트웨이로 이동합니다 . \nAWS의 Windows 파일 서버용 FSx를 사용하도록 클라우드 워크로드를 구성합니다 . FSx 파일 게이트웨\n이를 사용하도록 온프레미스 워크로드를 구성합니다 .\n​\n​"
    },
    "정답": "D"
  },
  {
    "문제번호": 65,
    "질문": "A hospital recently deployed a RESTful API with Amazon API Gateway and AWS Lambda. The \nhospital uses API Gateway and Lambda to upload reports that are in PDF format and JPEG format. \nThe hospital needs to modify the Lambda code to identify protected health information (PHI) in the \nreports.\nWhich solution will meet these requirements with the LEAST operational overhead?\n병원은 최근 Amazon API Gateway 및 AWS Lambda 와 함께 RESTful API를 배포했습니다 . 병원은 API \nGateway 및 Lambda 를 사용하여 PDF 형식 및 JPEG 형식의 보고서를 업로드합니다 . 병원은 보고서에\n서 보호되는 건강 정보(PHI)를 식별하기 위해 Lambda 코드를 수정해야 합니다 .\n최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까 ?\n​",
    "보기": {
      "A": "Use existing Python libraries to extract the text from the reports and to identify the PHI from the \nextracted text.\n기존 Python 라이브러리를 사용하여 보고서에서 텍스트를 추출하고 추출된 텍스트에서 PHI를 식별합니\n다.\n​",
      "B": "Use Amazon Textract to extract the text from the reports. Use Amazon SageMaker to identify the \nPHI from the extracted text.\n​Amazon Textract 를 사용하여 보고서에서 텍스트를 추출합니다 . Amazon SageMaker 를 사용하여 추출된 \n텍스트에서 PHI를 식별합니다 .\n​",
      "C": "Use Amazon Textract to extract the text from the reports. Use Amazon Comprehend Medical to \nidentify the PHI from the extracted text.\n​Amazon Textract 를 사용하여 보고서에서 텍스트를 추출합니다 . Amazon Comprehend Medical 을 사용\n하여 추출된 텍스트에서 PHI를 식별합니다 .\n​",
      "D": "Use Amazon Rekognition to extract the text from the reports. Use Amazon Comprehend Medical to \nidentify the PHI from the extracted text.\n​Amazon Rekognition 을 사용하여 보고서에서 텍스트를 추출합니다 . Amazon Comprehend Medical 을 사\n용하여 추출된 텍스트에서 PHI를 식별합니다 .\n​"
    },
    "정답": "C"
  },
  {
    "문제번호": 66,
    "질문": "A company has an application that generates a large number of files, each approximately 5 MB in \nsize. The files are stored in Amazon S3. Company policy requires the files to be stored for 4 years \nbefore they can be deleted. Immediate accessibility is always required as the files contain critical \nbusiness data that is not easy to reproduce. The files are frequently accessed in the first 30 days of \nthe object creation but are rarely accessed after the first 30 days.\nWhich storage solution is MOST cost-effective?\n​회사에 각각 크기가 약 5MB인 많은 수의 파일을 생성하는 응용 프로그램이 있습니다 . 파일은 Amazon \nS3에 저장됩니다 . 회사 정책에 따라 파일을 삭제하려면 4년 동안 보관해야 합니다 . 파일에는 재생산하\n기 쉽지 않은 중요한 비즈니스 데이터가 포함되어 있으므로 즉각적인 액세스가 항상 필요합니다 . 파일\n은 객체 생성 후 처음 30일 동안 자주 액세스되지만 처음 30일 후에는 거의 액세스되지 않습니다 .\n가장 비용 효율적인 스토리지 솔루션은 무엇입니까 ?\n​",
    "보기": {
      "A": "Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Glacier 30 days from \nobject creation. Delete the files 4 years after object creation.\n객체 생성 후 30일 동안 S3 Standard 에서 S3 Glacier 로 파일을 이동하는 S3 버킷 수명 주기 정책을 \n생성합니다 . 객체 생성 후 4년이 지나면 파일을 삭제합니다 .\n​",
      "B": "Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 One Zone-Infrequent \nAccess (S3 One Zone-IA) 30 days from object creation. Delete the files 4 years after object creation.\n​객체 생성 후 30일 동안 S3 Standard 에서 S3 One Zone-Infrequent Access(S3 One Zone-IA) 로 파일을 \n이동하는 S3 버킷 수명 주기 정책을 생성합니다 . 객체 생성 후 4년이 지나면 파일을 삭제합니다 .\n​",
      "C": "Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent \nAccess (S3 Standard-IA) 30 days from object creation. Delete the files 4 years after object creation.\n​객체 생성 후 30일 동안 S3 Standard 에서 S3 Standard-Infrequent Access(S3 Standard-IA) 로 파일을 \n이동하는 S3 버킷 수명 주기 정책을 생성합니다 . 객체 생성 후 4년이 지나면 파일을 삭제합니다 .\n​",
      "D": "Create an S3 bucket lifecycle policy to move files from S3 Standard to S3 Standard-Infrequent \nAccess (S3 Standard-IA) 30 days from object creation. Move the files to S3 Glacier 4 years after \nobject creation.\n​객체 생성 후 30일 동안 S3 Standard 에서 S3 Standard-Infrequent Access(S3 Standard-IA) 로 파일을 \n이동하는 S3 버킷 수명 주기 정책을 생성합니다 . 객체 생성 4년 후 파일을 S3 Glacier 로 이동합니다 .\n​"
    },
    "정답": "C"
  },
  {
    "문제번호": 67,
    "질문": "A company hosts an application on multiple Amazon EC2 instances. The application processes \nmessages from an Amazon SQS queue, writes to an Amazon RDS table, and deletes the message \nfrom the queue. Occasional duplicate records are found in the RDS table. The SQS queue does not \ncontain any duplicate messages.\nWhat should a solutions architect do to ensure messages are being processed once only?\n​회사는 여러 Amazon EC2 인스턴스에서 애플리케이션을 호스팅합니다 . 애플리케이션은 Amazon SQS \n대기열의 메시지를 처리하고 Amazon RDS 테이블에 쓰고 대기열에서 메시지를 삭제합니다 . RDS 테이\n블에서 가끔 중복 레코드가 발견됩니다 . SQS 대기열에는 중복 메시지가 없습니다 .\n메시지가 한 번만 처리되도록 솔루션 설계자는 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Use the CreateQueue API call to create a new queue.\nCreateQueue API 호출을 사용하여 새 대기열을 만듭니다 .\n​",
      "B": "Use the AddPermission API call to add appropriate permissions.\nAddPermission API 호출을 사용하여 적절한 권한을 추가합니다 .\n​",
      "C": "Use the ReceiveMessage API call to set an appropriate wait time.\n​ReceiveMessage API 호출을 사용하여 적절한 대기 시간을 설정합니다 .\n​",
      "D": "Use the ChangeMessageVisibility API call to increase the visibility timeout.\n​ChangeMessageVisibility API 호출을 사용하여 가시성 시간 초과를 늘립니다 .\n​"
    },
    "정답": "D"
  },
  {
    "문제번호": 68,
    "질문": "A solutions architect is designing a new hybrid architecture to extend a company's on-premises \ninfrastructure to AWS. The company requires a highly available connection with consistent low \nlatency to an AWS Region. The company needs to minimize costs and is willing to accept slower \ntraffic if the primary connection fails.\nWhat should the solutions architect do to meet these requirements?\n​솔루션 설계자는 회사의 온프레미스 인프라를 AWS로 확장하기 위해 새로운 하이브리드 아키텍처를 설\n계하고 있습니다 . 이 회사는 AWS 리전에 대해 일관되게 짧은 지연 시간과 고가용성 연결이 필요합니\n다. 회사는 비용을 최소화해야 하며 기본 연결이 실패할 경우 더 느린 트래픽을 기꺼이 받아들입니다 .\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Provision an AWS Direct Connect connection to a Region. Provision a VPN connection as a backup \nif the primary Direct Connect connection fails.\n​리전에 대한 AWS Direct Connect 연결을 프로비저닝합니다 . 기본 Direct Connect 연결이 실패하는 경\n우 백업으로 VPN 연결을 프로비저닝합니다 .\n​",
      "B": "Provision a VPN tunnel connection to a Region for private connectivity. Provision a second VPN \ntunnel for private connectivity and as a backup if the primary VPN connection fails.\n​개인 연결을 위해 지역에 VPN 터널 연결을 프로비저닝합니다 . 기본 VPN 연결이 실패할 경우 개인 연\n결 및 백업으로 두 번째 VPN 터널을 프로비저닝합니다 .\n​",
      "C": "Provision an AWS Direct Connect connection to a Region. Provision a second Direct Connect \nconnection to the same Region as a backup if the primary Direct Connect connection fails.\n​리전에 대한 AWS Direct Connect 연결을 프로비저닝합니다 . 기본 Direct Connect 연결이 실패하는 경\n우 백업과 동일한 지역에 두 번째 Direct Connect 연결을 프로비저닝합니다 .\n​\n​",
      "D": "Provision an AWS Direct Connect connection to a Region. Use the Direct Connect failover attribute \nfrom the AWS CLI to automatically create a backup connection if the primary Direct Connect \nconnection fails.\n​리전에 대한 AWS Direct Connect 연결을 프로비저닝합니다 . AWS CLI에서 Direct Connect 장애 조치 \n속성을 사용하여 기본 Direct Connect 연결이 실패할 경우 백업 연결을 자동으로 생성합니다 .\n​"
    },
    "정답": "A"
  },
  {
    "문제번호": 69,
    "질문": "A company is running a business-critical web application on Amazon EC2 instances behind an \nApplication Load Balancer. The EC2 instances are in an Auto Scaling group. The application uses an \nAmazon Aurora PostgreSQL database that is deployed in a single Availability Zone. The company \nwants the application to be highly available with minimum downtime and minimum loss of data.\nWhich solution will meet these requirements with the LEAST operational effort?\n회사는 Application Load Balancer 뒤의 Amazon EC2 인스턴스에서 비즈니스 크리티컬 웹 애플리케이\n션을 실행하고 있습니다 . EC2 인스턴스는 Auto Scaling 그룹에 있습니다 . 애플리케이션은 단일 가용 영\n역에 배포된 Amazon Aurora PostgreSQL 데이터베이스 를 사용합니다 . 회사는 다운타임과 데이터 손실\n을 최소화하면서 애플리케이션의 고가용성을 원합니다 .\n최소한의 운영 노력으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까 ?\n​",
    "보기": {
      "A": "Place the EC2 instances in different AWS Regions. Use Amazon Route 53 health checks to redirect \ntraffic. Use Aurora PostgreSQL Cross-Region Replication.\nEC2 인스턴스를 다른 AWS 리전에 배치합니다 . Amazon Route 53 상태 확인을 사용하여 트래픽을 리\n디렉션합니다 . Aurora PostgreSQL 교차 리전 복제를 사용합니다 .\n​",
      "B": "Configure the Auto Scaling group to use multiple Availability Zones. Configure the database as \nMulti-AZ. Configure an Amazon RDS Proxy instance for the database.\n여러 가용 영역을 사용하도록 Auto Scaling 그룹을 구성합니다 . 데이터베이스를 다중 AZ로 구성합니다 . \n데이터베이스에 대한 Amazon RDS 프록시 인스턴스를 구성합니다 .\n​",
      "C": "Configure the Auto Scaling group to use one Availability Zone. Generate hourly snapshots of the \ndatabase. Recover the database from the snapshots in the event of a failure.\n하나의 가용 영역을 사용하도록 Auto Scaling 그룹을 구성합니다 . 데이터베이스의 시간별 스냅샷을 생\n성합니다 . 장애가 발생한 경우 스냅샷에서 데이터베이스를 복구합니다 .\n​",
      "D": "Configure the Auto Scaling group to use multiple AWS Regions. Write the data from the application \nto Amazon S3. Use S3 Event Notifications to launch an AWS Lambda function to write the data to \nthe database.\n​여러 AWS 리전을 사용하도록 Auto Scaling 그룹을 구성합니다 . 애플리케이션의 데이터를 Amazon S3\n에 씁니다 . S3 이벤트 알림을 사용하여 AWS Lambda 함수를 시작하여 데이터베이스에 데이터를 씁니\n다.\n​"
    },
    "정답": "B"
  },
  {
    "문제번호": 70,
    "질문": "A company's HTTP application is behind a Network Load Balancer (NLB). The NLB's target group is \nconfigured to use an Amazon EC2 Auto Scaling group with multiple EC2 instances that run the web \nservice.\nThe company notices that the NLB is not detecting HTTP errors for the application. These errors \nrequire a manual restart of the EC2 instances that run the web service. The company needs to \nimprove the application's availability without writing custom scripts or code.\nWhat should a solutions architect do to meet these requirements?\n​회사의 HTTP 애플리케이션은 NLB(Network Load Balancer) 뒤에 있습니다 . NLB의 대상 그룹은 웹 서\n비스를 실행하는 여러 EC2 인스턴스와 함께 Amazon EC2 Auto Scaling 그룹을 사용하도록 구성됩니\n다.\n회사는 NLB가 애플리케이션에 대한 HTTP 오류를 감지하지 못한다는 것​을 알게 되었습니다 . 이러한 오\n류는 웹 서비스를 실행하는 EC2 인스턴스를 수동으로 다시 시작해야 합니다 . 회사는 사용자 정의 스크\n립트나 코드를 작성하지 않고 애플리케이션의 가용성을 개선해야 합니다 .\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Enable HTTP health checks on the NLB, supplying the URL of the company's application.\n​NLB에서 HTTP 상태 확인을 활성화하고 회사 응용 프로그램의 URL을 제공합니다 .\n​",
      "B": "Add a cron job to the EC2 instances to check the local application's logs once each minute. If \nHTTP errors are detected. the application will restart.\n​EC2 인스턴스에 cron 작업을 추가하여 1분에 한 번씩 로컬 애플리케이션의 로그를 확인합니다 . HTTP \n오류가 감지된 경우. 응용 프로그램이 다시 시작됩니다 .\n​",
      "C": "Replace the NLB with an Application Load Balancer. Enable HTTP health checks by supplying the \nURL of the company's application. Configure an Auto Scaling action to replace unhealthy instances.\n​NLB를 Application Load Balancer 로 교체합니다 . 회사 애플리케이션의 URL을 제공하여 HTTP 상태 확\n인을 활성화합니다 . 비정상 인스턴스를 교체하도록 Auto Scaling 작업을 구성합니다 .\n​\n→ NLB는 TPC 및 UDP 트래픽에 이상적이며 레이어 4에서 작동하며 , ALB는 HTTP 및 HTTPS 트래픽\n을 지원한다 . \n따라서 ELB를 NLB에서 ALB로 변경하는것이 옳은 정답이다 .",
      "D": "Create an Amazon Cloud Watch alarm that monitors the UnhealthyHostCount metric for the NLB. \nConfigure an Auto Scaling action to replace unhealthy instances when the alarm is in the ALARM \nstate.\n​NLB에 대한 UnhealthyHostCount 지표를 모니터링하는 Amazon Cloud Watch 경보를 생성합니다 . 경보\n가 ALARM 상태일 때 비정상 인스턴스를 교체하도록 Auto Scaling 작업을 구성합니다 .\n​"
    },
    "정답": "C"
  },
  {
    "문제번호": 71,
    "질문": "A company runs a shopping application that uses Amazon DynamoDB to store customer information. \nIn case of data corruption, a solutions architect needs to design a solution that meets a recovery CLB(ClassicLoadBalance)HTTP, HTTPS, TCP 및 TLS 트래픽을 처리할 수 있지만 ALB 및 \nNLB보다 기능은 훨씬 적다. 그래서 현재는 거의 사용하지 않는 LB이\n다. OSI 7레이어 중 응용계층 (7계층) 및 전송계층 (4계층) 에서 모두 \n동작 한다.\nALB(NetworkLoadBalancer)HTTP 및 HTTPS 트래픽 처리에 적합한 로드 밸런스 이다. OSI 7레\n이어중 7계층에서 동작 한다.\nNLB(ApplicationLoadBalanc\ner)TCP,UDP 및 TLS 트래픽 처리에 적합한 로드 밸런서 이다.\nALB보다 빠르게 로드에 반응하고 응답이 가능하며 , 확장 및 축소에 \nALB보다 빠르게 반응한다 .\nOSI 7레이어 중 4계층인 전송계층 에서 동작 한다.\npoint objective (RPO) of 15 minutes and a recovery time objective (RTO) of 1 hour.\nWhat should the solutions architect recommend to meet these requirements?\n한 회사는 Amazon DynamoDB 를 사용하여 고객 정보를 저장하는 쇼핑 애플리케이션을 실행합니다 . 데\n이터 손상의 경우 솔루션 설계자는 15분의 RPO(복구 시점 목표)와 1시간의 RTO(복구 시간 목표)를 충\n족하는 솔루션을 설계해야 합니다 .\n이러한 요구 사항을 충족하기 위해 솔루션 설계자는 무엇을 권장해야 합니까 ?\n​",
    "보기": {
      "A": "Configure DynamoDB global tables. For RPO recovery, point the application to a different AWS \nRegion.\nDynamoDB 전역 테이블을 구성합니다 . RPO 복구의 경우 애플리케이션이 다른 AWS 리전을 가리키도\n록 합니다\n→ DynamoDB 전역 테이블은 다중 리전 및 다중 활성 데이터베이스를 제공하지만 , 데이터 손상의 경우\n에는\n활용되지 않는다 . 이 경우에는 백업이 필요하다 .\n​",
      "B": "Configure DynamoDB point-in-time recovery. For RPO recovery, restore to the desired point in time.\nDynamoDB 특정 시간 복구를 구성합니다 . RPO 복구의 경우 원하는 시점으로 복원합니다 .\n​\n→ DynamoDB 특정 시간으로 복구\n•특정 시점으로 복구를 사용하면 우발적인 쓰기 또는 삭제 작업으로부터 DynamoDB 테이블을 \n보호 가능.\n•Amazon DynamoDB 테이블에 대한 온디맨드 백업을 생성하거나 특정 시점으로 복구를 사용\n한 연속 백업을 활성화 .\n•특정 시점으로 복구를 설정해 두면 온디맨드 백업의 생성, 유지 관리, 예약을 걱정할 필요가 \n없다.\n•특정 시점 작업은 성능이나 API 지연 시간에 영향을 주지 않는다 .\n​\n▶오답",
      "C": "Export the DynamoDB data to Amazon S3 Glacier on a daily basis. For RPO recovery, import the \ndata from S3 Glacier to DynamoDB.\n​DynamoDB 데이터를 매일 Amazon S3 Glacier 로 내보냅니다 . RPO 복구의 경우 S3 Glacier 에서 \nDynamoDB 로 데이터를 가져옵니다 .\n→ 매일 내보내기는 15분의 RPO를 포함하지 않는다 .\n​",
      "D": "Schedule Amazon Elastic Block Store (Amazon EBS) snapshots for the DynamoDB table e\n​DynamoDB 테이블에 대한 Amazon Elastic Block Store(Amazon EBS) 스냅샷을 15분마다 예약합니다 . \nRPO 복구의 경우 EBS 스냅샷을 사용하여 DynamoDB 테이블을 복원합니다 .\n→ DynamoDB 는 서버리스서비스 이므로 EBS 스냅샷과는 관계없다 ."
    },
    "정답": "B"
  },
  {
    "문제번호": 72,
    "질문": "A company runs a photo processing application that needs to frequently upload and download \npictures from Amazon S3 buckets that are located in the same AWS Region. A solutions architect \nhas noticed an increased cost in data transfer fees and needs to implement a solution to reduce \nthese costs.\nHow can the solutions architect meet this requirement?\n회사는 동일한 AWS 리전에 있는 Amazon S3 버킷에서 사진을 자주 업로드 및 다운로드해야 하는 사\n진 처리 애플리케이션을 실행합니다 . 솔루션 설계자는 데이터 전송 비용이 증가한다는 사실을 알게 되\n었고 이러한 비용을 줄이기 위한 솔루션 을 구현해야 합니다 .\n솔루션 설계자는 이 요구 사항을 어떻게 충족할 수 있습니까 ?\n​",
    "보기": {
      "A": "Deploy Amazon API Gateway into a public subnet and adjust the route table to route S3 calls \nthrough it.\nAmazon API Gateway 를 퍼블릭 서브넷에 배포하고 이를 통해 S3 호출을 라우팅하도록 라우팅 테이블\n을 조정합니다 .\n​",
      "B": "Deploy a NAT gateway into a public subnet and attach an endpoint policy that allows access to the \nS3 buckets.\nNAT 게이트웨이를 퍼블릭 서브넷에 배포하고 S3 버킷에 대한 액세스를 허용하는 엔드포인트 정책을 \n연결합니다 .\n​",
      "C": "Deploy the application into a public subnet and allow it to route through an internet gateway to \naccess the S3 buckets.\n​ 애플리케이션을 퍼블릭 서브넷에 배포하고 S3 버킷에 액세스하기 위해 인터넷 게이트웨이를 통해 라\n우팅하도록 허용합니다 .\n​",
      "D": "Deploy an S3 VPC gateway endpoint into the VPC and attach an endpoint policy that allows access \nto the S3 buckets.\n​S3 VPC 게이트웨이 엔드포인트를 VPC에 배포하고 S3 버킷에 대한 액세스를 허용하는 엔드포인트 정\n책을 연결합니다 .\n​\n→ 비용을 줄이기 위해서는 NAT 게이트웨이를 제거하고 , S3에대한 VPC 엔드포인트를 활용해야 한다.\n​"
    },
    "정답": "D"
  },
  {
    "문제번호": 73,
    "질문": "A company recently launched Linux-based application instances on Amazon EC2 in a private subnet \nand launched a Linux-based bastion host on an Amazon EC2 instance in a public subnet of a VPC. \nA solutions architect needs to connect from the on-premises network, through the company's internet \nconnection, to the bastion host, and to the application servers. The solutions architect must make \nsure that the security groups of all the EC2 instances will allow that access.\nWhich combination of steps should the solutions architect take to meet these requirements? (Choose \ntwo.)\n한 회사는 최근 프라이빗 서브넷의 Amazon EC2에서 Linux 기반 애플리케이션 인스턴스를 시작하고 \nVPC의 퍼블릭 서브넷에 있는 Amazon EC2 인스턴스에서 Linux 기반 배스천 호스트를 시작했습니다 . \n솔루션 설계자는 사내 네트워크에서 회사의 인터넷 연결을 통해 배스천 호스트 및 애플리케이션 서버\n에 연결해야 합니다 . 솔루션 설계자는 모든 EC2 인스턴스의 보안 그룹이 해당 액세스를 허용하는지 확\n인해야 합니다 .\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 어떤 단계 조합을 취해야 합니까 ? (2개를 선택하\n세요.)\n​",
    "보기": {
      "C": "Replace the current security group of the bastion host with one that only allows inbound access from \nthe external IP range for the company.\n​배스천 호스트의 현재 보안 그룹을 회사의 외부 IP 범위에서만 인바운드 액세스를 허용하는 보안 그룹\n으로 교체합니다 .\n→ 온프레미스 네트워크에서 인터넷 (온프레미스 리소스의 공개 IP사용)을 통해 배스천 호스트로 액세스 \n가능.(외부 IP에서만 액세스 허용) \n​",
      "A": "Replace the current security group of the bastion host with one that only allows inbound access from \nthe application instances.\n배스천 호스트의 현재 보안 그룹을 애플리케이션 인스턴스의 인바운드 액세스만 허용하는 보안 그룹으\n로 교체합니다 .\n​",
      "B": "Replace the current security group of the bastion host with one that only allows inbound access from \nthe internal IP range for the company.\n배스천 호스트의 현재 보안 그룹을 회사의 내부 IP 범위에서만 인바운드 액세스를 허용하는 보안 그룹\n으로 교체합니다 .\n​",
      "D": "Replace the current security group of the application instances with one that allows inbound SSH \naccess from only the private IP address of the bastion host.\n애플리케이션 인스턴스의 현재 보안 그룹을 배스천 호스트의 개인 IP 주소에서만 인바운드 SSH 액세\n스를 허용하는 보안 그룹으로 교체합니다 .\n→ 배스천과 EC2가 동일한 VPC에 있으므로 배스천이 프라이빗 IP 주소를 통해 EC2와 통신할 수 있음\n을 의미한다 .\n(배스천 호스트의 사설 IP에서 인바운드 SSH 허용).\n​\n배스천 호스트 (Bastion Host)\n: 출입 차단 소프트웨어가 설치되어 내부와 외부 네트워크 사이에서 일종의 게이트 역할을 수행하는 호\n스트\n​"
    },
    "정답": "C"
  },
  {
    "문제번호": 74,
    "질문": "A solutions architect is designing a two-tier web application. The application consists of a \npublic-facing web tier hosted on Amazon EC2 in public subnets. The database tier consists of \nMicrosoft SQL Server running on Amazon EC2 in a private subnet. Security is a high priority for the \ncompany.\nHow should security groups be configured in this situation? (Choose two.)\n솔루션 설계자는 2계층(two-tier) 웹 애플리케이션을 설계하고 있습니다 . 애플리케이션은 퍼블릭 서브넷\n의 Amazon EC2에서 호스팅되는 퍼블릭 웹 티어로 구성됩니다 . 데이터베이스 계층은 프라이빗 서브넷\n의 Amazon EC2에서 실행되는 Microsoft SQL Server 로 구성됩니다 . 보안은 회사의 최우선 과제입니다 .\n이 상황에서 보안 그룹을 어떻게 구성해야 합니까 ? (2개를 선택하세요 .)\n​",
    "보기": {
      "A": "Configure the security group for the web tier to allow inbound traffic on port 443 from 0.0.0.0/0.\n0.0.0.0/0 에서 포트 443의 인바운드 트래픽을 허용하도록 웹 계층에 대한 보안 그룹을 구성합니다 .\n→ 모든 IPv4 주소에서 인바운드 HTTPS 액세스를 허용한다 .\n​\n웹 서버 규칙\n: 임의의 IP 주소로부터 HTTP 및 HTTPS 액세스를 허용한다 . \nVPC가 IPv6용으로 활성화되면 IPv6 주소에서 인바운드 HTTP 및 HTTPS 트래픽을 제어하기 위한 규\n칙을 추가할 수 있다.\n​",
      "B": "Configure the security group for the web tier to allow outbound traffic on port 443 from 0.0.0.0/0.\n0.0.0.0/0 에서 포트 443의 아웃바운드 트래픽을 허용하도록 웹 계층에 대한 보안 그룹을 구성합니다 .\n​",
      "C": "Configure the security group for the database tier to allow inbound traffic on port 1433 from the \nsecurity group for the web tier.\n웹 계층에 대한 보안 그룹에서 포트 1433의 인바운드 트래픽을 허용하도록 데이터베이스 계층에 대한 \n보안 그룹을 구성합니다 .​\n​→ 1433(MS SQL)Microsoft SQL Server 데이터베이스에 액세스하기 위한 기본 포트\n​\n데이터베이스 서버 규칙\n: 인바운드 규칙은 인스턴스에서 실행 중인 데이터베이스의 유형에 따라 데이터베이스 액세스를 위해 \n추가할 수 있는 \n규칙을 예로 든 것이다 . 프로토콜 \n유형프로토콜 \n번호포트 소스 IP 참고\nTCP 6 80(HTTP) 0.0.0.0/0임의의 IPv4 주소에서 인바운드 HTTP 액세스\n를 허용함\nTCP 6443(HTTP\nS)0.0.0.0/0임의의 IPv4 주소에서 인바운드 HTTPS 액세\n스를 허용함\nTCP 6 80(HTTP) ::/0임의의 IPv6 주소에서 인바운드 HTTP 액세스\n를 허용함\nTCP 6443(HTTP\nS)::/0임의의 IPv6 주소에서 인바운드 HTTPS 액세\n스를 허용함\n프로\n토콜 \n유형프로토\n콜 번\n호포트 참고\nTCP 6 1433(MS SQL)Microsoft SQL Server 데이터베이스 액세스를 위한 기본 \n포트\n(예: Amazon RDS 인스턴스에서 )\nTCP 63306(MYSQL/Aur\nora)MySQL 또는 Aurora 데이터베이스 액세스를 위한 기본 포\n트\n(예: Amazon RDS 인스턴스에서 )",
      "D": "Configure the security group for the database tier to allow outbound traffic on ports 443 and 1433 to \nthe security group for the web tier.\n데이터베이스 계층의 보안 그룹을 구성하여 포트 443 및 1433의 아웃바운드 트래픽을 웹 계층의 보안 \n그룹으로 보냅니다 .\n​\nE. \nConfigure the security group for the database tier to allow inbound traffic on ports 443 and 1433 \nfrom the security group for the web tier.\n​웹 계층에 대한 보안 그룹의 포트 443 및 1433에서 인바운드 트래픽을 허용하도록 데이터베이스 계층\n에 대한 보안 그룹을 구성합니다 .\n​"
    },
    "정답": "A"
  },
  {
    "문제번호": 75,
    "질문": "A company wants to move a multi-tiered application from on premises to the AWS Cloud to improve \nthe application's performance. The application consists of application tiers that communicate with each \nother by way of RESTful services. Transactions are dropped when one tier becomes overloaded. A \nsolutions architect must design a solution that resolves these issues and modernizes the application.\nWhich solution meets these requirements and is the MOST operationally efficient?\n​한 회사에서 애플리케이션의 성능을 개선하기 위해 다계층 애플리케이션을 온프레미스에서 AWS 클라\n우드로 이동하려고 합니다 . 애플리케이션은 *RESTful 서비스를 통해 서로 통신하는 애플리케이션 계층\n으로 구성됩니다 . 한 계층이 오버로드되면 트랜잭션이 삭제됩니다 . 솔루션 설계자는 이러한 문제를 해\n결하고 애플리케이션을 현대화 하는 솔루션을 설계해야 합니다 .\n어떤 솔루션이 이러한 요구 사항을 충족하고 운영상 가장 효율적입니까 ?\n* RESTful API : RESTful API는 두 컴퓨터 시스템이 인터넷을 통해 정보를 안전하게 교환하기 위해 사\n용하는 인터페이스 .\n​",
    "보기": {
      "A": "Use Amazon API Gateway and direct transactions to the AWS Lambda functions as the application \nlayer. Use Amazon Simple Queue Service (Amazon SQS) as the communication layer between \napplication services.\nAmazon API Gateway 를 사용하고 애플리케이션 계층으로 AWS Lambda 함수에 트랜잭션을 전달합니\n다. Amazon Simple Queue Service(Amazon SQS)를 애플리케이션 서비스 간의 통신 계층으로 사용합\n니다.\n→ AWS Lambda 는 서버리스 서비스 + 자동 크기 조정 (현대화 )\nAmazon SQS 는 분리 (더이상 삭제 없음)\nAmazon SQS \n: 내구력 있고 가용성이 뛰어난 보안 호스팅 대기열을 제공하며 이를 통해 분산 소프트웨어 시스템과 \n구성 요소를 통합 및\n분리할 수 있다.",
      "B": "Use Amazon CloudWatch metrics to analyze the application performance history to determine the \nservers' peak utilization during the performance failures. Increase the size of the application server's \nAmazon EC2 instances to meet the peak requirements.\nAmazon CloudWatch 지표를 사용하여 애플리케이션 성능 기록을 분석하여 성능 장애 동안 서버의 최\n대 사용률을 결정합니다 . 최대 요구 사항을 충족하도록 애플리케이션 서버의 Amazon EC2 인스턴스 크\n기를 늘립니다 .\n​TCP 6 5439(Redshift)Amazon Redshift 클러스터 데이터베이스 액세스를 위한 \n기본 포트.\nTCP 6 5432(PostgreSQL)PostgreSQL 데이터베이스 액세스를 위한 기본 포트\n(예: Amazon RDS 인스턴스에서 )\nTCP 6 1521(Oracle)Oracle 데이터베이스 액세스를 위한 기본 포트\n(예: Amazon RDS 인스턴스에서 )",
      "C": "Use Amazon Simple Notification Service (Amazon SNS) to handle the messaging between application \nservers running on Amazon EC2 in an Auto Scaling group. Use Amazon CloudWatch to monitor the \nSNS queue length and scale up and down as required.\n​ Amazon Simple Notification Service(Amazon SNS)를 사용하여 Auto Scaling 그룹의 Amazon EC2에서 \n실행되는 애플리케이션 서버 간의 메시징을 처리합니다 . Amazon CloudWatch 를 사용하여 SNS 대기열 \n길이를 모니터링하고 필요에 따라 확장 및 축소합니다 .\n​",
      "D": "Use Amazon Simple Queue Service (Amazon SQS) to handle the messaging between application \nservers running on Amazon EC2 in an Auto Scaling group. Use Amazon CloudWatch to monitor the \nSQS queue length and scale up when communication failures are detected.\n​Amazon Simple Queue Service(Amazon SQS)를 사용하여 Auto Scaling 그룹의 Amazon EC2에서 실행\n되는 애플리케이션 서버 간의 메시징을 처리합니다 . Amazon CloudWatch 를 사용하여 SQS 대기열 길\n이를 모니터링하고 통신 오류가 감지되면 확장합니다 .\n​"
    },
    "정답": "A"
  },
  {
    "문제번호": 76,
    "질문": "A company receives 10 TB of instrumentation data each day from several machines located at a \nsingle factory. The data consists of JSON files stored on a storage area network (SAN) in an \non-premises data center located within the factory. The company wants to send this data to Amazon \nS3 where it can be accessed by several additional systems that provide critical near-real-time \nanalytics. A secure transfer is important because the data is considered sensitive.\nWhich solution offers the MOST reliable data transfer?\n​회사는 단일 공장에 있는 여러 기계에서 매일 10TB의 계측 데이터를 수신합니다 . 데이터는 공장 내에 \n위치한 온프레미스 데이터 센터의 SAN(Storage Area Network) 에 저장된 JSON 파일로 구성됩니다 . 회\n사는 이 데이터를 Amazon S3로 전송하여 중요한 실시간에 가까운 분석을 제공하는 여러 추가 시스템\n에서 액세스할 수 있기를 원합니다 . 데이터가 민감한 것으로 간주되기 때문에 안전한 전송이 중요합니\n다.\n가장 안정적인 데이터 전송을 제공하는 솔루션은 무엇입니까 ?\n​",
    "보기": {
      "A": "AWS DataSync over public internet\n공용 인터넷을 통한 AWS DataSync\n→ 공용 인터넷은 신뢰하기 어렵다 .\n​",
      "B": "AWS DataSync over AWS Direct Connect\nAWS Direct Connect 를 통한 AWS DataSync\n​\nAWS DataSync\n: AWS로의 데이터 마이그레이션을 간소화 및 가속화하고 온프레미스 스토리지 , 엣지 로케이션 , 다른 \n클라우드 및 AWS 스토리지 간에 데이터를 빠르고 안전하게 이동하는 데 도움이 되는 온라인 데이터 \n이동 및 검색 서비스 .\n​\n▶오답",
      "C": "AWS Database Migration Service (AWS DMS) over public internet\n​공용 인터넷을 통한 AWS Database Migration Service(AWS DMS)\n​",
      "D": "AWS Database Migration Service (AWS DMS) over AWS Direct Connect\n​AWS Direct Connect 를 통한 AWS Database Migration Service(AWS DMS)\n→ DMS은 데이터베이스용으로 올바르지않다 ."
    },
    "정답": "B"
  },
  {
    "문제번호": 77,
    "질문": "A company needs to configure a real-time data ingestion architecture for its application. The \ncompany needs an API, a process that transforms data as the data is streamed, and a storage \nsolution for the data.\nWhich solution will meet these requirements with the LEAST operational overhead?\n​회사는 애플리케이션에 대한 실시간 데이터 수집 아키텍처를 구성해야 합니다 . 회사에는 데이터가 스트\n리밍될 때 데이터를 변환하는 프로세스인 API와 데이터를 위한 스토리지 솔루션 이 필요합니다 .\n최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까 ?\n​",
    "보기": {
      "A": "Deploy an Amazon EC2 instance to host an API that sends data to an Amazon Kinesis data stream. \nCreate an Amazon Kinesis Data Firehose delivery stream that uses the Kinesis data stream as a \ndata source. Use AWS Lambda functions to transform the data. Use the Kinesis Data Firehose \ndelivery stream to send the data to Amazon S3.\nAmazon EC2 인스턴스를 배포하여 Amazon Kinesis 데이터 스트림으로 데이터를 전송하는 API를 호스\n팅합니다 . Kinesis 데이터 스트림을 데이터 원본으로 사용하는 Amazon Kinesis Data Firehose 전송 스\n트림을 생성합니다 . AWS Lambda 함수를 사용하여 데이터를 변환합니다 . Kinesis Data Firehose 전송 \n스트림을 사용하여 데이터를 Amazon S3로 보냅니다 .\n​",
      "B": "Deploy an Amazon EC2 instance to host an API that sends data to AWS Glue. Stop \nsource/destination checking on the EC2 instance. Use AWS Glue to transform the data and to send \nthe data to Amazon S3.\nAmazon EC2 인스턴스를 배포하여 AWS Glue에 데이터를 전송하는 API를 호스팅합니다 . EC2 인스턴\n스에서 소스/대상 확인을 중지합니다 . AWS Glue를 사용하여 데이터를 변환하고 데이터를 Amazon S3\n로 보냅니다 .\n​\n→ API를 호스팅하기 위해 EC2 인스턴스를 배포할 필요가 없다.\n​",
      "C": "Configure an Amazon API Gateway API to send data to an Amazon Kinesis data stream. Create an \nAmazon Kinesis Data Firehose delivery stream that uses the Kinesis data stream as a data source. \nUse AWS Lambda functions to transform the data. Use the Kinesis Data Firehose delivery stream to \nsend the data to Amazon S3.\n​Amazon Kinesis 데이터 스트림으로 데이터를 보내도록 Amazon API Gateway API를 구성합니다 . \nKinesis 데이터 스트림을 데이터 원본으로 사용하는 Amazon Kinesis Data Firehose 전송 스트림을 생\n성합니다 . AWS Lambda 함수를 사용하여 데이터를 변환합니다 . Kinesis Data Firehose 전송 스트림을 \n사용하여 데이터를 Amazon S3로 보냅니다 .\n​\nAmazon API Gateway API\n: 어떤 규모에서든 개발자가 API를 손쉽게 게시, 유지 관리, 모니터링 및 보호할 수 있도록 지원하는 완\n전관리형 서비스 . \n트래픽 관리, 권한 부여 및 액세스 제어, 모니터링 , API 버전 관리를 비롯해 최대 수십만 건의 동시 \nAPI 호출을 수락 및 \n처리하는 데 관련된 모든 작업을 처리함 .\n​\n▶오답",
      "D": "Configure an Amazon API Gateway API to send data to AWS Glue. Use AWS Lambda functions to \ntransform the data. Use AWS Glue to send the data to Amazon S3.\n​데이터를 AWS Glue로 보내도록 Amazon API Gateway API를 구성합니다 . AWS Lambda 함수를 사용\n하여 데이터를 변환합니다 . AWS Glue를 사용하여 데이터를 Amazon S3로 보냅니다 .\n→ AWS Glue는 S3에서 데이터를 가져오며 , 자체적으로 ETL을 수행할 수 있으므로 Lambda 가 필요하\n지 않다.\n​\nAWS Glue\n: 분석, 기계 학습 및 애플리케이션 개발을 위해 데이터를 쉽게 탐색, 준비, 그리고 조합할 수 있도록 \n지원하는 \n서버리스 데이터 통합 서비스 .\nAWS Glue 데이터 카탈로그를 사용하여 데이터를 쉽게 찾고 액세스할 수 있다. \n데이터 엔지니어와 ETL (추출, 변형 및 로드) 개발자는 AWS Glue Studio 에서 몇 번의 클릭으로 ETL \n워크플로를 \n시각적으로 생성, 실행 및 모니터링할 수 있다."
    },
    "정답": "C"
  },
  {
    "문제번호": 78,
    "질문": "A company needs to keep user transaction data in an Amazon DynamoDB table. The company must \nretain the data for 7 years.\nWhat is the MOST operationally efficient solution that meets these requirements?\n​회사는 사용자 트랜잭션 데이터를 Amazon DynamoDB 테이블 에 보관해야 합니다 . 회사는 데이터를 7\n년간 보관해야 합니다 .\n이러한 요구 사항을 충족하는 가장 운영 효율성이 높은 솔루션은 무엇입니까 ?\n​",
    "보기": {
      "A": "Use DynamoDB point-in-time recovery to back up the table continuously.\nDynamoDB 지정 시간 복구를 사용하여 테이블을 지속적으로 백업합니다 .\n​→PITR은 테이블의 지속적인 백업을 제공하고 테이블 데이터를 이전 35일의 특정 시점으로 복원할 수 \n있도록 한다. \n35일 이상 데이터 백업을 저장해야 하는 경우 주문형 백업을 사용할 수 있다. \n​",
      "B": "Use AWS Backup to create backup schedules and retention policies for the table.\nAWS Backup 을 사용하여 테이블에 대한 백업 일정 및 보존 정책을 생성합니다 .\n​\n→ AWS Backup 을 사용하여 Amazon DynamoDB 에 대한 예약 백업 설정\nAWS DynamoDB\n: 지정 시간 복구(PITR) 및 온디맨드 백업의 두 가지 백업 유형을 제공한다 . PITR은 롤링 35일 창의 특\n정 시점으로 테이블을 복구하는 데 사용되며 , \n고객이 잘못된 코드, 악의적인 액세스 또는 사용자 오류로부터 실수로 테이블을 삭제하거나 쓰는 것을 \n완화하는 데 사용된다 . \n온 디맨드 백업은 일반적으로 고객이 규정 준수 및 규정 요구 사항을 충족하도록 지원하는 데 사용되\n는 장기 보관 및 보존을 위해 설계되었다 .\n​\n​ AWS Backup\n: 데이터 보호를 쉽게 중앙 집중화하고 자동화할 수 있는 종합 관리형 서비스 . \n•대규모 데이터 보호를 더욱 간소화하는 비용 효율적인 완전 관리형 정책 기반 서비스를 제공\n•정기적 또는 향후 백업을 예약할 수 있음.\n•백업 계획에는 리소스에 대한 일정 및 보존 정책이 포함되며 , AWS Backup 은 보존 일정에 따\n라 백업을 생성하고 이전 백업을 삭제한다 .\n•예약 및 삭제를 자동화하여 수동으로 온디맨드 백업을 만들고 삭제하는 획일적인 무거운 작업\n을 제거\n​\n​\n▶오답",
      "C": "Create an on-demand backup of the table by using the DynamoDB console. Store the backup in an \nAmazon S3 bucket. Set an S3 Lifecycle configuration for the S3 bucket.\n​DynamoDB 콘솔을 사용하여 테이블의 주문형 백업을 생성합니다 . 백업을 Amazon S3 버킷에 저장합니\n다. S3 버킷에 대한 S3 수명 주기 구성을 설정합니다 .\n​→ DynamoDB 콘솔X\n​",
      "D": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule to invoke an AWS Lambda \nfunction. Configure the Lambda function to back up the table and to store the backup in an Amazon \nS3 bucket. Set an S3 Lifecycle configuration for the S3 bucket.\n​AWS Lambda 함수를 호출하는 Amazon EventBridge(Amazon CloudWatch Events) 규칙을 생성합니다 . \n테이블을 백업하고 Amazon S3 버킷에 백업을 저장하도록 Lambda 함수를 구성합니다 . S3 버킷에 대\n한 S3 수명 주기 구성을 설정합니다 .\n​​→ Amazon DynamoDB 는 지정 시간 복구(PITR) 및 주문형 백업의 두 가지 백업 유형을 제공한다 . ​"
    },
    "정답": "B"
  },
  {
    "문제번호": 79,
    "질문": "A company is planning to use an Amazon DynamoDB table for data storage. The company is \nconcerned about cost optimization. The table will not be used on most mornings. In the evenings, \nthe read and write traffic will often be unpredictable. When traffic spikes occur, they will happen very \nquickly.\nWhat should a solutions architect recommend?\n회사에서 데이터 저장을 위해 Amazon DynamoDB 테이블을 사용할 계획입니다 . 회사는 비용 최적화 에 \n대해 우려하고 있습니다 . 대부분의 아침에는 테이블을 사용하지 않습니다 . 저녁에는 읽기 및 쓰기 트래\n픽이 예측할 수 없는 경우가 많습니다 .트래픽 급증이 발생하면 매우 빠르게 발생합니다 .\n솔루션 아키텍트는 무엇을 추천해야 합니까 ?\n​",
    "보기": {
      "A": "Create a DynamoDB table in on-demand capacity mode.\n온디맨드 용량 모드에서 DynamoDB 테이블을 생성합니다 .\n​\nAmazon DynamoDB 의 On-demand mode (주문형 모드)\n: Amazon DynamoDB 온디맨드는 용량 계획 없이 초당 수천 개의 요청을 처리할 수 있는 유연한 결제 \n옵션이다 . \nDynamoDB 온디맨드는 읽기 및 쓰기 요청에 대해 요청당 지불 요금을 제공하므로 사용한 만큼만 비용\n을 지불하면 된다.\n온디맨드 모드를 선택하면 DynamoDB 는 워크로드가 이전에 도달한 트래픽 수준으로 증가하거나 감소\n할 때 즉시 \n수용한다 . 워크로드의 트래픽 수준이 새로운 피크에 도달하면 DynamoDB 는 워크로드를 수용하도록 빠\n르게 적응한다 .\n​\nOn-demand mode (주문형 모드)가 적합한 환경\n•알 수 없는 작업 부하가 있는 새 테이블을 만듭니다 .\n•예측할 수 없는 애플리케이션 트래픽이 있습니다 .\n•사용한 만큼만 지불하는 간편한 방식을 선호합니다 .\n​\n▶오답",
      "B": "Create a DynamoDB table with a global secondary index.\n글로벌 보조 인덱스가 있는 DynamoDB 테이블을 생성합니다 .\n​→ 예측할 수 없는 트래픽과는 관련이 없음.\n​",
      "C": "Create a DynamoDB table with provisioned capacity and auto scaling.\n프로비저닝된 용량 및 Auto Scaling 을 사용하여 DynamoDB 테이블을 생성합니다 .​\n​",
      "D": "Create a DynamoDB table in provisioned capacity mode, and configure it as a global table.\n​프로비저닝된 용량 모드에서 DynamoDB 테이블을 생성하고 전역 테이블로 구성합니다 .\n→ 프로비저닝된 용량 모드에서는 지정된 패턴이 권장된다 ."
    },
    "정답": "A"
  },
  {
    "문제번호": 80,
    "질문": "A company recently signed a contract with an AWS Managed Service Provider (MSP) Partner for \nhelp with an application migration initiative. A solutions architect needs ta share an Amazon Machine \nImage (AMI) from an existing AWS account with the MSP Partner's AWS account. The AMI is \nbacked by Amazon Elastic Block Store (Amazon EBS) and uses an AWS Key Management Service \n(AWS KMS) customer managed key to encrypt EBS volume snapshots.\nWhat is the MOST secure way for the solutions architect to share the AMI with the MSP Partner's \nAWS account?\n​한 회사는 최근 애플리케이션 마이그레이션 이니셔티브에 대한 지원을 위해 AWS 관리형 서비스 공급\n자(MSP) 파트너와 계약을 체결했습니다 . 솔루션 설계자는 기존 AWS 계정의 Amazon 머신 이미지\n(AMI)를 MSP 파트너의 AWS 계정과 공유해야 합니다 . AMI는 Amazon Elastic Block Store(Amazon \nEBS)의 지원을 받으며 AWS Key Management Service(AWS KMS) 고객 관리형 키를 사용하여 EBS 볼\n륨 스냅샷을 암호화 합니다 .\n솔루션 설계자가 MSP 파트너의 AWS 계정과 AMI를 공유하는 가장 안전한 방법은 무엇입니까 ?\n​",
    "보기": {
      "A": "Make the encrypted AMI and snapshots publicly available. Modify the key policy to allow the MSP \nPartner's AWS account to use the key.\n암호화된 AMI 및 스냅샷을 공개적으로 사용할 수 있도록 합니다 . MSP 파트너의 AWS 계정이 키를 사\n용할 수 있도록 키 정책을 수정합니다 .\n​",
      "B": "Modify the launchPermission property of the AMI. Share the AMI with the MSP Partner's AWS \naccount only. Modify the key policy to allow the MSP Partner's AWS account to use the key.\nAMI의 *launchPermission 속성을 수정합니다 . MSP 파트너의 AWS 계정과만 AMI를 공유하십시오 . \nMSP 파트너의 AWS 계정이 키를 사용할 수 있도록 키 정책을 수정합니다 .\n→ EBS 스냅샷이 암호화된 경우 액세스할 수 있도록 동일한 KMS 키를 파트너와 공유해야 한다.\n​\nAWS 계정과 AMI 공유시 고려 사항\n공유 제한 태그 암호화 및 키 리전\n​공유 AMI에 연결하는 \n사용자 정의 태그는 \nAWS 계정에서만 사용\n할 수 있으며 AMI가 \n공유되는 다른 계정에\n서는 사용할 수 없다.- 암호화된 스냅샷은 \nKMS 키로 암호화해야 \n한다. 기본 AWS 관리\n형 키로 암호화된 스냅\n샷으로 지원되는 AMI\n는 공유할 수 없다. AMI는 리전 리소스이\n다. AMI를 공유하면 \n해당 리전에서만 사용\n할 수 있다. 다른 리전\n에서 AMI를 사용할 수 \n있게 하려면 AMI를 리\n* AMI의 launchPermission 속성\n: 각 AMI에는 소유자를 제외하고 해당 AMI를 사용한 인스턴스 시작이 허용된 launchPermission 계정을 \n설정할 수 있는 AWS 속성이 존재한다 .\nAMI의 launchPermission 속성을 변경하여 이 AMI를 퍼블릭 설정(모든 AWS 계정에 시작 권한 허용)하\n거나 사용자가 지정한 AWS 계정과만 공유할 수 있다.\n​",
      "C": "Modify the launchPermission property of the AMI. Share the AMI with the MSP Partner's AWS \naccount only. Modify the key policy to trust a new KMS key that is owned by the MSP Partner for \nencryption.\n​AMI의 launchPermission 속성을 수정합니다 . MSP 파트너의 AWS 계정과만 AMI를 공유하십시오 . 암호\n화를 위해 MSP 파트너가 소유한 새 KMS 키를 신뢰하도록 키 정책을 수정합니다 .\n​",
      "D": "Export the AMI from the source account to an Amazon S3 bucket in the MSP Partner's AWS \naccount, Encrypt the S3 bucket with a new KMS key that is owned by the MSP Partner. Copy and \nlaunch the AMI in the MSP Partner's AWS account.\n​소스 계정에서 MSP 파트너의 AWS 계정에 있는 Amazon S3 버킷으로 AMI를 내보내고 MSP 파트너가 \n소유한 새 KMS 키로 S3 버킷을 암호화합니다 . MSP 파트너의 AWS 계정에서 AMI를 복사하고 시작합\n니다.\n​"
    },
    "정답": "B"
  },
  {
    "문제번호": 81,
    "질문": "A solutions architect is designing the cloud architecture for a new application being deployed on \nAWS. The process should run in parallel while adding and removing application nodes as needed \nbased on the number of jobs to be processed. The processor application is stateless. The solutions \narchitect must ensure that the application is loosely coupled and the job items are durably stored.\nWhich design should the solutions architect use?\n​솔루션 설계자는 AWS에 배포되는 새 애플리케이션을 위한 클라우드 아키텍처를 설계하고 있습니다 . \n처리할 작업 수에 따라 필요에 따라 애플리케이션 노드를 추가 및 제거하면서 프로세스가 병렬로 실행\n되어야 합니다 . 프로세서 응용 프로그램은 상태 비저장입니다 . 솔루션 설계자는 응용 프로그램이 느슨\n하게 연결되어 있고 작업 항목이 영구적으로 저장되어 있는지 확인해야 합니다 .\n솔루션 설계자는 어떤 디자인을 사용해야 합니까 ?\n​",
    "보기": {
      "A": "- 암호화된 스냅샷으로 \n지원되는 AMI를 공유\n하는 경우 AWS 계정\n이 스냅샷을 암호화하\n는 데 사용된 KMS 키\n를 사용하도록 허용해\n야 한다.전에 복사한 다음 공유\n한다.\n사용법 공유 AMI 복사 결제 ​\nAMI를 공유하면 사용\n자는 AMI에서만 인스\n턴스를 시작할 수 있\n다. 삭제, 공유 또는 \n수정할 수 없다. 그러\n나 AMI를 사용하여 인\n스턴스를 시작한 후에\n는 해당 인스턴스에서 \nAMI를 생성할 수 있\n다.다른 계정의 사용자가 \n공유 AMI를 복사하려\n는 경우 AMI를 지원하\n는 스토리지에 대한 읽\n기 권한을 부여해야 한\n다. 다른 AWS 계정에서 \n인스턴스를 시작하기 \n위해 AMI를 사용하는 \n경우에는 비용이 청구\n되지 않는다 . AMI를 \n사용하여 인스턴스를 \n시작하는 계정에는 시\n작하는 인스턴스에 대\n한 요금이 청구된다 .​\nCreate an Amazon SNS topic to send the jobs that need to be processed. Create an Amazon \nMachine Image (AMI) that consists of the processor application. Create a launch configuration that \nuses the AMI. Create an Auto Scaling group using the launch configuration. Set the scaling policy \nfor the Auto Scaling group to add and remove nodes based on CPU usage.\n처리해야 하는 작업을 보낼 Amazon SNS 주제를 생성합니다 . 프로세서 애플리케이션으로 구성된 \nAmazon 머신 이미지 (AMI)를 생성합니다 . AMI를 사용하는 시작 구성을 생성합니다 . 시작 구성을 사용\n하여 Auto Scaling 그룹을 생성합니다 . CPU 사용량에 따라 노드를 추가 및 제거하도록 Auto Scaling \n그룹에 대한 조정 정책을 설정합니다 .\n​",
      "B": "Create an Amazon SQS queue to hold the jobs that need to be processed. Create an Amazon \nMachine Image (AMI) that consists of the processor application. Create a launch configuration that \nuses the AMI. Create an Auto Scaling group using the launch configuration. Set the scaling policy \nfor the Auto Scaling group to add and remove nodes based on network usage.\n처리해야 하는 작업을 보관할 Amazon SQS 대기열을 생성합니다 . 프로세서 애플리케이션으로 구성된 \nAmazon 머신 이미지 (AMI)를 생성합니다 . AMI를 사용하는 시작 구성을 생성합니다 . 시작 구성을 사용\n하여 Auto Scaling 그룹을 생성합니다 . Auto Scaling 그룹의 조정 정책을 설정하여 네트워크 사용량에 \n따라 노드를 추가 및 제거합니다 .\n​",
      "C": "Create an Amazon SQS queue to hold the jobs that need to be processed. Create an Amazon \nMachine Image (AMI) that consists of the processor application. Create a launch template that uses \nthe AMI. Create an Auto Scaling group using the launch template. Set the scaling policy for the Auto \nScaling group to add and remove nodes based on the number of items in the SQS queue.\n​\nAmazon Simple Queue Service (Amazon SQS)\n: 마이크로서비스 , 분산 시스템 및 서버리스 애플리케이션을 분리하고 확장할 수 있는 완전 관리형 메\n시지 대기열 서비스 . \n•메시지 지향 미들웨어 관리 및 운영과 관련된 복잡성과 오버헤드를 제거하고 개발자가 작업 \n차별화에 집중할 수 있게한다 .\n•메시지를 손실하거나 다른 서비스를 사용할 필요 없이 모든 볼륨의 소프트웨어 구성 요소 간\n에 메시지를 보내고 , 저장하고 , 받을 수 있다.\n•AWS 콘솔, 원하는 명령줄 인터페이스 또는 SDK, 세 가지 간단한 명령을 사용하여 몇 분 만\n에 SQS를 시작가능하다 .\n•두 가지 유형의 메시지 대기열을 제공\n- 표준 대기열 : 최대 처리량 , 최선형 주문 및 최소 한 번 배달을 제공\n- SQS FIFO 대기열 : 메시지가 전송된 정확한 순서로 정확히 한 번 처리되도록 설계\n​\nAmazon SQS 기반 확장\n: Amazon SQS 대기열의 활동에 따라 확장할 수 있는 몇 가지 시나리오가 있다. \n예를 들어 사용자가 이미지를 업로드하고 온라인으로 사용할 수 있는 웹 앱이 있다고 가정할때 , 이 시\n나리오에서 각 이미지를 게시하려면 크기 조정 및 인코딩이 필요하다 . \n앱은 자동 확장 그룹의 EC2 인스턴스에서 실행되며 일반적인 업로드 속도를 처리하도록 구성된다 . 비\n정상인 인스턴스는 종료되고 \n교체되어 항상 현재 인스턴스 수준을 유지한다 . 앱은 처리를 위해 이미지의 원시 비트맵 데이터를 SQS \n대기열에 배치한다 . \n이미지를 처리한 다음 사용자가 볼 수 있는 위치에 처리된 이미지를 게시한다 . 이 시나리오의 아키텍처\n는 이미지 업로드 수가 시간에 따라 달라지지 않으면 잘 작동한다 . 그러나 업로드 수가 시간이 지남에 \n따라 변경되는 경우 동적 확장을 사용하여 자동 확장 그룹의 \n용량을 확장하는 것을 고려할 수 있다.",
      "D": "Create an Amazon SNS topic to send the jobs that need to be processed. Create an Amazon \nMachine Image (AMI) that consists of the processor application. Create a launch template that uses \nthe AMI. Create an Auto Scaling group using the launch template. Set the scaling policy for the Auto \nScaling group to add and remove nodes based on the number of messages published to the SNS \ntopic.\n처리해야 하는 작업을 보낼 Amazon SNS 주제를 생성합니다 . 프로세서 애플리케이션으로 구성된 \nAmazon 머신 이미지 (AMI)를 생성합니다 . AMI를 사용하는 시작 템플릿을 생성합니다 . 시작 템플릿을 \n사용하여 Auto Scaling 그룹을 생성합니다 . SNS 주제에 게시된 메시지 수에 따라 노드를 추가 및 제거\n하도록 Auto Scaling 그룹에 대한 조정 정책을 설정합니다 .\n​"
    },
    "정답": null
  },
  {
    "문제번호": 82,
    "질문": "A company hosts its web applications in the AWS Cloud. The company configures Elastic Load \nBalancers to use certificates that are imported into AWS Certificate Manager (ACM). The company's \nsecurity team must be notified 30 days before the expiration of each certificate.\nWhat should a solutions architect recommend to meet this requirement?\n​회사는 AWS 클라우드에서 웹 애플리케이션을 호스팅합니다 . 회사는 AWS Certificate Manager(ACM) 로 \n가져온 인증서를 사용하도록 Elastic Load Balancer 를 구성합니다 . 각 인증서가 만료되기 30일 전에 회\n사 보안팀에 알려야 합니다 .\n솔루션 설계자는 이 요구 사항을 충족하기 위해 무엇을 권장해야 합니까 ?\n​",
    "보기": {
      "A": "Add a rule in ACM to publish a custom message to an Amazon Simple Notification Service (Amazon \nSNS) topic every day, beginning 30 days before any certificate will expire.\n​ACM에 규칙을 추가하여 인증서가 만료되기 30일 전부터 매일 Amazon Simple Notification \nService(Amazon SNS) 주제에 사용자 지정 메시지를 게시합니다 .\n​",
      "B": "Create an AWS Config rule that checks for certificates that will expire within 30 days. Configure \nAmazon EventBridge (Amazon CloudWatch Events) to invoke a custom alert by way of Amazon \nSimple Notification Service (Amazon SNS) when AWS Config reports a noncompliant resource.\n​30일 이내에 만료되는 인증서를 확인하는 AWS Config 규칙을 생성합니다 . AWS Config 가 비준수 리소\n스를 보고할 때 Amazon Simple Notification Service(Amazon SNS)를 통해 사용자 지정 알림을 호출하\n도록 Amazon EventBridge(Amazon CloudWatch Events) 를 구성합니다 .\n​\n→ ACM에서 가져온 인증서 만료가 가까워질 때 알림 설정 방법\n: ACM은 가져온 인증서에 대해 관리형 갱신을 제공하지 않으며 , 가져온 인증서를 갱신하려면 인증서 \n발급자에게 새 인증서를 요청해야 한다.\n그 다음 인증서를 다시 수동으로 ACM으로 가져온다 .\n​\nAWS Config 를 사용하여 만료 날짜가 가까워지는 인증서를 확인할 수 있다. 인증서 만료 날짜가 가까워\n지면 Amazon EventBridge 를\n사용하여 이메일 알림을 받을 수도 있다. AWS Config 규칙을 설정하기 전에 Amazon Simple \nNotification Service(SNS) 주제와 \nAmazon EventBridge 규칙이 생성되었는지 확인하면 모든 비준수 인증서가 만료 날짜 전에 알림을 트\n리거한다 .\n​\nEventBridge 규칙 생성\n: EventBridge 규칙과 함께 사용자 지정 이벤트 패턴을 사용하여 AWS Config 관리형 규칙 \nacm-certificate-expiration-check 와 일치시킨다 . \n그런 다음 응답을 Amazon Simple Notification Service 주제로 라우팅한다 .",
      "C": "Use AWS Trusted Advisor to check for certificates that will expire within 30 days. Create an Amazon \nCloudWatch alarm that is based on Trusted Advisor metrics for check status changes. Configure the \nalarm to send a custom alert by way of Amazon Simple Notification Service (Amazon SNS).\n​AWS Trusted Advisor 를 사용하여 30일 이내에 만료되는 인증서를 확인합니다 . 상태 변경 확인에 대한 \nTrusted Advisor 지표를 기반으로 하는 Amazon CloudWatch 경보를 생성합니다 . Amazon Simple \nNotification Service(Amazon SNS)를 통해 사용자 지정 알림을 보내도록 경보를 구성합니다 .\n​",
      "D": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule to detect any certificates that will \nexpire within 30 days. Configure the rule to invoke an AWS Lambda function. Configure the Lambda \nfunction to send a custom alert by way of Amazon Simple Notification Service (Amazon SNS).\n​30일 이내에 만료되는 모든 인증서를 감지하는 Amazon EventBridge(Amazon CloudWatch Events) 규\n칙을 생성합니다 . AWS Lambda 함수를 호출하도록 규칙을 구성합니다 . Amazon Simple Notification \nService(Amazon SNS)를 통해 사용자 지정 알림을 보내도록 Lambda 함수를 구성합니다 .\n​"
    },
    "정답": "B"
  },
  {
    "문제번호": 83,
    "질문": "A company's dynamic website is hosted using on-premises servers in the United States. The \ncompany is launching its product in Europe, and it wants to optimize site loading times for new \nEuropean users. The site's backend must remain in the United States. The product is being \nlaunched in a few days, and an immediate solution is needed.\nWhat should the solutions architect recommend?\n회사의 동적 웹 사이트는 미국의 온프레미스 서버를 사용하여 호스팅됩니다 . 이 회사는 유럽에서 제품\n을 출시하고 있으며 새로운 유럽 사용자를 위해 사이트 로딩 시간을 최적화하려고 합니다 . 사이트의 백\n엔드는 미국에 있어야 합니다 . 제품이 며칠 안에 출시되며 즉각적인 솔루션이 필요합니다 .\n솔루션 설계자는 무엇을 권장해야 합니까 ?\n​",
    "보기": {
      "A": "Launch an Amazon EC2 instance in us-east-1 and migrate the site to it.\n​us-east-1 에서 Amazon EC2 인스턴스를 시작하고 사이트를 마이그레이션합니다 .\n​",
      "B": "Move the website to Amazon S3. Use Cross-Region Replication between Regions.\n​웹사이트를 Amazon S3로 이동합니다 . 지역 간 교차 지역 복제를 사용합니다 .\n​",
      "C": "Use Amazon CloudFront with a custom origin pointing to the on-premises servers.\n온프레미스 서버를 가리키는 사용자 지정 오리진과 함께 Amazon CloudFront 를 사용합니다 .\n​\nAmazon CloudFront\n: Amazon CloudFront 는 정적 및 동적 콘텐츠의 원본 최종 버전을 보유하는 모든 오리진 서버에서 작동\n한다. \n사용자 지정 출처를 사용하기 위한 추가 비용은 없다.\n온프레미스 서버를 가리키는 사용자 지정 오리진과 함께 Amazon CloudFront 사용해야 한다.",
      "D": "Use an Amazon Route 53 geoproximity routing policy pointing to on-premises servers.\n​온프레미스 서버를 가리키는 Amazon Route 53 지리 근접 라우팅 정책을 사용합니다 .\n​"
    },
    "정답": null
  },
  {
    "문제번호": 84,
    "질문": "A company wants to reduce the cost of its existing three-tier web architecture. The web, application, \nand database servers are running on Amazon EC2 instances for the development, test, and \nproduction environments. The EC2 instances average 30% CPU utilization during peak hours and \n10% CPU utilization during non-peak hours.\nThe production EC2 instances run 24 hours a day. The development and test EC2 instances run for \nat least 8 hours each day. The company plans to implement automation to stop the development \nand test EC2 instances when they are not in use.\nWhich EC2 instance purchasing solution will meet the company's requirements MOST cost-effectively?\n회사는 기존 3계층 웹 아키텍처의 비용을 절감하려고 합니다 . 웹, 애플리케이션 및 데이터베이스 서버\n는 개발, 테스트 및 프로덕션 환경을 위한 Amazon EC2 인스턴스에서 실행됩니다 . EC2 인스턴스의 평\n균 CPU 사용률은 사용량이 많은 시간에는 30%이고 사용량이 많지 않은 시간에는 10%입니다 .\n프로덕션 EC2 인스턴스 는 하루 24시간 실행됩니다 . 개발 및 테스트 EC2 인스턴스는 매일 최소 8시간 \n동안 실행됩니다 . 회사는 개발을 중지하고 사용하지 않을 때 EC2 인스턴스를 테스트하는 자동화를 구\n현할 계획입니다 .\n어떤 EC2 인스턴스 구매 솔루션이 가장 비용 효율적으로 회사의 요구 사항을 충족합니까 ?\n​",
    "보기": {
      "A": "Use Spot Instances for the production EC2 instances. Use Reserved Instances for the development \nand test EC2 instances.\n​프로덕션 EC2 인스턴스에 스팟 인스턴스를 사용합니다 . EC2 인스턴스 개발 및 테스트에 예약 인스턴\n스를 사용합니다 .\n​",
      "B": "Use Reserved Instances for the production EC2 instances. Use On-Demand Instances for the \ndevelopment and test EC2 instances.\n​프로덕션 EC2 인스턴스에 예약 인스턴스를 사용합니다 . 개발 및 테스트 EC2 인스턴스에 온디맨드 인\n스턴스를 사용합니다 .\n→ 스팟 블록은 더 이상 사용할 수 없으며 Prod 시스템에서 24x7 스팟 인스턴스를 사용할 수 없으므로 \n정답이 될 수 없다.\n​\n스팟 블록(인스턴스 )\nDefined duration workloads : 1~6시간 동안의 스팟 블록(지정된 지속시간 ) 인스턴스 사용.",
      "C": "Use Spot blocks for the production EC2 instances. Use Reserved Instances for the development and \ntest EC2 instances.\n​프로덕션 EC2 인스턴스에 스팟 블록을 사용합니다 . EC2 인스턴스 개발 및 테스트에 예약 인스턴스를 \n사용합니다 .\n​",
      "D": "Use On-Demand Instances for the production EC2 instances. Use Spot blocks for the development \nand test EC2 instances.\n​프로덕션 EC2 인스턴스에 온디맨드 인스턴스를 사용합니다 . 개발 및 테스트 EC2 인스턴스에 스팟 블\n록을 사용합니다 .\n​"
    },
    "정답": null
  },
  {
    "문제번호": 85,
    "질문": "A company has a production web application in which users upload documents through a web \ninterface or a mobile app. According to a new regulatory requirement. new documents cannot be \nmodified or deleted after they are stored.\nWhat should a solutions architect do to meet this requirement?\n​회사에 사용자가 웹 인터페이스 또는 모바일 앱을 통해 문서를 업로드하는 프로덕션 웹 애플리케이션\n이 있습니다 . 새로운 규제 요구 사항에 따라. 새 문서는 저장 후에 수정하거나 삭제할 수 없습니다 .\n솔루션 설계자는 이 요구 사항을 충족하기 위해 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Store the uploaded documents in an Amazon S3 bucket with S3 Versioning and S3 Object Lock \nenabled.\n​업로드된 문서를 S3 버전 관리 및 S3 객체 잠금이 활성화된 Amazon S3 버킷에 저장합니다 .\n​\n→ S3 Object Lock (S3 객체 잠금)\n: S3 Object Lock을 사용하여 WORM(Write-Once-Read-Many) 모델을 사용하여 객체를 저장할 수 있다. \n객체 잠금을 사용하면 고정된 시간 동안 또는 무기한으로 객체가 삭제되거나 덮어쓰여지는 것을 방지\n할 수 있다. \nS3 객체 잠금을 사용하여 WORM 스토리지가 필요한 규정 요구 사항을 충족하거나 객체 변경 및 삭제\n에 대한 보호 계층을 \n추가할 수 있다. 버전 관리가 필요하며 객체 잠금이 활성화되면 자동으로 활성화된다 .\n​\n▶오답\nACL(액세스 제어 목록)\n: Amazon S3 ACL(액세스 제어 목록)로 버킷과 객체에 대한 액세스를 관리한다 . 각 버킷과 객체마다 \n하위 리소스로서 연결되어 있는 ACL이 있다. ACL은 액세스를 허용할 AWS 계정이나 그룹과 액세스 유\n형을 정의한다 . 리소스에 대한 요청을 수신하면 , Amazon S3는 해당 ACL을 확인해 요청자가 필요한 액\n세스 권한을 보유하고 있는지 판단한다 .\n​\n기본적으로 다른 AWS 계정이 S3 버킷에 객체를 업로드하면 해당 계정(객체 작성자 )이 객체를 소유하\n고 객체에 액세스할 수 있으며 ACL을 통해 다른 사용자에게 객체에 대한 액세스 권한을 부여할 수 있\n다. 객체 소유권을 사용하여 ACL이 사용 중지되고 버킷 소유자로서 버킷의 모든 객체를 자동으로 소유\n하도록 이 기본 동작을 변경할 수 있다. 결과적으로 데이터에 대한 액세스 제어는 IAM 정책, S3 버킷 \n정책, Virtual Private Cloud(VPC) 엔드포인트 정책 및 AWS Organizations 서비스 제어 정책(SCP) 과 같\n은 정책을 기반으로 한다.\n​\nAmazon S3의 최신 사용 사례 대부분은 더 이상 ACL을 사용할 필요가 없으며 , 각 객체에 대해 액세스\n를 개별적으로 제어해야 하는 비정상적인 상황을 제외하고는 ACL을 사용 중지하는 것이 좋다. 객체 소\n유권으로 ACL을 사용 중지하고 액세스 제어 정책을 사용할 수 있다. ACL을 사용 중지하면 다른 AWS \n계정이 업로드한 객체로 버킷을 쉽게 유지 관리할 수 있다.",
      "B": "Store the uploaded documents in an Amazon S3 bucket. Configure an S3 Lifecycle policy to archive \nthe documents periodically.\n​업로드된 문서를 Amazon S3 버킷에 저장합니다 . 문서를 주기적으로 보관하도록 S3 수명 주기 정책을 \n구성합니다 .\n​",
      "C": "Store the uploaded documents in an Amazon S3 bucket with S3 Versioning enabled. Configure an \nACL to restrict all access to read-only.\n​업로드된 문서를 S3 버전 관리가 활성화된 Amazon S3 버킷에 저장합니다 . 모든 액세스를 읽기 전용으\n로 제한하도록 ACL을 구성합니다 .\n​",
      "D": "Store the uploaded documents on an Amazon Elastic File System (Amazon EFS) volume. Access the \ndata by mounting the volume in read-only mode.\n업로드된 문서를 Amazon Elastic File System(Amazon EFS) 볼륨에 저장합니다 . 읽기 전용 모드에서 \n볼륨을 마운트하여 데이터에 액세스합니다 .\n​"
    },
    "정답": null
  },
  {
    "문제번호": 86,
    "질문": "​A company has several web servers that need to frequently access a common Amazon RDS MySQL \nMulti-AZ DB instance. The company wants a secure method for the web servers to connect to the \ndatabase while meeting a security requirement to rotate user credentials frequently.\nWhich solution meets these requirements?\n회사에는 공통 Amazon RDS MySQL 다중 AZ DB 인스턴스에 자주 액세스해야 하는 여러 웹 서버가 \n있습니다 . 회사는 사용자 자격 증명을 자주 교체해야 하는 보안 요구 사항을 충족하면서 웹 서버가 데\n이터베이스에 연결할 수 있는 안전한 방법을 원합니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n​",
    "보기": {
      "A": "Store the database user credentials in AWS Secrets Manager. Grant the necessary IAM permissions \nto allow the web servers to access AWS Secrets Manager.\n​AWS Secrets Manager 에 데이터베이스 사용자 자격 증명을 저장합니다 . 웹 서버가 AWS Secrets \nManager 에 액세스할 수 있도록 필요한 IAM 권한을 부여합니다 .\n​\n→ AWS Secrets Manager\n: Secrets Manager 를 사용하면 암호를 포함하여 코드의 하드코딩된 자격 증명을 Secrets Manager 에 \n대한 API 호출로 교체하여 프로그래밍 방식으로 암호를 검색할 수 있다. 이렇게 하면 암호가 더 이상 \n코드에 존재하지 않기 때문에 다른 사람이 코드를 검사하는 것에 의해 암호가 손상되지 않도록 하는 \n데 도움이 된다. 또한 지정된 일정에 따라 암호를 자동으로 교체하도록 Secrets Manager 를 구성할 수 \n있습니다 . 이를 통해 장기 비밀을 단기 비밀로 교체할 수 있으므로 손상 위험이 크게 줄어든다 . \n​\n보안 암호에 대한 액세스 제어\n: 특정 보안 암호에 대한 액세스 권한을 부여하거나 거부하려는 사용자 , 그룹 및 역할에 AWS Identity \nand Access Management(IAM) 권한 정책을 연결해 해당 보안 암호의 관리를 제한할 수 있다. 예를 들\n어, 보안 암호를 완전히 관리 및 구성하는 권한이 필요한 구성원이 있는 그룹에 정책 하나를 연결할 수 \n있다. 애플리케이션에서 사용하는 역할에 연결된 다른 정책은 애플리케이션이 실행되는 데 필요한 보안 \n암호에 대한 읽기 권한만 부여할 수 있다.\n​\n​\n▶오답\nAWS Systems Manager OpsCenter\n: AWS 고객이 여러 서비스에 걸친 문제, 이벤트 및 경보를 집계할 수 있게 해 준다. 이를 통해 고객은 \n단일 위치에서 문제를 검토, 조사 및 해결할 수 있으므로 서로 다른 여러 AWS 서비스로 이동할 필요\n가 감소된다 . 관리 콘솔에서 문제, 이벤트 및 경보 등의 정보는 OpsItem( 운영 항목)으로 표시되며 컨텍\n스트 정보, 과거 기록에 기반한 안내 및 빠른 해결 단계를 제공하며 , 이 기능은 단일 위치에서 주요 조\n사 데이터를 제공함으로써 , 해결에 소요되는 평균 시간을 단축하고 , 엔지니어의 생산성을 향상하도록 \n설계되었다 .​",
      "B": "Store the database user credentials in AWS Systems Manager OpsCenter. Grant the necessary IAM \npermissions to allow the web servers to access OpsCenter.\n​AWS Systems Manager OpsCenter 에 데이터베이스 사용자 자격 증명을 저장합니다 . 웹 서버가 \nOpsCenter 에 액세스할 수 있도록 필요한 IAM 권한을 부여합니다 .\n​",
      "C": "Store the database user credentials in a secure Amazon S3 bucket. Grant the necessary IAM \npermissions to allow the web servers to retrieve credentials and access the database.\n​안전한 Amazon S3 버킷에 데이터베이스 사용자 자격 증명을 저장합니다 . 웹 서버가 자격 증명을 검색\n하고 데이터베이스에 액세스할 수 있도록 필요한 IAM 권한을 부여합니다 .\n​",
      "D": "Store the database user credentials in files encrypted with AWS Key Management Service (AWS \nKMS) on the web server file system. The web server should be able to decrypt the files and access \nthe database.\n​웹 서버 파일 시스템의 AWS Key Management Service(AWS KMS)로 암호화된 파일에 데이터베이스 \n사용자 자격 증명을 저장합니다 . 웹 서버는 파일을 해독하고 데이터베이스에 액세스할 수 있어야 합니\n다.\n​"
    },
    "정답": null
  },
  {
    "문제번호": 87,
    "질문": "A company hosts an application on AWS Lambda functions that are invoked by an Amazon API \nGateway API. The Lambda functions save customer data to an Amazon Aurora MySQL database. \nWhenever the company upgrades the database, the Lambda functions fail to establish database \nconnections until the upgrade is complete. The result is that customer data is not recorded for some \nof the event.\nA solutions architect needs to design a solution that stores customer data that is created during \ndatabase upgrades.\nWhich solution will meet these requirements?\n​회사는 Amazon API Gateway API에 의해 호출되는 AWS Lambda 함수에서 애플리케이션을 호스팅합\n니다. Lambda 함수는 고객 데이터를 Amazon Aurora MySQL 데이터베이스에 저장합니다 . 회사에서 데\n이터베이스를 업그레이드할 때마다 Lambda 함수는 업그레이드가 완료될 때까지 데이터베이스 연결을 \n설정하지 못합니다 . 그 결과 일부 이벤트에 대한 고객 데이터가 기록되지 않습니다 .\n솔루션 설계자는 데이터베이스 업그레이드 중에 생성되는 고객 데이터를 저장하는 솔루션 을 설계해야 \n합니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n​",
    "보기": {
      "A": "Provision an Amazon RDS proxy to sit between the Lambda functions and the database. Configure \nthe Lambda functions to connect to the RDS proxy.\n​Lambda 함수와 데이터베이스 사이에 위치하도록 Amazon RDS 프록시를 프로비저닝합니다 . RDS 프록\n시에 연결하도록 Lambda 함수를 구성합니다 .\n→ RDS Proxy 는 AuroraDB 를 지원하지 않는다 .",
      "B": "Increase the run time of the Lambda functions to the maximum. Create a retry mechanism in the \ncode that stores the customer data in the database.\n​ Lambda 함수의 실행 시간을 최대로 늘립니다 . 데이터베이스에 고객 데이터를 저장하는 코드에서 재시\n도 메커니즘을 만듭니다 .\n​",
      "C": "Persist the customer data to Lambda local storage. Configure new Lambda functions to scan the \nlocal storage to save the customer data to the database.\n​고객 데이터를 Lambda 로컬 스토리지에 유지합니다 . 고객 데이터를 데이터베이스에 저장하기 위해 로\n컬 스토리지를 스캔하도록 새로운 Lambda 함수를 구성합니다 .\n​",
      "D": "Store the customer data in an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Create a \nnew Lambda function that polls the queue and stores the customer data in the database.\n​ Amazon Simple Queue Service(Amazon SQS) FIFO 대기열에 고객 데이터를 저장합니다 . 대기열을 \n폴링하고 고객 데이터를 데이터베이스에 저장하는 새 Lambda 함수를 생성합니다 .\n→ DB가 유지 관리되는 동안 SQS는 트래픽을 대기열에 유지합니다 .\n​\n▶오답"
    },
    "정답": null
  },
  {
    "문제번호": 88,
    "질문": "A survey company has gathered data for several years from areas in the United States. The \ncompany hosts the data in an Amazon S3 bucket that is 3 TB in size and growing. The company \nhas started to share the data with a European marketing firm that has S3 buckets. The company \nwants to ensure that its data transfer costs remain as low as possible.\nWhich solution will meet these requirements?\n설문 조사 회사는 미국 지역에서 수년 동안 데이터를 수집했습니다 . 이 회사는 크기가 3TB이고 계속 \n증가하는 Amazon S3 버킷에 데이터를 호스팅합니다 . 이 회사는 S3 버킷이 있는 유럽 마케팅 회사와 \n데이터를 공유하기 시작했습니다 . 회사는 데이터 전송 비용이 가능한 한 낮게 유지되기를 원합니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n​",
    "보기": {
      "A": "Configure the Requester Pays feature on the company's S3 bucket.\n회사의 S3 버킷에서 요청자 지불 기능을 구성합니다 .\n​",
      "B": "Configure S3 Cross-Region Replication from the company's S3 bucket to one of the marketing firm's \nS3 buckets.\n회사의 S3 버킷에서 마케팅 회사의 S3 버킷 중 하나로 S3 교차 리전 복제를 구성합니다 .",
      "C": "Configure cross-account access for the marketing firm so that the marketing firm has access to the \ncompany's S3 bucket.\n​마케팅 회사가 회사의 S3 버킷에 액세스할 수 있도록 마케팅 회사에 대한 교차 계정 액세스를 구성합\n니다.\n​",
      "D": "Configure the company's S3 bucket to use S3 Intelligent-Tiering. Sync the S3 bucket to one of the \nmarketing firm's S3 buckets.\nS3 Intelligent-Tiering 을 사용하도록 회사의 S3 버킷을 구성합니다 . S3 버킷을 마케팅 회사의 S3 버킷 \n중 하나와 동기화합니다 .\n​"
    },
    "정답": null
  },
  {
    "문제번호": 89,
    "질문": "A company uses Amazon S3 to store its confidential audit documents. The S3 bucket uses bucket \npolicies to restrict access to audit team IAM user credentials according to the principle of least \nprivilege. Company managers are worried about accidental deletion of documents in the S3 bucket \nand want a more secure solution.\nWhat should a solutions architect do to secure the audit documents?\n회사는 Amazon S3를 사용하여 기밀 감사 문서를 저장합니다 . S3 버킷은 버킷 정책을 사용하여 최소 \n권한 원칙에 따라 감사 팀 IAM 사용자 자격 증명에 대한 액세스를 제한합니다 . 회사 관리자는 S3 버킷\n에서 실수로 문서가 삭제되는 것을 걱정하고 더 안전한 솔루션 을 원합니다 .\n솔루션 설계자는 감사 문서를 보호하기 위해 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Enable the versioning and MFA Delete features on the S3 bucket.\n​S3 버킷에서 버전 관리 및 MFA 삭제 기능을 활성화합니다 .\n​\n실수로 인한 삭제를 방지하거나 , 완화하기 위한 기능\n•버전 관리를 활성화하여 객체의 이전 버전을 유지한다 .\n•객체의 교차 리전 복제를 활성화 한다.\n•객체 버전을 삭제할 때 Multi-Factor Authemtication(MFA) 을 요구하려면 MFA 삭제를 활성화 \n한다.",
      "B": "Enable multi-factor authentication (MFA) on the IAM user credentials for each audit team IAM user \naccount.\n​각 감사 팀 IAM 사용자 계정의 IAM 사용자 자격 증명에 대해 다단계 인증(MFA) 을 활성화합니다 .\n​",
      "C": "Add an S3 Lifecycle policy to the audit team's IAM user accounts to deny the s3:DeleteObject action \nduring audit dates.\n​감사 날짜 동안 s3:DeleteObject 작업을 거부하도록 감사 팀의 IAM 사용자 계정에 S3 수명 주기 정책\n을 추가합니다 .\n​",
      "D": "Use AWS Key Management Service (AWS KMS) to encrypt the S3 bucket and restrict audit team \nIAM user accounts from accessing the KMS key.\n​AWS Key Management Service(AWS KMS)를 사용하여 S3 버킷을 암호화하고 감사 팀 IAM 사용자 계\n정이 KMS 키에 액세스하지 못하도록 제한합니다 .\n​"
    },
    "정답": null
  },
  {
    "문제번호": 90,
    "질문": "A company is using a SQL database to store movie data that is publicly accessible. The database \nruns on an Amazon RDS Single-AZ DB instance. A script runs queries at random intervals each day \nto record the number of new movies that have been added to the database. The script must report \na final total during business hours.\nThe company's development team notices that the database performance is inadequate for \ndevelopment tasks when the script is running. A solutions architect must recommend a solution to \nresolve this issue.\nWhich solution will meet this requirement with the LEAST operational overhead?\n​회사에서 SQL 데이터베이스를 사용하여 공개적으로 액세스할 수 있는 영화 데이터를 저장하고 있습니\n다. 데이터베이스는 Amazon RDS 단일 AZ DB 인스턴스에서 실행됩니다 . 스크립트는 데이터베이스에 \n추가된 새로운 영화의 수를 기록하기 위해 매일 임의의 간격으로 쿼리를 실행합니다 .스크립트는 업무 \n시간 동안의 최종 합계를 보고해야 합니다 .\n회사의 개발 팀은 스크립트가 실행 중일 때 데이터베이스 성능이 개발 작업에 부적절하다는 것을 알아\n차렸습니다 . 솔루션 설계자는 이 문제를 해결하기 위한 솔루션을 권장해야 합니다 .\n최소한의 운영 오버헤드로 이 요구 사항을 충족하는 솔루션은 무엇입니까 ?\n​",
    "보기": {
      "A": "Modify the DB instance to be a Multi-AZ deployment.\n​DB 인스턴스를 다중 AZ 배포로 수정합니다 .\n​",
      "B": "Create a read replica of the database. Configure the script to query only the read replica.\n​데이터베이스의 읽기 전용 복제본을 생성합니다 . 읽기 전용 복제본만 쿼리하도록 스크립트를 구성합니\n다.\n→ 스크립트는 보고서를 생성하기 위해 데이터만 읽고 스크립트가 실행 중일 때만 속도 저하가 발생하\n므로 \n읽기 전용 복제본이 있으면 최소한의 오버헤드로 문제를 해결할 수 있다.",
      "C": "Instruct the development team to manually export the entries in the database at the end of each \nday.\n​개발 팀에게 매일 일과가 끝날 때 데이터베이스의 항목을 수동으로 내보내도록 지시합니다 .\n​",
      "D": "Use Amazon ElastiCache to cache the common queries that the script runs against the database.\n​Amazon ElastiCache 를 사용하여 스크립트가 데이터베이스에 대해 실행하는 일반적인 쿼리를 캐시합니\n다.\n​"
    },
    "정답": null
  },
  {
    "문제번호": 91,
    "질문": "A company has applications that run on Amazon EC2 instances in a VPC. One of the applications \nneeds to call the Amazon S3 API to store and read objects. According to the company's security \nregulations, no traffic from the applications is allowed to travel across the internet.\nWhich solution will meet these requirements?\n​회사에 VPC의 Amazon EC2 인스턴스 에서 실행되는 애플리케이션이 있습니다 . 애플리케이션 중 하나\n는Amazon S3 API를 호출하여 객체를 저장하고 읽어야 합니다 . 회사의 보안 규정에 따라 응용 프로그\n램의 트래픽은 인터넷을 통해 이동할 수 없습니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n​",
    "보기": {
      "C": "Create an S3 bucket in the same AWS Region as the EC2 instances.\n​EC2 인스턴스와 동일한 AWS 리전에 S3 버킷을 생성합니다 .\n​",
      "A": "Configure an S3 gateway endpoint.\n​ S3 게이트웨이 엔드포인트를 구성합니다 .\n​\n→ 게이트웨이 엔드포인트\n: 인터넷 게이트웨이나 VPC용 NAT 디바이스 없이 Amazon S3 및 DynamoDB 에 안정적인 연결을 제공\n한다.\n게이트웨이 엔드포인트 사용에 대한 추가 비용은 없다.",
      "B": "Create an S3 bucket in a private subnet.\n​프라이빗 서브넷에 S3 버킷을 생성합니다 .\n​",
      "D": "Configure a NAT gateway in the same subnet as the EC2 instances.\n​EC2 인스턴스와 동일한 서브넷에 NAT 게이트웨이를 구성합니다 .\n​"
    },
    "정답": "A"
  },
  {
    "문제번호": 92,
    "질문": "A company is storing sensitive user information in an Amazon S3 bucket. The company wants to \nprovide secure access to this bucket from the application tier running on Amazon EC2 instances \ninside a VPC.\nWhich combination of steps should a solutions architect take to accomplish this? (Choose two.)\n회사에서 Amazon S3 버킷에 민감한 사용자 정보를 저장하고 있습니다 . 회사는 VPC 내부의 Amazon \nEC2 인스턴스 에서 실행되는 애플리케이션 계층에서 이 버킷에 대한 보안 액세스를 제공하려고 합니다 .\n솔루션 설계자는 이를 달성하기 위해 어떤 단계 조합을 취해야 합니까 ? (2개를 선택하세요 .)\n​",
    "보기": {
      "C": "Create a bucket policy that limits access to only the application tier running in the VPC.\n​VPC에서 실행되는 애플리케이션 계층으로만 액세스를 제한하는 버킷 정책을 생성합니다 .\n​\n→ IAM등 자격 증명과 같은 인증을 사용하지 않고 S3 버킷에 비공개로 연결하도록 Amazon VPC를 구\n성하는 방법\n: Amazon VPC에서 버킷에 액세스 하는 경우 인증하지 않고 비공개로 S3버킷에 액세스 할 수 있다.\n하지만 사용된 VPC 엔드포인트가 Amazon S3를 가르키는지 확인해야 한다.\n​\n•Amazon S3에 대한 VPC 엔드포인트를 생성\n•VPC 엔드포인트에서 액세스를 허용하는 버킷 정책을 추가",
      "A": "Configure a VPC gateway endpoint for Amazon S3 within the VPC.\n​VPC 내에서 Amazon S3용 VPC 게이트웨이 엔드포인트를 구성합니다 .\n​",
      "B": "Create a bucket policy to make the objects in the S3 bucket public.\n​S3 버킷의 객체를 퍼블릭으로 만들기 위한 버킷 정책을 생성합니다 .\n​",
      "D": "Create an IAM user with an S3 access policy and copy the IAM credentials to the EC2 instance.\nS3 액세스 정책으로 IAM 사용자를 생성하고 IAM 자격 증명을 EC2 인스턴스에 복사합니다 .\n​\nE.\nCreate a NAT instance and have the EC2 instances use the NAT instance to access the S3 bucket.\nNAT 인스턴스를 생성하고 EC2 인스턴스가 NAT 인스턴스를 사용하여 S3 버킷에 액세스하도록 합니\n다.\n​"
    },
    "정답": "A"
  },
  {
    "문제번호": 93,
    "질문": "A company runs an on-premises application that is powered by a MySQL database. The company is \nmigrating the application to AWS to increase the application's elasticity and availability.\nThe current architecture shows heavy read activity on the database during times of normal operation. \nEvery 4 hours, the company's development team pulls a full export of the production database to \npopulate a database in the staging environment. During this period, users experience unacceptable \napplication latency. The development team is unable to use the staging environment until the \nprocedure completes.\nA solutions architect must recommend replacement architecture that alleviates the application latency \nissue. The replacement architecture also must give the development team the ability to continue \nusing the staging environment without delay.\nWhich solution meets these requirements?\n회사는 MySQL 데이터베이스 로 구동되는 온프레미스 애플리케이션을 실행합니다 . 이 회사는 애플리케\n이션의 탄력성과 가용성을 높이기 위해 애플리케이션을 AWS로 마이그레이션하고 있습니다 .\n현재 아키텍처는 정상 작동 시간 동안 데이터베이스에서 많은 읽기 활동을 보여줍니다 . 회사의 개발 팀\n은 4시간마다 프로덕션 데이터베이스의 전체 내보내기를 가져와 준비 환경의 데이터베이스를 채웁니다 . \n이 기간 동안 사용자는 허용할 수 없는 애플리케이션 대기 시간을 경험합니다 . 개발 팀은 절차가 완료\n될 때까지 스테이징 환경을 사용할 수 없습니다 .\n솔루션 설계자는 애플리케이션 지연 문제를 완화하는 대체 아키텍처를 권장해야 합니다 . 또한 대체 아\n키텍처는 개발 팀이 지연 없이 *스테이징 환경을 계속 사용할 수 있는 능력을 제공해야 합니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n* 스테이징 환경 : 실제 운영 환경과 거의 동일한 환경으로 만들어놓고 기능을 검증하는 환경.\n​",
    "보기": {
      "A": "Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production. Populate the staging \ndatabase by implementing a backup and restore process that uses the mysqldump utility.\n​프로덕션용 다중 AZ Aurora 복제본과 함께 Amazon Aurora MySQL 을 사용합니다 . mysqldump 유틸리\n티를 사용하는 백업 및 복원 프로세스를 구현하여 스테이징 데이터베이스를 채웁니다 .\n​",
      "B": "Use Amazon Aurora MySQL with Multi-AZ Aurora Replicas for production. Use database cloning to \ncreate the staging database on-demand.\n프로덕션용 다중 AZ Aurora 복제본과 함께 Amazon Aurora MySQL 을 사용합니다 . 데이터베이스 복제\n를 사용하여 요청 시 스테이징 데이터베이스를 생성합니다 .\n​\nAurora DB 클러스터에 대한 볼륨 복제\n: Aurora 복제를 사용하면 동일한 Aurora 클러스터 볼륨을 사용하고 원본과 동일한 데이터를 갖는 새 \n클러스터를 생성할 수 있다. \n이 프로세스는 빠르고 비용 효율적으로 진행되도록 설계되었다 . 연결된 데이터 볼륨이 있는 새 클러스\n터를 복제본 이라고 한다. \n복제본 생성은 스냅샷 복원과 같은 다른 기술을 사용하여 데이터를 물리적으로 복사하는 것보다 빠르\n고 공간 효율적이다 .\n​\nAurora 복제 작업은 데이터 손상 위험 없이 프로덕션 데이터를 사용하여 테스트 환경을 신속하게 설정\n하는 데 특히 유용하다 . \n다음과 같은 여러 유형의 애플리케이션에 복제본을 사용할 수 있다.\n​\n•잠재적 변경 사항(예: 스키마 변경 및 파라미터 그룹 변경)을 실험하여 모든 영향을 평가한다 .\n•데이터 내보내기 또는 복제본에서 분석 쿼리 실행과 같은 워크로드 집약적인 작업을 수행하는 \n경우\n•개발, 테스트 또는 기타 용도로 프로덕션 DB 클러스터의 복사본을 생성한다 .",
      "C": "Use Amazon RDS for MySQL with a Multi-AZ deployment and read replicas for production. Use the \nstandby instance for the staging database.\n​다중 AZ 배포 및 프로덕션용 읽기 전용 복제본과 함께 MySQL 용 Amazon RDS를 사용합니다 . 스테이\n징 데이터베이스에 대해 대기 인스턴스를 사용합니다 .\n​",
      "D": "Use Amazon RDS for MySQL with a Multi-AZ deployment and read replicas for production. Populate \nthe staging database by implementing a backup and restore process that uses the mysqldump utility.\n다중 AZ 배포 및 프로덕션용 읽기 전용 복제본과 함께 MySQL 용 Amazon RDS를 사용합니다 . \nmysqldump 유틸리티를 사용하는 백업 및 복원 프로세스를 구현하여 스테이징 데이터베이스를 채웁니\n다.\n​"
    },
    "정답": "B"
  },
  {
    "문제번호": 94,
    "질문": "A company is designing an application where users upload small files into Amazon S3. After a user \nuploads a file, the file requires one-time simple processing to transform the data and save the data \nin JSON format for later analysis.\nEach file must be processed as quickly as possible after it is uploaded. Demand will vary. On some \ndays, users will upload a high number of files. On other days, users will upload a few files or no \nfiles.\nWhich solution meets these requirements with the LEAST operational overhead?\n한 회사에서 사용자가 Amazon S3에 작은 파일을 업로드하는 애플리케이션 을 설계하고 있습니다 . 사용\n자가 파일을 업로드한 후 데이터를 변환하고 나중에 분석할 수 있도록 데이터를 JSON 형식으로 저장\n하려면 파일에 일회성 단순 처리가 필요합니다 .\n각 파일은 업로드 후 최대한 빨리 처리해야 합니다 . 수요는 다양할 것입니다 . 어떤 날에는 사용자가 많\n은 수의 파일을 업로드합니다 . 다른 날에는 사용자가 몇 개의 파일을 업로드하거나 파일을 업로드하지 \n않습니다 .\n최소한의 운영 오버헤드 로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까 ?\n​",
    "보기": {
      "A": "Configure Amazon EMR to read text files from Amazon S3. Run processing scripts to transform the \ndata. Store the resulting JSON file in an Amazon Aurora DB cluster.\n​Amazon S3에서 텍스트 파일을 읽도록 Amazon EMR을 구성합니다 . 처리 스크립트를 실행하여 데이터\n를 변환합니다 . 결과 JSON 파일을 Amazon Aurora DB 클러스터에 저장합니다 .\n​",
      "B": "Configure Amazon S3 to send an event notification to an Amazon Simple Queue Service (Amazon \nSQS) queue. Use Amazon EC2 instances to read from the queue and process the data. Store the \nresulting JSON file in Amazon DynamoDB.\n​ Amazon SQS(Amazon Simple Queue Service) 대기열에 이벤트 알림을 보내도록 Amazon S3를 구성\n합니다 . Amazon EC2 인스턴스를 사용하여 대기열에서 읽고 데이터를 처리합니다 . 결과 JSON 파일을 \nAmazon DynamoDB 에 저장합니다 .\n​",
      "C": "Configure Amazon S3 to send an event notification to an Amazon Simple Queue Service (Amazon \nSQS) queue. Use an AWS Lambda function to read from the queue and process the data. Store the \nresulting JSON file in Amazon DynamoDB.\n​이벤트 알림을 Amazon Simple Queue Service(Amazon SQS) 대기열로 보내도록 Amazon S3를 구성합\n니다. AWS Lambda 함수를 사용하여 대기열에서 읽고 데이터를 처리합니다 . 결과 JSON 파일을 \nAmazon DynamoDB 에 저장합니다 .\n​\n→ Amazon S3는 S3 버킷(예: 객체 생성, 객체 제거 또는 객체 복원)에 대한 이벤트 알림을 동일한 리\n전의 SNS 주제로 보낸다 . \nSNS 주제는 이벤트를 중앙 리전의 SQS 대기열에 게시한다 . SQS 대기열은 Lambda 함수에 대한 이벤\n트 소스로 구성되고 Lambda \n함수에 대한 이벤트 메시지를 버퍼링한다 . Lambda 함수는 SQS 대기열에서 메시지를 폴링하고 애플리\n케이션 요구 사항에 따라 Amazon S3 이벤트 알림을 처리한다 . \n​\n수요변동 , 서버리스 (운영 오버헤드 감소) → Lambda / JSON 형식의 데이터 → DynamoDB",
      "D": "Configure Amazon EventBridge (Amazon CloudWatch Events) to send an event to Amazon Kinesis \nData Streams when a new file is uploaded. Use an AWS Lambda function to consume the event \nfrom the stream and process the data. Store the resulting JSON file in an Amazon Aurora DB \ncluster.\n새 파일이 업로드될 때 Amazon Kinesis Data Streams 에 이벤트를 보내도록 Amazon \nEventBridge(Amazon CloudWatch Events) 를 구성합니다 . AWS Lambda 함수를 사용하여 스트림에서 \n이벤트를 소비하고 데이터를 처리합니다 . 결과 JSON 파일을 Amazon Aurora DB 클러스터에 저장합니\n다.\n​"
    },
    "정답": "C"
  },
  {
    "문제번호": 95,
    "질문": "An application allows users at a company's headquarters to access product data. The product data \nis stored in an Amazon RDS MySQL DB instance. The operations team has isolated an application \nperformance slowdown and wants to separate read traffic from write traffic. A solutions architect \nneeds to optimize the application's performance quickly.\nWhat should the solutions architect recommend?\n응용 프로그램을 사용하면 회사 본사의 사용자가 제품 데이터에 액세스할 수 있습니다 . 제품 데이터는 \nAmazon RDS MySQL DB 인스턴스 에 저장됩니다 . 운영 팀은 애플리케이션 성능 저하를 격리하고 쓰기 \n트래픽에서 읽기 트래픽을 분리하려고 합니다 . 솔루션 설계자는 애플리케이션의 성능을 신속하게 최적\n화해야 합니다 .\n솔루션 설계자는 무엇을 권장해야 합니까 ?\n​",
    "보기": {
      "A": "Change the existing database to a Multi-AZ deployment. Serve the read requests from the primary \nAvailability Zone.\n​기존 데이터베이스를 다중 AZ 배포로 변경합니다 . 기본 가용 영역에서 읽기 요청을 제공합니다 .\n​",
      "B": "Change the existing database to a Multi-AZ deployment. Serve the read requests from the secondary \nAvailability Zone.\n기존 데이터베이스를 다중 AZ 배포로 변경합니다 . 보조 가용 영역에서 읽기 요청을 제공합니다 .​\n​\n→ Amazon Aurora 의 고가용성\n: Aurora DB 클러스터는 내결함성을 고려하여 설계되었다 . 클러스터 볼륨은 단일 AWS 리전에 속하는 \n다중 가용 영역(AZ)을 모두 아우르며 , 각 가용 영역에는 클러스터 볼륨 데이터의 사본이 복사된다 . 이 \n기능은 가용 영역 한 곳에서 결함이 발생하더라도 DB 클러스터가 잠시 서비스가 중단될 뿐 전혀 데이\n터 손실 없이 결함을 견딜 수 있음을 의미한다 .",
      "C": "Create read replicas for the database. Configure the read replicas with half of the compute and \nstorage resources as the source database.\n데이터베이스에 대한 읽기 전용 복제본을 생성합니다 . 컴퓨팅 및 스토리지 리소스의 절반을 원본 데이\n터베이스로 사용하여 읽기 전용 복제본을 구성합니다 .\n​",
      "D": "Create read replicas for the database. Configure the read replicas with the same compute and \nstorage resources as the source database.\n데이터베이스에 대한 읽기 전용 복제본을 생성합니다 . 원본 데이터베이스와 동일한 컴퓨팅 및 스토리지 \n리소스로 읽기 전용 복제본을 구성합니다 .\n→ MySQL 읽기 전용 복제본 작업\nDB 인스턴스 하나에서 최대 5개까지 읽기 전용 복제본을 생성할 수 있다. \n효과적인 복제를 위해서는 읽기 전용 복제본도 각각 원본 DB 인스턴스와 동일한 양의 컴퓨팅 및 스토\n리지 리소스를 가져야 한다. \n원본 DB 인스턴스를 확장하는 경우 읽기 전용 복제본도 확장한다 .\n​\n​\n▶오답"
    },
    "정답": "D"
  },
  {
    "문제번호": 96,
    "질문": "An Amazon EC2 administrator created the following policy associated with an IAM group containing \nseveral users:\nAmazon EC2 관리자가 여러 사용자를 포함하는 IAM 그룹과 연결된 다음 정책을 생성했습니다 .\nWhat is the effect of this policy?\n이 정책의 효과는 무엇입니까 ?\n​",
    "보기": {
      "A": "Users can terminate an EC2 instance in any AWS Region except us-east-1.\n​사용자는 us-east-1 을 제외한 모든 AWS 리전에서 EC2 인스턴스를 종료할 수 있습니다 .\n​",
      "B": "Users can terminate an EC2 instance with the IP address 10.100.100.1 in the us-east-1 Region.\n​사용자는 us-east-1 리전에서 IP 주소가 10.100.100.1 인 EC2 인스턴스를 종료할 수 있습니다 .\n​",
      "C": "Users can terminate an EC2 instance in the us-east-1 Region when the user's source IP is \n10.100.100.254.\n​사용자는 사용자의 소스 IP가 10.100.100.254 일 때 us-east-1 리전에서 EC2 인스턴스를 종료할 수 있\n습니다 .\n​\n→ 1. 거부 정책은 AWS에 따라 항상 허용 정책보다 우선합니다 . \n2. 첫 번째 명령문은 IP 주소 범위에서 오는 ec2 종료 작업을 허용합니다 (조건은 IP 범위). \n3. 두 번째 명령문은 ec2에 대한 모든 작업을 거부합니다 (이제 조건은 StringNotEquals, 이는 AWS에 \n따른 부정 일치를 의미합니다 . \n이는 요청이 us-east-1 에서 오지 않는 한 ec2에 대한 모든 작업을 거부합니다 . 즉, us-east-1 에서만 작\n업을 허용합니다 (일치 부정). \n3. 이렇게 하면 옵션 A와 D를 제거하는 데 도움이 됩니다 . 이제 B와 C만 남습니다 . \n4. 이제 거부 규칙이 허용 규칙보다 먼저 적용됩니다 . \n5. 이제 사용자는 IP 범위 10....범위가 무엇이든 상관없이 us-east-1 의 인스턴스를 종료할 수 있습니다 .",
      "D": "Users cannot terminate an EC2 instance in the us-east-1 Region when the user's source IP is \n10.100.100.254.\n​사용자의 소스 IP가 10.100.100.254 인 경우 사용자는 us-east-1 리전에서 EC2 인스턴스를 종료할 수 \n없습니다 .\n​"
    },
    "정답": "C"
  },
  {
    "문제번호": 97,
    "질문": "A company has a large Microsoft SharePoint deployment running on-premises that requires Microsoft \nWindows shared file storage. The company wants to migrate this workload to the AWS Cloud and is \nconsidering various storage options. The storage solution must be highly available and integrated with \nActive Directory for access control.\nWhich solution will satisfy these requirements?\n​회사에는 Microsoft Windows 공유 파일 저장소 가 필요한 온프레미스에서 실행되는 대규모 Microsoft \nSharePoint 배포가 있습니다 . 회사는 이 워크로드를 AWS 클라우드로 마이그레이션하기를 원하며 다양\n한 스토리지 옵션을 고려하고 있습니다 . 저장소 솔루션은 액세스 제어를 위해 고가용성 및 Active \nDirectory 와 통합되어야 합니다 .\n이러한 요구 사항을 충족하는 솔루션은 무엇입니까 ?\n​",
    "보기": {
      "A": "Configure Amazon EFS storage and set the Active Directory domain for authentication.\n​Amazon EFS 스토리지를 구성하고 인증을 위해 Active Directory 도메인을 설정합니다 .\n​",
      "B": "Create an SMB file share on an AWS Storage Gateway file gateway in two Availability Zones.\n​두 가용 영역의 AWS Storage Gateway 파일 게이트웨이에 SMB 파일 공유를 생성합니다 .\n​",
      "C": "Create an Amazon S3 bucket and configure Microsoft Windows Server to mount it as a volume.\nAmazon S3 버킷을 생성하고 볼륨으로 탑재하도록 Microsoft Windows Server 를 구성합니다 .\n​",
      "D": "Create an Amazon FSx for Windows File Server file system on AWS and set the Active Directory \ndomain for authentication.\nAWS에서 Windows 파일 서버용 Amazon FSx 파일 시스템을 생성하고 인증을 위해 Active Directory \n도메인을 설정합니다 .\n​\nWindows 키워드 → Amazon FSx \n​\nWindows 파일 서버용 FSx에서 Microsoft Active Directory 작업\n: Amazon FSx는 Microsoft Active Directory(AD) 와 함께 작동하여 기존 Microsoft Windows 환경과 통합 \n된다.\nAmazon FSx로 파일 시스템을 생성할 때 Active Directory 도메인에 가입하여 사용자 인증과 파일 및 \n폴더 수준 액세스 제어를 제공한다 . \n그러면 사용자는 Active Directory 의 기존 사용자 자격 증명을 사용하여 자신을 인증하고 Amazon FSx \n파일 시스템에 액세스할 수 있다. \n사용자는 기존 ID를 사용하여 개별 파일 및 폴더에 대한 액세스를 제어할 수도 있다. 또한 기존 파일 \n및 폴더와 이러한 항목의 보안 ACL(액세스 제어 목록) 구성을 수정 없이 Amazon FSx로 마이그레이션\n할 수 있다.\n​\nActive Directory\n: 네트워크의 개체에 대한 정보를 저장하고 관리자와 사용자가 이 정보를 쉽게 찾고 사용할 수 있도록 \n하는 데 사용되는 \nMicrosoft 디렉터리 서비스이다 . \n이러한 개체에는 일반적으로 파일 서버, 네트워크 사용자 및 컴퓨터 계정과 같은 공유 리소스가 포함된\n다."
    },
    "정답": "D"
  },
  {
    "문제번호": 98,
    "질문": "An image-processing company has a web application that users use to upload images. The \napplication uploads the images into an Amazon S3 bucket. The company has set up S3 event \nnotifications to publish the object creation events to an Amazon Simple Queue Service (Amazon \nSQS) standard queue. The SQS queue serves as the event source for an AWS Lambda function \nthat processes the images and sends the results to users through email.\nUsers report that they are receiving multiple email messages for every uploaded image. A solutions \narchitect determines that SQS messages are invoking the Lambda function more than once, resulting \nin multiple email messages.\nWhat should the solutions architect do to resolve this issue with the LEAST operational overhead?\n​이미지 처리 회사에는 사용자가 이미지를 업로드하는 데 사용하는 웹 응용 프로그램이 있습니다 . 애플\n리케이션은 이미지를 Amazon S3 버킷에 업로드합니다 . 회사는 객체 생성 이벤트를 Amazon Simple \nQueue Service(Amazon SQS) 표준 대기열에 게시하도록 S3 이벤트 알림을 설정했습니다 . SQS 대기열\n은 이미지를 처리하고 결과를 이메일을 통해 사용자에게 보내는 AWS Lambda 함수의 이벤트 소스 역\n할을 합니다 .\n사용자는 업로드된 모든 이미지에 대해 여러 이메일 메시지를 수신하고 있다고 보고합니다 . 솔루션 설\n계자는 SQS 메시지가 Lambda 함수를 두 번 이상 호출하여 여러 이메일 메시지를 생성한다고 판단합\n니다.\n솔루션 설계자는 이 문제를 최소한의 운영 오버헤드로 해결하기 위해 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Set up long polling in the SQS queue by increasing the ReceiveMessage wait time to 30 seconds.\n​ReceiveMessage 대기 시간을 30초로 늘려 SQS 대기열에서 긴 폴링을 설정합니다 .\n→ 최대 긴 폴링 대기 시간은 20초이다 .",
      "B": "Change the SQS standard queue to an SQS FIFO queue. Use the message deduplication ID to \ndiscard duplicate messages.\n​SQS 표준 대기열을 SQS FIFO 대기열로 변경합니다 . 메시지 중복 제거 ID를 사용하여 중복 메시지를 \n버리십시오 .\n​",
      "C": "Increase the visibility timeout in the SQS queue to a value that is greater than the total of the \nfunction timeout and the batch window timeout.\n​SQS 대기열의 가시성 제한 시간을 함수 제한 시간과 일괄 처리 창 제한 시간의 합계보다 큰 값으로 \n늘립니다 .\n→람다는 메시지를 폴링하고 메시지 처리를 시작합니다 .\n그러나 첫 번째 람다가 메시지 처리를 완료하기 전에 SQS에서 가시성 시간 초과가 만료되고 SQS는 \n메시지를 폴링으로 반환하여\n다른 람다 노드가 동일한 메시지를 처리하도록 합니다 .\n가시성 시간 초과를 늘리면 람다가 메시지 처리를 완료하기 전에 SQS가 폴링에 메시지를 다시 반환하\n는 것을 방지할 수 있다.\n​\n▶오답",
      "D": "Modify the Lambda function to delete each message from the SQS queue immediately after the \nmessage is read before processing.\n처리 전에 메시지를 읽은 직후 SQS 대기열에서 각 메시지를 삭제하도록 Lambda 함수를 수정합니다 .\n​"
    },
    "정답": "C"
  },
  {
    "문제번호": 99,
    "질문": "A company is implementing a shared storage solution for a gaming application that is hosted in an \non-premises data center. The company needs the ability to use Lustre clients to access data. The \nsolution must be fully managed.\nWhich solution meets these requirements?\n회사는 온프레미스 데이터 센터에서 호스팅되는 게임 애플리케이션을 위한 공유 스토리지 솔루션 을 구\n현하고 있습니다 . 회사는 Lustre 클라이언트 를 사용하여 데이터에 액세스할 수 있는 기능이 필요합니다 . \n솔루션은 완전히 관리되어야 합니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n​\n​",
    "보기": {
      "A": "Create an AWS Storage Gateway file gateway. Create a file share that uses the required client \nprotocol. Connect the application server to the file share.\n​AWS Storage Gateway 파일 게이트웨이를 생성합니다 . 필요한 클라이언트 프로토콜을 사용하는 파일 \n공유를 만듭니다 . 응용 프로그램 서버를 파일 공유에 연결합니다 .\n​",
      "B": "Create an Amazon EC2 Windows instance. Install and configure a Windows file share role on the \ninstance. Connect the application server to the file share.\n​Amazon EC2 Windows 인스턴스를 생성합니다 . 인스턴스에 Windows 파일 공유 역할을 설치하고 구성\n합니다 . 응용 프로그램 서버를 파일 공유에 연결합니다 .\n​",
      "C": "Create an Amazon Elastic File System (Amazon EFS) file system, and configure it to support Lustre. \nAttach the file system to the origin server. Connect the application server to the file system.\n​Amazon Elastic File System(Amazon EFS) 파일 시스템을 생성하고 Lustre 를 지원하도록 구성합니다 . \n파일 시스템을 원본 서버에 연결합니다 . 응용 프로그램 서버를 파일 시스템에 연결합니다 .\n​",
      "D": "Create an Amazon FSx for Lustre file system. Attach the file system to the origin server. Connect \nthe application server to the file system.\nLustre 파일 시스템용 Amazon FSx를 생성합니다 . 파일 시스템을 원본 서버에 연결합니다 . 응용 프로그\n램 서버를 파일 시스템에 연결합니다 .\n→ Lustre 는 FSx로만 사용할 수 있다.\n​\nAmazon FSx for Lustre\n: Amazon FSx for Lustre 는 널리 사용되는 Lustre 파일 시스템의 확장성과 성능을 가진 완전관리형 공\n유 스토리지를 제공한다 .\n완전 관리형 서비스인 Amazon FSx는 Lustre 를 대중에게 제공하므로 스토리지 속도가 중요한 모든 워\n크로드에 사용할 수 있다. Amazon FSx는 고성능 Lustre 파일 시스템을 설정하고 관리하는 기존의 복\n잡성을 제거하여 전투 테스트를 거친 고성능 파일 시스템을 몇 분 안에 스핀업 , 실행 및 확장할 수 있\n다. 또한 여러 배포 옵션을 제공하므로 필요에 따라 비용을 최적화할 수 있다."
    },
    "정답": "D"
  },
  {
    "문제번호": 100,
    "질문": "A company's containerized application runs on an Amazon EC2 instance. The application needs to \ndownload security certificates before it can communicate with other business applications. The \ncompany wants a highly secure solution to encrypt and decrypt the certificates in near real time. The \nsolution also needs to store data in highly available storage after the data is encrypted.\nWhich solution will meet these requirements with the LEAST operational overhead?\n회사의 컨테이너화된 애플리케이션은 Amazon EC2 인스턴스에서 실행됩니다 . 애플리케이션은 다른 비\n즈니스 애플리케이션과 통신하기 전에 보안 인증서를 다운로드해야 합니다 . 회사는 거의 실시간으로 인\n증서를 암호화하고 해독할 수 있는 매우 안전한 솔루션 을 원합니다 . 또한 솔루션은 데이터가 암호화된 \n후 고가용성 스토리지에 데이터를 저장해야 합니다 .\n최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까 ?\n​",
    "보기": {
      "A": "Create AWS Secrets Manager secrets for encrypted certificates. Manually update the certificates as \nneeded. Control access to the data by using fine-grained IAM access.\n​암호화된 인증서에 대한 AWS Secrets Manager 암호를 생성합니다 . 필요에 따라 인증서를 수동으로 업\n데이트합니다 . 세분화된 IAM 액세스를 사용하여 데이터에 대한 액세스를 제어합니다 .\n​",
      "B": "Create an AWS Lambda function that uses the Python cryptography library to receive and perform \nencryption operations. Store the function in an Amazon S3 bucket.\nPython 암호화 라이브러리를 사용하여 암호화 작업을 수신하고 수행하는 AWS Lambda 함수를 생성합\n니다. 함수를 Amazon S3 버킷에 저장합니다 .\n​",
      "C": "Create an AWS Key Management Service (AWS KMS) customer managed key. Allow the EC2 role \nto use the KMS key for encryption operations. Store the encrypted data on Amazon S3.\nAWS Key Management Service(AWS KMS) 고객 관리형 키를 생성합니다 . EC2 역할이 암호화 작업에 \nKMS 키를 사용하도록 허용합니다 . 암호화된 데이터를 Amazon S3에 저장합니다 .\n→ 고가용성 스토리지 = Amazon S3\n​\nAWS Key Management Service (AWS KMS)\n: AWS KMS는 암호화 작업에 사용되는 키를 쉽게 생성하고 제어할 수 있도록 지원하는 관리형 서비\n스. \n이 서비스는 고가용성 키 생성, 스토리지 , 관리 및 감사 솔루션을 제공하므로 , 이를 통해 자체 애플리케\n이션 내 데이터를 암호화 또는 디지털 방식으로 서명하거나 , AWS 서비스 전체에서 데이터의 암호화를 \n제어할 수 있다.\n​\n사용자가 생성하는 KMS 키는 고객 관리형 키이다 . KMS 키를 사용하여 서비스 리소스를 암호화하는 \nAWS 서비스는 종종 사용자를 위해 키를 만든다 . AWS 서비스가 AWS 계정에서 생성하는 KMS 키는 \nAWS 관리형 키이다 .",
      "D": "Create an AWS Key Management Service (AWS KMS) customer managed key. Allow the EC2 role \nto use the KMS key for encryption operations. Store the encrypted data on Amazon Elastic Block \nStore (Amazon EBS) volumes.\nAWS Key Management Service(AWS KMS) 고객 관리형 키를 생성합니다 . EC2 역할이 암호화 작업에 \nKMS 키를 사용하도록 허용합니다 . 암호화된 데이터를 Amazon Elastic Block Store(Amazon EBS) 볼륨\n에 저장합니다 .\n​"
    },
    "정답": "C"
  },
  {
    "문제번호": 101,
    "질문": "A solutions architect is designing a VPC with public and private subnets. The VPC and subnets use \nIPv4 CIDR blocks. There is one public subnet and one private subnet in each of three Availability \nZones (AZs) for high availability. An internet gateway is used to provide internet access for the \npublic subnets. The private subnets require access to the internet to allow Amazon EC2 instances to \ndownload software updates.\nWhat should the solutions architect do to enable Internet access for the private subnets?\n솔루션 설계자는 퍼블릭 및 프라이빗 서브넷이 있는 VPC를 설계하고 있습니다 . VPC와 서브넷은 IPv4 \nCIDR 블록을 사용합니다 . 고가용성을 위해 세 개의 가용 영역(AZ) 각각에 하나의 퍼블릭 서브넷과 하\n나의 프라이빗 서브넷이 있습니다 . 인터넷 게이트웨이는 퍼블릭 서브넷에 대한 인터넷 액세스를 제공하\n는 데 사용됩니다 . 프라이빗 서브넷은 Amazon EC2 인스턴스가 소프트웨어 업데이트를 다운로드할 수 \n있도록 인터넷에 액세스할 수 있어야 합니다 .\n솔루션 설계자는 프라이빗 서브넷에 대한 인터넷 액세스를 활성화하기 위해 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Create three NAT gateways, one for each public subnet in each AZ. Create a private route table for \neach AZ that forwards non-VPC traffic to the NAT gateway in its AZ.\n각 AZ의 각 퍼블릭 서브넷에 대해 하나씩 3개의 NAT 게이트웨이를 생성합니다 . 비 VPC 트래픽을 해\n당 AZ의 NAT 게이트웨이로 전달하는 각 AZ에 대한 프라이빗 라우팅 테이블을 생성합니다 .\n→ Private Subnet 에 있는 인스턴스의 인터넷 트래픽은 NAT 인스턴스로 라우팅되어 인터넷과 통신한\n다. \n따라서 NAT 인스턴스는 인터넷에 액세스할 수 있어야 한다. 퍼블릭 서브넷 (인터넷 게이트웨이에 대한 \n경로가 있는 \n라우팅 테이블이 있는 서브넷 )에 있어야 하며 퍼블릭 IP 주소 또는 탄력적 IP 주소가 있어야 한다. \nNAT 게이트웨이\nNAT 게이트웨이는 NAT(네트워크 주소 변환) 서비스 . \n프라이빗 서브넷의 인스턴스가 VPC 외부의 서비스에 연결할 수 있지만 \n외부 서비스에서 이러한 인스턴스와의 연결을 시작할 수 없도록 NAT 게이트웨이를 사용할 수 있다.\n퍼블릭 프라이빗\n•(기본값 ) 프라이빗 서브넷의 인스턴스는 \n퍼블릭 NAT 게이트웨이를 통해 인터넷\n에 연결할 수 있지만 인터넷에서 원치 \n않는 인바운드 연결을 수신할 수 없습\n니다. •프라이빗 서브넷의 인스턴스는 프라이\n빗 NAT 게이트웨이를 통해 다른 VPC \n또는 온프레미스 네트워크에 연결할 수 \n있습니다 . \n•트래픽을 NAT 게이트웨이에서 Transit \n​\n▶오답",
      "B": "Create three NAT instances, one for each private subnet in each AZ. Create a private route table for \neach AZ that forwards non-VPC traffic to the NAT instance in its AZ.\n각 AZ의 프라이빗 서브넷마다 하나씩 3개의 NAT 인스턴스를 생성합니다 . 비 VPC 트래픽을 해당 AZ\n의 NAT 인스턴스로 전달하는 각 AZ에 대한 프라이빗 라우팅 테이블을 생성합니다 .\n​\n→ 모든 NAT(게이트웨이 /인스턴스 )는 인터넷에 연결하기 위해 퍼블릭으로 설정해야 하므로 프라이빗 \n서브넷에서 NAT 인스턴스를 \n생성하는 것은 옳지 않다.",
      "C": "Create a second internet gateway on one of the private subnets. Update the route table for the \nprivate subnets that forward non-VPC traffic to the private internet gateway.\n프라이빗 서브넷 중 하나에 두 번째 인터넷 게이트웨이를 생성합니다 . VPC가 아닌 트래픽을 프라이빗 \n인터넷 게이트웨이로 전달하는 프라이빗 서브넷의 라우팅 테이블을 업데이트합니다 .​\n​",
      "D": "Create an egress-only internet gateway on one of the public subnets. Update the route table for the \nprivate subnets that forward non-VPC traffic to the egress-only Internet gateway.\n​퍼블릭 서브넷 중 하나에 송신 전용 인터넷 게이트웨이를 생성합니다 . VPC가 아닌 트래픽을 외부 전용 \n인터넷 게이트웨이로 전달하는 프라이빗 서브넷에 대한 라우팅 테이블을 업데이트합니다 .\n​"
    },
    "정답": "A"
  },
  {
    "문제번호": 102,
    "질문": "A company wants to migrate an on-premises data center to AWS. The data center hosts an SFTP \nserver that stores its data on an NFS-based file system. The server holds 200 GB of data that \nneeds to be transferred. The server must be hosted on an Amazon EC2 instance that uses an \nAmazon Elastic File System (Amazon EFS) file system.\nWhich combination of steps should a solutions architect take to automate this task? (Choose two.)\n회사에서 온프레미스 데이터 센터를 AWS로 마이그레이션하려고 합니다 . 데이터 센터는 NFS 기반 파\n일 시스템에 데이터를 저장하는 SFTP 서버를 호스팅합니다 . 서버에는 전송해야 하는 200GB 의 데이터\n가 있습니다 . 서버는 Amazon Elastic File System(Amazon EFS) 파일 시스템을 사용하는 Amazon EC2 \n인스턴스 에서 호스팅되어야 합니다 .\n솔루션 설계자는 이 작업을 자동화하기 위해 어떤 단계 조합을 취해야 합니까 ? (2개를 선택하세요 .)\n​",
    "보기": {
      "A": "Launch the EC2 instance into the same Availability Zone as the EFS file system.\nEFS 파일 시스템과 동일한 가용 영역에서 EC2 인스턴스를 시작합니다 .\n​→ EFS 저장소가 있는 것과 동일한 AZ에 인스턴스를 두는 것이 타당하다 .\n​",
      "B": "Install an AWS DataSync agent in the on-premises data center.\n​온프레미스 데이터 센터에 AWS DataSync 에이전트를 설치합니다 .\n→ DataSync 는 이미 EC2 인스턴스를 사용하는 EFS로 데이터를 이동한다 .\n​\nAWS DataSync\n: AWS로 데이터 마이그레이션을 단순화하고 가속화하고 온프레미스 스토리지 , 에지 위치, 기타 클라우\n드 및 \nAWS 스토리지 간에 데이터를 마이그레이션하는 온라인 데이터 마이그레이션 및 검색 서비스이다 .\n​\n▶오답",
      "C": "Create a secondary Amazon Elastic Block Store (Amazon EBS) volume on the EC2 instance for the \ndata.\n데이터에 대한 EC2 인스턴스에 보조 Amazon Elastic Block Store(Amazon EBS) 볼륨을 생성합니다 .\n​→ 데이터에 대한 EC2 인스턴스에 보조 Amazon EBS는 필요하지 않다. 데이터는 EFS로 이동해야 한\n다.\n​\nE.\nUse AWS DataSync to create a suitable location configuration for the on-premises SFTP server.\nAWS DataSync 를 사용하여 온프레미스 SFTP 서버에 적합한 위치 구성을 생성합니다 .\n​→ DataSync 와 SFTP\"on-prem\" 구성 간의 관계를 지을 수 없다. \n​",
      "D": "Manually use an operating system copy command to push the data to the EC2 instance.\n수동으로 운영 체제 복사 명령을 사용하여 데이터를 EC2 인스턴스로 푸시합니다 .\n​\nE.\nUse AWS DataSync to create a suitable location configuration for the on-premises SFTP server.\nAWS DataSync 를 사용하여 온프레미스 SFTP 서버에 적합한 위치 구성을 생성합니다 .\n​"
    },
    "정답": "A"
  },
  {
    "문제번호": 103,
    "질문": "A company has an AWS Glue extract, transform, and load (ETL) job that runs every day at the \nsame time. The job processes XML data that is in an Amazon S3 bucket. New data is added to the \nS3 bucket every day. A solutions architect notices that AWS Glue is processing all the data during \neach run.\nWhat should the solutions architect do to prevent AWS Glue from reprocessing old data?\n회사에 매일 같은 시간에 실행되는 AWS Glue 추출, 변환 및 로드(ETL) 작업이 있습니다 . 작업은 \nAmazon S3 버킷에 있는 XML 데이터를 처리합니다 . 매일 새로운 데이터가 S3 버킷에 추가됩니다 . 솔\n루션 설계자는 AWS Glue가 각 실행 중에 모든 데이터를 처리하고 있음을 알아차렸습니다 .\n솔루션 아키텍트는 AWS Glue가 오래된 데이터를 재처리하지 못하도록 하려면 어떻게 해야 합니까 ?\n​",
    "보기": {
      "A": "Edit the job to use job bookmarks.\n작업 북마크를 사용하도록 작업을 편집합니다 .\n​\n→ AWS Glue 처리된 데이터를 작업 북마크로 추적\n: AWS Glue는 작업 실행의 상태 정보를 유지하여 이전에 ETL 작업을 실행할 때 이미 처리된 데이터\n를 추적한다 . \n이와 같은 지속 상태 정보를 작업 북마크 라고 합니다 .\nAWS Glue는 작업 북마크로 상태 정보를 유지하고 이전 데이터의 재처리를 방지한다 . 작업 북마크를 \n사용하면 예약된 간격으로 재실행 중인 새 데이터를 처리할 수 있다. 작업 북마크는 원본, 변환 및 대\n상과 같은 다양한 작업 요소의 상태로 구성된다 . \n예를 들어, ETL 작업에서 Amazon S3 파일의 새 파티션을 읽어야 할 수 있다. \nAWS Glue는 그 작업에서 처리한 파티션을 추적하여 중복 실행을 방지하고 작업 대상인 데이터 스토어\n에 데이터를 복제한다 .\n​\nAWS Glue\n: 분석 사용자가 여러 소스의 데이터를 쉽게 검색, 준비, 이동, 통합할 수 있도록 하는 서버리스 데이터 \n통합 서비스 . 분석, 기계 학습 및 애플리케이션 개발에 사용할 수 있다. 또한 작성, 작업 실행, 비즈니스 \n워크플로 구현을 위한 추가 생산성 및 데이터 운영 도구도 포함된다 .\n​\n•70개 이상의 다양한 데이터 소스를 검색하여 연결하고 중앙 집중식 데이터 카탈로그에서 데이\n터를 관리할 수 있다.\n•추출, 변환, 로드(ETL) 파이프라인을 시각적으로 생성, 실행, 모니터링하여 데이터 레이크에 데\n이터를 로드할 수 있다.\n•Amazon Athena, Amazon EMR, Amazon Redshift Spectrum 을 사용하여 카탈로그화된 데이터\n를 즉시 검색하고 쿼리할 수 있다.\n​",
      "B": "Edit the job to delete data after the data is processed.\n​데이터가 처리된 후 데이터를 삭제하도록 작업을 편집합니다 .\n​",
      "C": "Edit the job by setting the NumberOfWorkers field to 1.\nNumberOfWorkers 필드를 1로 설정하여 작업을 편집합니다 .\n​",
      "D": "Use a FindMatches machine learning (ML) transform.\nFindMatches 기계 학습(ML) 변환을 사용합니다 .\n​"
    },
    "정답": "A"
  },
  {
    "문제번호": 104,
    "질문": "A solutions architect must design a highly available infrastructure for a website. The website is \npowered by Windows web servers that run on Amazon EC2 instances. The solutions architect must \nimplement a solution that can mitigate a large-scale DDoS attack that originates from thousands of \nIP addresses. Downtime is not acceptable for the website.\nWhich actions should the solutions architect take to protect the website from such an attack? \n(Choose two.)\n솔루션 설계자는 웹사이트를 위한 고가용성 인프라 를 설계해야 합니다 . 웹 사이트는 Amazon EC2 인스\n턴스에서 실행되는 Windows 웹 서버에 의해 구동됩니다 . 솔루션 설계자는 수천 개의 IP 주소에서 시작\n되는대규모 DDoS 공격을 완화할 수 있는 솔루션 을 구현해야 합니다 . 다운타임은 웹사이트에 허용되지 \n않습니다 .\n솔루션 설계자는 이러한 공격으로부터 웹사이트를 보호하기 위해 어떤 조치를 취해야 합니까 ? (2개를 \n선택하세요 .)\n​",
    "보기": {
      "A": "Use AWS Shield Advanced to stop the DDoS attack.\n​AWS Shield Advanced 를 사용하여 DDoS 공격을 차단하십시오 .\n​",
      "B": "Configure Amazon GuardDuty to automatically block the attackers.\n​공격자를 자동으로 차단하도록 Amazon GuardDuty 를 구성합니다 .\n​",
      "C": "Configure the website to use Amazon CloudFront for both static and dynamic content.\n정적 및 동적 콘텐츠 모두에 Amazon CloudFront 를 사용하도록 웹 사이트를 구성합니다 .\n​\n→ AWS Shield 는 DDoS 공격을 처리, 고가용성을 위한 CloudFront.\n​",
      "D": "Use an AWS Lambda function to automatically add attacker IP addresses to VPC network ACLs.\nAWS Lambda 함수를 사용하여 VPC 네트워크 ACL에 공격자 IP 주소를 자동으로 추가합니다 .\n​\nE.\nUse EC2 Spot Instances in an Auto Scaling group with a target tracking scaling policy that is set to \n80% CPU utilization.\n80% CPU 사용률로 설정된 대상 추적 조정 정책과 함께 Auto Scaling 그룹의 EC2 스팟 인스턴스를 사\n용합니다 .\n​"
    },
    "정답": "A"
  },
  {
    "문제번호": 105,
    "질문": "A company is preparing to deploy a new serverless workload. A solutions architect must use the \nprinciple of least privilege to configure permissions that will be used to run an AWS Lambda \nfunction. An Amazon EventBridge (Amazon CloudWatch Events) rule will invoke the function.\nWhich solution meets these requirements?\n회사에서 새로운 서버리스 워크로드 를 배포할 준비를 하고 있습니다 . 솔루션 설계자는 최소 권한 원칙을 \n사용하여 AWS Lambda 함수를 실행하는 데 사용할 권한을 구성해야 합니다 . Amazon \nEventBridge(Amazon CloudWatch Events) 규칙이 함수를 호출합니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n​",
    "보기": {
      "A": "Add an execution role to the function with lambda:InvokeFunction as the action and * as the \nprincipal.\n​lambda:InvokeFunction 을 작업으로 , *를 보안 주체로 사용하여 함수에 실행 역할을 추가합니다 .\n​",
      "B": "Add an execution role to the function with lambda:InvokeFunction as the action and Service: \nlambda.amazonaws.com as the principal.\n​작업으로 lambda:InvokeFunction 을 사용하고 보안 주체로 Service: lambda.amazonaws.com 을 사용하여 \n함수에 실행 \n역할을 추가합니다 .\n​",
      "C": "Add a resource-based policy to the function with lambda:* as the action and Service: \nevents.amazonaws.com as the principal.\n작업으로 lambda:* 를 사용하고 보안 주체로 Service: events.amazonaws.com 을 사용하여 리소스 기반 \n정책을 함수에 \n추가합니다 .\n​",
      "D": "Add a resource-based policy to the function with lambda:InvokeFunction as the action and Service: \nevents.amazonaws.com as the principal.\nlambda:InvokeFunction 을 작업으로 , Service: events.amazonaws.com 을 보안 주체로 사용하여 리소스 \n기반 정책을 함수에 \n추가합니다 .\n​"
    },
    "정답": "D"
  },
  {
    "문제번호": 106,
    "질문": "A company is preparing to store confidential data in Amazon S3. For compliance reasons, the data \nmust be encrypted at rest. Encryption key usage must be logged for auditing purposes. Keys must \nbe rotated every year.\nWhich solution meets these requirements and is the MOST operationally efficient?\n회사에서 Amazon S3에 기밀 데이터를 저장할 준비를 하고 있습니다 . 규정 준수를 위해 미사용 데이터\n를 암호화해야 합니다 . 암호화 키 사용은 감사 목적으로 기록되어야 합니다 . 키는 매년 순환해야 합니\n다.\n어떤 솔루션이 이러한 요구 사항을 충족하고 운영상 가장 효율적입니까 ?\n​",
    "보기": {
      "A": "Server-side encryption with customer-provided keys (SSE-C)\n​고객 제공 키를 사용한 서버 측 암호화 (SSE-C)\n​",
      "B": "Server-side encryption with Amazon S3 managed keys (SSE-S3)\n​Amazon S3 관리형 키를 사용한 서버 측 암호화 (SSE-S3)\n​",
      "C": "Server-side encryption with AWS KMS keys (SSE-KMS) with manual rotation\n수동 교체가 있는 AWS KMS 키(SSE-KMS) 를 사용한 서버 측 암호화\n​",
      "D": "Server-side encryption with AWS KMS keys (SSE-KMS) with automatic rotation\n자동 교체 기능이 있는 AWS KMS 키(SSE-KMS) 를 사용한 서버 측 암호화\n​\nAWS KMS\n: 암호화 작업에 사용되는 키를 쉽게 생성하고 제어할 수 있도록 지원하는 관리형 서비스 .\n고가용성 키 생성, 스토리지 , 관리 및 감사 솔루션을 제공하므로 , 이를 통해 자체 애플리케이션 내 데이\n터를 암호화 또는\n디지털 방식으로 서명하거나 , AWS 서비스 전체에서 데이터의 암호화를 제어할 수 있다.\n​\nAWS KMS HSM 내에서 생성된 키에 한해 AWS KMS가 KMS 키를 매년 자동으로 교체하도록 선택할 \n수 있다. \n키를 가져왔거나 , 키가 비대칭이거나 , AWS CloudHSM 클러스터에서 AWS KMS 사용자 지정 키 스토\n어 기능을 사용해 키를 생성했다면 자동 키 교체 기능이 지원되지 않는다 . \n​\n​"
    },
    "정답": "D"
  },
  {
    "문제번호": 107,
    "질문": "A bicycle sharing company is developing a multi-tier architecture to track the location of its bicycles \nduring peak operating hours. The company wants to use these data points in its existing analytics \nplatform. A solutions architect must determine the most viable multi-tier option to support this \narchitecture. The data points must be accessible from the REST API.\nWhich action meets these requirements for storing and retrieving location data?\n자전거 공유 회사는 피크 운영 시간 동안 자전거의 위치를 ​​추적하기 위해 다층 아키텍처를 개발하고 \n있습니다 . 회사는 기존 분석 플랫폼에서 이러한 데이터 포인트를 사용하려고 합니다 . 솔루션 설계자는 SSE-S3 SSE-KMS SSE-C\nAmazon S3 관리형 키를 사용\n한 서버 측 암호화 (SSE-S3) 를 \n사용하면 각 객체는 고유한 키\n로 암호화된다 . \n또한 추가 보안 조치로 주기적\n으로 교체되는 루트 키를 사용\n하여 키 자체를 암호화한다 . \nAmazon S3 서버 측 암호화는 \n가장 강력한 블록 암호 중 하나\n인 256비트 Advanced \nEncryption Standard(AES-256)\n를 사용하여 데이터를 암호화한\n다.AWS KMS keys를 사용한 서버 \n측 암호화 (SSE-KMS) 는 \nSSE-S3 와 유사하지만 이 서비\n스 사용 시 몇 가지 추가적인 \n이점이 있고 비용이 발생한다 .\n​\nAmazon S3의 객체에 대한 무\n단 액세스에 대응하여 추가적인 \n보호를 제공하는 -KMS 키를 사\n용하려면 별도의 권한이 필요한\n다. \nSSE-KMS 도 KMS 키가 사용된 \n때와 사용 주체를 표시하는 감\n사 추적 기능을 제공한다 . \n또한 고객 관리형 키를 생성하\n고 관리하거나 사용자 , 서비스 \n및 리전에 고유한 AWS 관리형 \nCMK를 사용할 수 있다.고객 제공 키를 사용한 서버 측 \n암호화 (SSE-C) 를 사용하면 사\n용자는 암호화 키를 관리하고 \nAmazon S3는 암호화 (디스크에 \n쓸 때) 및 해독(객체에 액세스\n할 때)을 관리한다 .\n이 아키텍처를 지원하기 위해 가장 실행 가능한 다중 계층 옵션을 결정해야 합니다 . 데이터 포인트는 \nREST API에서 액세스 할 수 있어야 합니다 .\n위치 데이터 저장 및 검색에 대한 이러한 요구 사항을 충족하는 작업은 무엇입니까 ?\n​",
    "보기": {
      "A": "Use Amazon Athena with Amazon S3.\n​Amazon S3와 함께 Amazon Athena 를 사용하십시오 .\n​",
      "B": "Use Amazon API Gateway with AWS Lambda.\n​AWS Lambda 와 함께 Amazon API Gateway 를 사용합니다 .\n​\n→ Amazon API Gateway \n: 어떤 규모에서든 개발자가 API를 손쉽게 게시, 유지 관리, 모니터링 및 보호할 수 있도록 지원하는 완\n전관리형 서비스 .\n​\n▶오답",
      "C": "Use Amazon QuickSight with Amazon Redshift.\nAmazon Redshift 와 함께 Amazon QuickSight 를 사용합니다 .\n→ QuickSight 는 Rest API를 지원하지 않는다 .\n​",
      "D": "Use Amazon API Gateway with Amazon Kinesis Data Analytics.\nAmazon Kinesis Data Analytics 와 함께 Amazon API Gateway 를 사용합니다 .\n→ Athena , Kinesis Data Analytics 는 이미 분석 플랫폼을 보유하고 있다.\n​"
    },
    "정답": "B"
  },
  {
    "문제번호": 108,
    "질문": "A company has an automobile sales website that stores its listings in a database on Amazon RDS. \nWhen an automobile is sold, the listing needs to be removed from the website and the data must \nbe sent to multiple target systems.\nWhich design should a solutions architect recommend?\n한 회사에 Amazon RDS의 데이터베이스에 목록을 저장하는 자동차 판매 웹사이트가 있습니다 . 자동차\n가 판매되면 웹사이트에서 목록을 제거해야 하고 데이터를 여러 대상 시스템으로 보내야 합니다 .\n솔루션 아키텍트는 어떤 디자인을 추천해야 할까요 ?\n​",
    "보기": {
      "A": "Create an AWS Lambda function triggered when the database on Amazon RDS is updated to send \nthe information to an Amazon Simple Queue Service (Amazon SQS) queue for the targets to \nconsume.\n​Amazon RDS의 데이터베이스가 업데이트되어 대상이 소비할 Amazon Simple Queue Service(Amazon \nSQS) 대기열로 정보를 보내도록 업데이트될 때 트리거되는 AWS Lambda 함수를 생성합니다 .\n​",
      "B": "Create an AWS Lambda function triggered when the database on Amazon RDS is updated to send \nthe information to an Amazon Simple Queue Service (Amazon SQS) FIFO queue for the targets to \nconsume.\n​Amazon RDS의 데이터베이스가 대상이 사용할 Amazon Simple Queue Service(Amazon SQS) FIFO \n대기열로 정보를 보내도록 업데이트될 때 트리거되는 AWS Lambda 함수를 생성합니다 .\n​",
      "C": "Subscribe to an RDS event notification and send an Amazon Simple Queue Service (Amazon SQS) \nqueue fanned out to multiple Amazon Simple Notification Service (Amazon SNS) topics. Use AWS \nLambda functions to update the targets.\nRDS 이벤트 알림을 구독하고 여러 Amazon Simple Notification Service(Amazon SNS) 주제로 팬아웃\n된 Amazon Simple Queue Service(Amazon SQS) 대기열을 보냅니다 . AWS Lambda 함수를 사용하여 \n대상을 업데이트합니다 .\n→ Amazon SQS 대기열로 팬아웃\n: Amazon SNS는 Amazon Simple Queue Service(Amazon SQS)와 긴밀하게 작동합니다 . 이러한 서비\n스는 개발자에게 다양한 이점을 제공합니다 . Amazon SNS를 사용하면 애플리케이션이 \"푸시\" 메커니즘\n을 통해 시간이 중요한 메시지를 여러 구독자에게 보낼 수 있으므로 업데이트를 주기적으로 확인하거\n나 \"폴링\"할 필요가 없습니다 . \n​\nAmazon SQS는 분산 애플리케이션이 폴링 모델을 통해 메시지를 교환하는 데 사용하는 메시지 대기열 \n서비스이며 , 각 구성 요소를 동시에 사용할 필요 없이 전송 및 수신 구성 요소를 분리하는 데 사용할 \n수 있습니다 . Amazon SNS와 Amazon SQS를 함께 사용하면 즉각적인 이벤트 알림을 필요로 하는 애\n플리케이션에 메시지를 전송할 수 있고, 다른 애플리케이션에서 나중에 처리할 수 있도록 메시지를 \nAmazon SNS 대기열에 계속 보관할 수도 있습니다 .",
      "D": "Subscribe to an RDS event notification and send an Amazon Simple Notification Service (Amazon \nSNS) topic fanned out to multiple Amazon Simple Queue Service (Amazon SQS) queues. Use AWS \nLambda functions to update the targets.\nRDS 이벤트 알림을 구독하고 Amazon Simple Notification Service(Amazon SNS) 주제를 여러 Amazon \nSimple Queue Service(Amazon SQS) 대기열로 보냅니다 . AWS Lambda 함수를 사용하여 대상을 업데\n이트합니다 .\n→ RDS 이벤트 알림은 SNS를 사용하여 알림을 보낸다 . \n​\n▶오답"
    },
    "정답": "D"
  },
  {
    "문제번호": 109,
    "질문": "A company needs to store data in Amazon S3 and must prevent the data from being changed. The \ncompany wants new objects that are uploaded to Amazon S3 to remain unchangeable for a \nnonspecific amount of time until the company decides to modify the objects. Only specific users in \nthe company's AWS account can have the ability 10 delete the objects.\nWhat should a solutions architect do to meet these requirements?\n​회사는 Amazon S3에 데이터를 저장해야 하며 데이터가 변경되지 않도록 해야 합니다 . 회사는 Amazon \nS3에 업로드된 새 객체가 회사가 객체를 수정하기로 결정할 때까지 일정하지 않은 시간 동안 변경할 \n수 없는 상태로 유지되기를 원합니다 .회사 AWS 계정의 특정 사용자만 객체를 삭제할 수 있습니다 .\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Create an S3 Glacier vault. Apply a write-once, read-many (WORM) vault lock policy to the objects.\n​S3 Glacier 볼트를 생성합니다 . WORM(Write-Once, Read-Many) 볼트 잠금 정책을 개체에 적용합니다 .\n→ 특정 사용자가 삭제할 수 있음\n​\n볼트 \n: 아카이브를 저장할 수 있는 컨테이너이다 . 볼트를 생성할 때는 볼트 이름과 AWS 리전에서 볼트를 생\n성할 수 있다. \n​\nS3 Glacier 볼트 잠금 기능 \n: S3 Glacier 볼트마다 볼트 잠금 정책을 사용해 규정 준수 제어 항목을 쉽게 배포하고 적용할 수 있다. \n이러한 볼트 잠금 정책에서는 'write once, read many' (WORM) 같은 제어 항목을 지정하여 future 편집\n하지 못하도록 정책을 잠글 수 있다.\n볼트 잠금 정책은 future 변경할 수 없도록 볼트를 잠근다는 점에서 규정 준수 제어 항목을 강력하게 \n적용할 수 있다. \n​\n​",
      "B": "Create an S3 bucket with S3 Object Lock enabled. Enable versioning. Set a retention period of 100 \nyears. Use governance mode as the S3 bucket’s default retention mode for new objects.\n​S3 객체 잠금이 활성화된 S3 버킷을 생성합니다 . 버전 관리를 활성화합니다 . 보존 기간을 100년으로 \n설정합니다 . 거버넌스 모드를 새 객체에 대한 S3 버킷의 기본 보존 모드로 사용합니다 .\n→ 비특정 시간\n​",
      "C": "Create an S3 bucket. Use AWS CloudTrail to track any S3 API events that modify the objects. Upon \nnotification, restore the modified objects from any backup versions that the company has.\nS3 버킷을 생성합니다 . AWS CloudTrail 을 사용하여 객체를 수정하는 모든 S3 API 이벤트를 추적합니\n다. 통지 시 회사가 보유한 모든 백업 버전에서 수정된 개체를 복원합니다 .\n→ 데이터 변경 방지",
      "D": "Create an S3 bucket with S3 Object Lock enabled. Enable versioning. Add a legal hold to the \nobjects. Add the s3:PutObjectLegalHold permission to the IAM policies of users who need to delete \nthe objects.\nS3 객체 잠금이 활성화된 S3 버킷을 생성합니다 . 버전 관리를 활성화합니다 . 개체에 법적 보존을 추가\n합니다 . 객체를 삭제해야 하는 사용자의 IAM 정책에 s3:PutObjectLegalHold 권한을 추가합니다 .\n→ 오브젝트 잠금 법적 보류 작업을 통해 오브젝트 버전을 법적 보류할 수 있습니다 . 보존 기간 설정과 \n마찬가지로 법적 보류는 개체 버전을 덮어쓰거나 삭제하는 것을 방지합니다 . 그러나 법적 보류는 관련 \n보존 기간이 없으며 제거될 때까지 유효합니다 .\n​\nS3 객체 잠금 법적 보존\n: 객체 잠금 법적 보존작업을 사용하면 객체 버전에 법적 보전을 적용할 수 있다. 보관 기간 설정과 마\n찬가지로 법적 보존을 사용하면 객체 \n버전을 덮어쓰거나 삭제할 수 없다. 그러나 법적 보존에는 연결된 보관 기간이 없고, 제거될 때까지 유\n효하다 .\n​\n•객체 잠금을 적용한 S3 배치 작업을 사용하여 법적 보존을 한 번에 여러 Amazon S3 객체에 \n추가할 수 있다. \n•매니페스트에 대상 객체를 나열하고 해당 목록을 배치 작업에 제출하면 이 작업을 수행할 수 \n있다. \n•객체 잠금 법적 보존을 사용한 S3 배치 작업은 완료될 때까지 , 취소될 때까지 또는 실패 상태\n에 도달할 때까지 실행된다 .\n​\nS3 배치 작업은 매니페스트의 키를 처리하기 전에 S3 버킷에서 객체 잠금이 활성화되어 있는지 확인\n한다. \n객체 작업 및 버킷 수준 유효성 검사를 수행하려면 S3 배치 작업이 사용자를 대신하여 S3 객체 잠금을 \n호출할 수 있도록 IAM 역할의 s3:PutObjectLegalHold 및 s3:GetBucketObjectLockConfiguration 가 필요\n하다.\n​\n​\n▶오답"
    },
    "정답": "D"
  },
  {
    "문제번호": 110,
    "질문": "A social media company allows users to upload images to its website. The website runs on Amazon \nEC2 instances. During upload requests, the website resizes the images to a standard size and \nstores the resized images in Amazon S3. Users are experiencing slow upload requests to the \nwebsite.\nThe company needs to reduce coupling within the application and improve website performance. A \nsolutions architect must design the most operationally efficient process for image uploads.\nWhich combination of actions should the solutions architect take to meet these requirements? \n(Choose two.)\n소셜 미디어 회사는 사용자가 웹사이트에 이미지를 업로드할 수 있도록 합니다 . 웹 사이트는 Amazon \nEC2 인스턴스에서 실행됩니다 . 업로드 요청 중에 웹 사이트는 이미지의 크기를 표준 크기로 조정하고 \n크기가 조정된 이미지를 Amazon S3에 저장합니다 . 사용자가 웹 사이트에 대한 느린 업로드 요청을 경\n험하고 있습니다 .\n회사는 애플리케이션 내 커플링을 줄이고 웹사이트 성능을 개선해야 합니다 . 솔루션 설계자는 이미지 \n업로드를 위한 운영상 가장 효율적인 프로세스를 설계해야 합니다 .\n이러한 요구 사항을 충족하기 위해 솔루션 설계자가 취해야 하는 조치의 조합은 무엇입니까 ? (2개를 \n선택하세요 .)\n​",
    "보기": {
      "A": "Configure the application to upload images to S3 Glacier.\nS3 Glacier 에 이미지를 업로드하도록 애플리케이션을 구성합니다 .\n​",
      "B": "Configure the web server to upload the original images to Amazon S3.\n원본 이미지를 Amazon S3에 업로드하도록 웹 서버를 구성합니다 .\n된 URL을 사용하여 각 사용자의 브라우저에서 Amazon S3로 이미지를 직접 업로드하도록 애플리케이\n션 구성\n​",
      "C": "Configure the application to upload images directly from each user's browser to Amazon S3 through \nthe use of a presigned URL\n미리 서명된 URL을 사용하여 각 사용자의 브라우저에서 Amazon S3로 이미지를 직접 업로드하도록 애\n플리케이션 구성\n→ 미리 서명된 URL은 성능 향상과는 관계 없음.\n​",
      "D": "Configure S3 Event Notifications to invoke an AWS Lambda function when an image is uploaded. \nUse the function to resize the image.\n이미지가 업로드될 때 AWS Lambda 함수를 호출하도록 S3 이벤트 알림을 구성합니다 . 기능을 사용하\n여 이미지 크기를 조정합니다 .\n​\n▶오답"
    },
    "정답": "B"
  },
  {
    "문제번호": 111,
    "질문": "A company recently migrated a message processing system to AWS. The system receives messages \ninto an ActiveMQ queue running on an Amazon EC2 instance. Messages are processed by a \nconsumer application running on Amazon EC2. The consumer application processes the messages \nand writes results to a MySQL database running on Amazon EC2. The company wants this \napplication to be highly available with low operational complexity.\nWhich architecture offers the HIGHEST availability?\n​한 회사는 최근에 메시지 처리 시스템 을 AWS로 마이그레이션했습니다 . 시스템은 Amazon EC2 인스턴\n스에서 실행되는 ActiveMQ 대기열로 메시지를 수신합니다 . 메시지는 Amazon EC2에서 실행되는 소비\n자 애플리케이션에 의해 처리됩니다 . 소비자 애플리케이션은 메시지를 처리하고 결과를 Amazon EC2에\n서 실행되는 MySQL 데이터베이스에 씁니다 . 회사는 이 애플리케이션이 낮은 운영 복잡성으로 고가용\n성을 갖기를 원합니다 .\n가장 높은 가용성 을 제공하는 아키텍처는 무엇입니까 ?\n​",
    "보기": {
      "A": "Add a second ActiveMQ server to another Availability Zone. Add an additional consumer EC2 \ninstance in another Availability Zone. Replicate the MySQL database to another Availability Zone.\n​다른 가용 영역에 두 번째 ActiveMQ 서버를 추가합니다 . 다른 가용 영역에 소비자 EC2 인스턴스를 추\n가합니다 . MySQL 데이터베이스를 다른 가용 영역에 복제합니다 .\n​",
      "B": "Use Amazon MQ with active/standby brokers configured across two Availability Zones. Add an \nadditional consumer EC2 instance in another Availability Zone. Replicate the MySQL database to \nanother Availability Zone.\n두 가용 영역에 구성된 활성/대기 브로커와 함께 Amazon MQ를 사용합니다 . 다른 가용 영역에 소비자 \nEC2 인스턴스를 추가합니다 . MySQL 데이터베이스를 다른 가용 영역에 복제합니다 .\n​\n→ Amazon RDS for MySQL with Multi-AZ : 낮은 운영 복잡성에 부합하지 않음.\n​",
      "C": "Use Amazon MQ with active/standby brokers configured across two Availability Zones. Add an \nadditional consumer EC2 instance in another Availability Zone. Use Amazon RDS for MySQL with \nMulti-AZ enabled.\n​두 가용 영역에 구성된 활성/대기 브로커와 함께 Amazon MQ를 사용합니다 . 다른 가용 영역에 소비자 \nEC2 인스턴스를 추가합니다 . 다중 AZ가 활성화된 MySQL 용 Amazon RDS를 사용합니다 .\n→ D의 Auto Scaling 그룹은 한개의 인스턴스를 추가하는 것 보다 더 많은 가용성을 제공하며 운영 복\n잡성을 줄여준다 .",
      "D": "Use Amazon MQ with active/standby brokers configured across two Availability Zones. Add an Auto \nScaling group for the consumer EC2 instances across two Availability Zones. Use Amazon RDS for \nMySQL with Multi-AZ enabled.\n​두 가용 영역에 구성된 활성/대기 브로커와 함께 Amazon MQ를 사용합니다 . 두 가용 영역에 걸쳐 소비\n자 EC2 인스턴스에 대한 Auto Scaling 그룹을 추가합니다 . 다중 AZ가 활성화된 MySQL 용 Amazon \nRDS를 사용합니다 .\n​\nAmazon MQ\nAWS에서 메시지 브로커를 쉽게 설정하고 운영할 수 있도록 지원하는 Apache ActiveMQ 및 \nRabbitMQ 용 관리형 메시지 브로커 서비스 . \nAmazon MQ는 메시지 브로커의 프로비저닝 , 설정, 유지보수를 대신 관리하여 운영 부담을 덜어준\n다. \nAmazon MQ는 산업 표준 API 및 프로토콜을 사용하여 기존 애플리케이션에 연결하기 때문에 코드\n를 다시 쓰지 않고도 AWS로 쉽게 옮길 수 있다.\n신속한 마이그레이션 운영 부담 완화 간편하게 메세지 내구성 확보\n•JMS, NMS, AMQP \n1.0 및 0-9-1, STOMP, \nMQTT 및 WebSocket \n등을 비롯한 업계 표\n준 메시징 API와 프로\n토콜을 사용하므로 기\n존 애플리케이션을 쉽\n게 연결할 수 있습니\n다. \n•따라서 애플리케이션\n의 엔드포인트가 •메시지 브로커의 관리 \n및 유지보수를 대신 \n관리하고 자동으로 프\n로비저닝하여 가용성\n을 높여줍니다 .\n•하드웨어를 프로비저\n닝하거나 소프트웨어\n를 설치, 유지할 필요\n가 없으며 Amazon \nMQ가 소프트웨어 업\n그레이드 , 보안 업데이•메시지 브로커에 연결\n할 때 고가용성과 메\n시지 내구성을 확보하\n도록 자동 프로비저닝\n합니다 .\n•AWS 리전의 여러 가\n용 영역(AZ)에 메시지\n를 중복으로 저장하고 \n구성 요소 또는 AZ에 \n장애가 발생하면 이를 \n연속해서 제공합니다 .\n​\n▶오답"
    },
    "정답": "D"
  },
  {
    "문제번호": 112,
    "질문": "A company hosts a containerized web application on a fleet of on-premises servers that process \nincoming requests. The number of requests is growing quickly. The on-premises servers cannot \nhandle the increased number of requests. The company wants to move the application to AWS with \nminimum code changes and minimum development effort.Amazon MQ로 연결되\n도록 업데이트하기만 \n하면 위의 표준을 사\n용하는 메시지 브로커\n에서 Amazon MQ로 \n옮길 수 있습니다 .트, 장애 탐지, 복원 \n등의 작업을 자동 관\n리합니다 .\nWhich solution will meet these requirements with the LEAST operational overhead?\n​회사는 들어오는 요청을 처리하는 온프레미스 서버 집합에서 컨테이너화된 웹 애플리케이션을 호스팅\n합니다 . 요청 수가 빠르게 증가하고 있습니다 . 온프레미스 서버는 증가된 요청 수를 처리할 수 없습니\n다. 회사는 최소한의 코드 변경과 최소한의 개발 노력으로 애플리케이션을 AWS로 옮기기를 원합니다 .\n최소한의 운영 오버헤드 로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까 ?\n​",
    "보기": {
      "A": "Use AWS Fargate on Amazon Elastic Container Service (Amazon ECS) to run the containerized web \napplication with Service Auto Scaling. Use an Application Load Balancer to distribute the incoming \nrequests.\nAmazon Elastic Container Service(Amazon ECS)에서 AWS Fargate 를 사용하여 Service Auto Scaling\n으로 컨테이너화된 웹 애플리케이션을 실행합니다 . Application Load Balancer 를 사용하여 수신 요청을 \n배포합니다 .\n​\n→ AWS Fargate / ECS의 컨테이너 이동 / Auto Scaling / ALB\n: 운영 오버헤드 최소화\n​\n▶오답",
      "B": "Use two Amazon EC2 instances to host the containerized web application. Use an Application Load \nBalancer to distribute the incoming requests.\n​두 개의 Amazon EC2 인스턴스를 사용하여 컨테이너화된 웹 애플리케이션을 호스팅합니다 . Application \nLoad Balancer 를 사용하여 수신 요청을 배포합니다 .\n→ EC2 구성 필요\n​",
      "C": "Use AWS Lambda with a new code that uses one of the supported languages. Create multiple \nLambda functions to support the load. Use Amazon API Gateway as an entry point to the Lambda \nfunctions.\n​지원되는 언어 중 하나를 사용하는 새 코드와 함께 AWS Lambda 를 사용합니다 . 로드를 지원하기 위해 \n여러 Lambda 함수를 생성합니다 . Amazon API Gateway 를 Lambda 함수에 대한 진입점으로 사용합니\n다.\n→ Lambda 의 코드 추가 필요\n​",
      "D": "Use a high performance computing (HPC) solution such as AWS ParallelCluster to establish an HPC \ncluster that can process the incoming requests at the appropriate scale.\nAWS ParallelCluster 와 같은 고성능 컴퓨팅 (HPC) 솔루션을 사용하여 적절한 규모로 들어오는 요청을 \n처리할 수 있는 HPC 클러스터를 설정합니다 .\n→ HPC 플랫폼을 활용하는데 앱을 다시 설정할 필요가 있는것과 같이 복잡한 방식임 ."
    },
    "정답": "A"
  },
  {
    "문제번호": 113,
    "질문": "A company uses 50 TB of data for reporting. The company wants to move this data from on \npremises to AWS. A custom application in the company’s data center runs a weekly data \ntransformation job. The company plans to pause the application until the data transfer is complete \nand needs to begin the transfer process as soon as possible.\nThe data center does not have any available network bandwidth for additional workloads. A solutions \narchitect must transfer the data and must configure the transformation job to continue to run in the \nAWS Cloud.\nWhich solution will meet these requirements with the LEAST operational overhead?\n​회사는 보고를 위해 50TB의 데이터 를 사용합니다 . 회사는 이 데이터를 온프레미스에서 AWS로 이동하\n려고 합니다 . 회사 데이터 센터의 사용자 지정 응용 프로그램은 매주 데이터 변환 작업을 실행합니다 . \n회사는 데이터 이전이 완료되고 가능한 한 빨리 이전 프로세스를 시작해야 할 때까지 응용 프로그램을 \n일시 중지할 계획입니다 .\n데이터 센터에는 추가 워크로드에 사용할 수 있는 네트워크 대역폭이 없습니다 . 솔루션 설계자는 데이\n터를 전송하고 AWS 클라우드에서 계속 실행되도록 변환 작업을 구성해야 합니다 .\n최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까 ?\n​",
    "보기": {
      "A": "Use AWS DataSync to move the data. Create a custom transformation job by using AWS Glue.\n​AWS DataSync 를 사용하여 데이터를 이동합니다 . AWS Glue를 사용하여 사용자 지정 변환 작업을 생\n성합니다 .\n​",
      "B": "Order an AWS Snowcone device to move the data. Deploy the transformation application to the \ndevice.\n​ AWS Snowcone 디바이스에 데이터를 이동하도록 주문합니다 . 장치에 변환 응용 프로그램을 배포합니\n다.\n​→ Snowcone 은 14TB(SSD 구성)만 저장할 수 있다. \n​",
      "C": "Order an AWS Snowball Edge Storage Optimized device. Copy the data to the device. Create a \ncustom transformation job by using AWS Glue.\nAWS Snowball Edge Storage Optimized 디바이스를 주문합니다 . 데이터를 장치에 복사합니다 . AWS \nGlue를 사용하여 사용자 지정 변환 작업을 생성합니다\n​\n​\n▶오답",
      "D": "Order an AWS Snowball Edge Storage Optimized device that includes Amazon EC2 compute. Copy AWS Glue\n분석, 기계 학습 및 애플리케이션 개발을 위해 데이터를 쉽게 탐색, 준비, \n그리고 조합할 수 있도록 지원하는 서버리스 데이터 통합 서비스\n데이터 통합에 필요한 모든 기능을 제공하므로 , 몇 개월이 아니라 몇 분 안에 \n데이터 분석을 시작하고 해당 내용을 활용할 수 있다.\n보다 빠른 데이터 통합 대규모 데이터 통합 자동화 관리할 서버 없음\n•조직 전체의 여러 그\n룹이 AWS Glue를 사\n용하여 추출, 정리, 정\n규화, 조합, 로드, 확장 \n가능한 ETL 워크플로 \n실행 등의 데이터 통\n합 작업을 함께 수행\n할 수 있습니다 . \n•이러한 방식으로 데이\n터를 분석하고 사용하\n는데 걸리는 시간을 \n몇 개월에서 몇 분으\n로 단축할 수 있습니\n다.•AWS Glue는 데이터 \n통합에 필요한 많은 \n작업을 자동화합니다 .\n•AWS Glue는 데이터 \n원본을 크롤링하고 , 데\n이터 형식을 파악하고 , \n데이터 저장을 위한 \n스키마를 제안합니다 . \n•Glue는 자동으로 코드\n를 생성하여 데이터 \n변환 및 데이터 로딩 \n프로세스를 실행합니\n다. \n•AWS Glue를 사용하\n여 수천 개의 ETL 작\n업을 쉽게 실행 및 관\n리하거나 SQL을 사용\n하는 여러 데이터 저\n장소 간에서 데이터를 \n조합 및 복제할 수 있\n습니다 .•AWS Glue는 서버리스 \n환경에서 작동합니다 . \n관리할 인프라가 없으\n며 AWS Glue가 데이\n터 확장 작업을 실행\n하는 데 필요한 리소\n스를 프로비저닝 , 구성 \n및 확장합니다 . \n•고객은 실행 중 작업\n에 사용되는 리소스에 \n대해서만 비용을 지불\n합니다 .\nthe data to the device. Create a new EC2 instance on AWS to run the transformation application.\n​Amazon EC2 컴퓨팅이 포함된 AWS Snowball Edge Storage Optimized 디바이스를 주문합니다 . 데이\n터를 장치에 복사합니다 . AWS에서 새 EC2 인스턴스를 생성하여 변환 애플리케이션을 실행합니다 .\n​→ C와 동일하지만 ETL 작업을 수행하려면 EC2 인스턴스의 배포/구성/유지보수가 필요한 반면 Glue는 \n서버리스 서비스이다 . \n이는 D가 C보다 운영 오버헤드가 더 많다는 것을 의미한다 .\n​"
    },
    "정답": "C"
  },
  {
    "문제번호": 114,
    "질문": "A company has created an image analysis application in which users can upload photos and add \nphoto frames to their images. The users upload images and metadata to indicate which photo frames \nthey want to add to their images. The application uses a single Amazon EC2 instance and Amazon \nDynamoDB to store the metadata.\nThe application is becoming more popular, and the number of users is increasing. The company \nexpects the number of concurrent users to vary significantly depending on the time of day and day \nof week. The company must ensure that the application can scale to meet the needs of the growing \nuser base.\nWhich solution meats these requirements?\n​한 회사는 사용자가 사진을 업로드하고 이미지에 액자를 추가할 수 있는 이미지 분석 응용 프로그램을 \n만들었습니다 . 사용자는 이미지와 메타데이터를 업로드하여 이미지에 추가할 사진 프레임을 나타냅니\n다. 애플리케이션은 단일 Amazon EC2 인스턴스와 Amazon DynamoDB 를 사용하여 메타데이터를 저장\n합니다 .\n응용 프로그램이 대중화되고 사용자 수가 증가하고 있습니다 . 회사는 동시 접속자 수가 시간과 요일에 \n따라 크게 달라질 것으로 예상하고 있습니다 . 회사는 증가하는 사용자 기반의 요구 사항을 충족하도록 \n애플리케이션을 확장할 수 있는지 확인해야 합니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n​",
    "보기": {
      "A": "Use AWS Lambda to process the photos. Store the photos and metadata in DynamoDB.\n​AWS Lambda 를 사용하여 사진을 처리합니다 . 사진과 메타데이터를 DynamoDB 에 저장합니다 .\n​",
      "B": "Use Amazon Kinesis Data Firehose to process the photos and to store the photos and metadata.\n​Amazon Kinesis Data Firehose 를 사용하여 사진을 처리하고 사진과 메타데이터를 저장합니다 .\n​",
      "C": "Use AWS Lambda to process the photos. Store the photos in Amazon S3. Retain DynamoDB to \nstore the metadata.\n​AWS Lambda 를 사용하여 사진을 처리합니다 . Amazon S3에 사진을 저장합니다 . DynamoDB 를 유지하\n여 메타데이터를 저장합니다 .\n→ DB에 이미지를 저장하는 것은 S3에 비해 확장성이 높지 않다.\n메타데이터는 많은 공간을 차지하지 않으며 DB에 더 효율적으로 저장된다 .",
      "D": "Increase the number of EC2 instances to three. Use Provisioned IOPS SSD (io2) Amazon Elastic \nBlock Store (Amazon EBS) volumes to store the photos and metadata.\nEC2 인스턴스 수를 3개로 늘립니다 . 프로비저닝된 IOPS SSD(io2) Amazon Elastic Block \nStore(Amazon EBS) 볼륨을 사용하여 사진과 메타데이터를 저장합니다 .\n​"
    },
    "정답": "C"
  },
  {
    "문제번호": 115,
    "질문": "A medical records company is hosting an application on Amazon EC2 instances. The application \nprocesses customer data files that are stored on Amazon S3. The EC2 instances are hosted in \npublic subnets. The EC2 instances access Amazon S3 over the internet, but they do not require any \nother network access.\nA new requirement mandates that the network traffic for file transfers take a private route and not be \nsent over the internet.\nWhich change to the network architecture should a solutions architect recommend to meet this \nrequirement?\n​의료 기록 회사는 Amazon EC2 인스턴스에서 애플리케이션을 호스팅하고 있습니다 . 애플리케이션은 \nAmazon S3에 저장된 고객 데이터 파일을 처리합니다 . EC2 인스턴스는 퍼블릭 서브넷에서 호스팅됩니\n다. EC2 인스턴스는 인터넷을 통해 Amazon S3에 액세스하지만 다른 네트워크 액세스는 필요하지 않\n습니다 .\n새로운 요구 사항은 파일 전송을 위한 네트워크 트래픽이 인터넷을 통해 전송되지 않고 개인 경로를 \n사용하도록 규정하고 있습니다 .\n솔루션 설계자가 이 요구 사항을 충족하기 위해 권장해야 하는 네트워크 아키텍처 변경 사항은 무엇입\n니까?\n​",
    "보기": {
      "A": "Create a NAT gateway. Configure the route table for the public subnets to send traffic to Amazon \nS3 through the NAT gateway.\n​NAT 게이트웨이를 생성합니다 . NAT 게이트웨이를 통해 Amazon S3로 트래픽을 전송하도록 퍼블릭 서\n브넷에 대한 라우팅 테이블을 구성합니다 .\n→NAT 게이트웨이에는 아웃바운드 인터넷 트래픽을 위한 인터넷 게이트웨이가 필요하다 .\n​",
      "B": "Configure the security group for the EC2 instances to restrict outbound traffic so that only traffic to \nthe S3 prefix list is permitted.\n​ S3 접두사 목록에 대한 트래픽만 허용되도록 아웃바운드 트래픽을 제한하도록 EC2 인스턴스에 대한 \n보안 그룹을 \n구성합니다 .\n→ 인터넷을 통과해야한다 .\n​",
      "C": "Move the EC2 instances to private subnets. Create a VPC endpoint for Amazon S3, and link the \nendpoint to the route table for the private subnets.\n​EC2 인스턴스를 프라이빗 서브넷으로 이동합니다 . Amazon S3용 VPC 엔드포인트를 생성하고 엔드포\n인트를 프라이빗 \n서브넷의 라우팅 테이블에 연결합니다 .\n​\n→ VPC엔드포인트\n: 인터넷 게이트웨이 , NAT 디바이스 , VPN 연결 또는 AWS Direct Connect 연결이 필요 없이 Virtual \nPrivate Cloud(VPC) 와 \n지원 서비스 간에 연결을 설정할 수 있다. 그러므로 VPC 엔드포인트는 인터넷을 통과하지 않고 S3 트\n래픽을 라우팅하는\n최선의 선택이다 . \n​\n​\n▶오답",
      "D": "Remove the internet gateway from the VPC. Set up an AWS Direct Connect connection, and route \ntraffic to Amazon S3 over the Direct Connect connection.\n​VPC에서 인터넷 게이트웨이를 제거합니다 . AWS Direct Connect 연결을 설정하고 Direct Connect 연결\n을 통해 Amazon S3로 트래픽을 라우팅합니다 .\n​\n→ AWS Direct Connect\n: 온프레미스에서 AWS로 전용 네트워크 연결을 쉽게 설정할 수 있는 클라우드 서비스 솔루션이다 . \nAWS Direct Connect 로 AWS와 고객의 데이터 센터, 사무실 또는 코로케이션 환경 간에 프라이빗 연결\n을 설정할 수 있다."
    },
    "정답": "C"
  },
  {
    "문제번호": 116,
    "질문": "A company uses a popular content management system (CMS) for its corporate website. However, \nthe required patching and maintenance are burdensome. The company is redesigning its website and \nwants anew solution. The website will be updated four times a year and does not need to have any \ndynamic content available. The solution must provide high scalability and enhanced security.\nWhich combination of changes will meet these requirements with the LEAST operational overhead? \n(Choose two.)\n​회사는 회사 웹 사이트에 널리 사용되는 CMS( 콘텐츠 관리 시스템 )를 사용합니다 . 그러나 필요한 패치 \n및 유지 보수가 부담됩니다 . 회사는 웹사이트를 재설계하고 있으며 새로운 솔루션을 원합니다 . 웹사이\n트는 1년에 4번 업데이트되며 사용 가능한 동적 콘텐츠가 필요하지 않습니다 .솔루션은 높은 확장성과 \n향상된 보안을 제공해야 합니다 .\n최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 변경 조합은 무엇입니까 ? (2개를 선택하세요 .)\n​",
    "보기": {
      "A": "Configure Amazon CloudFront in front of the website to use HTTPS functionality.\n​HTTPS 기능을 사용하도록 웹 사이트 앞에 Amazon CloudFront 를 구성합니다 .\n→ 최종 사용자와 CloudFront 간의 통신에 HTTPS 요구\n: CloudFront 배포에 하나 이상의 캐시 동작을 구성하여 최종 사용자와 CloudFront 간의 통신에 \nHTTPS 를 요구할 수 있다. \n클라이언트에서 HTTPS 를 요구하도록 CloudFront 를 구성함으로 보안을 강화시킬 수 있다.\n​",
      "B": "Deploy an AWS WAF web ACL in front of the website to provide HTTPS functionality.\n​웹 사이트 앞에 AWS WAF 웹 ACL을 배포하여 HTTPS 기능을 제공합니다 .\n​",
      "C": "Create and deploy an AWS Lambda function to manage and serve the website content.\n​웹 사이트 콘텐츠를 관리하고 제공하기 위해 AWS Lambda 함수를 생성 및 배포합니다 .\n​",
      "D": "Create the new website and an Amazon S3 bucket. Deploy the website on the S3 bucket with static \nwebsite hosting enabled.\n새 웹 사이트와 Amazon S3 버킷을 생성합니다 . 정적 웹 사이트 호스팅이 활성화된 S3 버킷에 웹 사이\n트를 배포합니다 .\n​→ S3에 정적 웹사이트를 저장하여 확장성과 운영 오버헤드를 줄일 수 있다. ​"
    },
    "정답": "A"
  },
  {
    "문제번호": 117,
    "질문": "A company stores its application logs in an Amazon CloudWatch Logs log group. A new policy \nrequires the company to store all application logs in Amazon OpenSearch Service (Amazon \nElasticsearch Service) in near-real time.\nWhich solution will meet this requirement with the LEAST operational overhead?\n회사는 Amazon CloudWatch Logs 로그 그룹에 애플리케이션 로그를 저장합니다 . 새로운 정책에 따라 \n회사는 모든 애플리케이션 로그를 Amazon OpenSearch Service(Amazon Elasticsearch Service) 에 거의 \n실시간으로 저장해야 합니다 .\n최소한의 운영 오버헤드 로 이 요구 사항을 충족하는 솔루션은 무엇입니까 ?\n​",
    "보기": {
      "A": "Configure a CloudWatch Logs subscription to stream the logs to Amazon OpenSearch Service \n(Amazon Elasticsearch Service).\n​Amazon OpenSearch Service(Amazon Elasticsearch Service) 로 로그를 스트리밍하도록 CloudWatch \nLogs 구독을 \n구성합니다 .\n​\n→ 구독을 통한 로그 데이터 실시간 처리\n: CloudWatch Logs 구독을 사용하여 로그 데이터를 거의 실시간으로 Amazon OpenSearch Service 클\n러스터로 스트리밍 할 수 있다.\n스트리밍되는 로그 데이터의 양에 따라 함수에 함수 수준의 동시 실행 제한을 설정할 수 있습니다 . \n​\nAmazon OpenSearch Service\n: AWS 클라우드에서 OpenSearch 클러스터를 손쉽게 배포, 운영 및 확장할 수 있도록 해주는 관리형 \n서비스 .\nOpenSearch 는 로그 분석, 실시간 애플리케이션 모니터링 , 클릭 스트림 분석 같은 사용 사례를 위한 완\n전한 오픈 소스 검색 및 분석 엔진.\n•클러스터의 리소스를 모두 프로비저닝하고 클러스터를 시작한다 .\n•실패한 OpenSearch Service 노드를 자동으로 감지한 다음 교체해 자체 관리형 인프라와 관련\n된 오버헤드를 줄인다 .\n•API를 한 번만 호출하거나 콘솔에서 몇 번만 클릭하여 클러스터를 조정할 수 있다.",
      "B": "Create an AWS Lambda function. Use the log group to invoke the function to write the logs to \nAmazon OpenSearch Service (Amazon Elasticsearch Service).\n​AWS Lambda 함수를 생성합니다 . 로그 그룹을 사용하여 Amazon OpenSearch Service(Amazon \nElasticsearch Service) 에 \n로그를 작성하는 함수를 호출합니다 .\n​",
      "C": "Create an Amazon Kinesis Data Firehose delivery stream. Configure the log group as the delivery \nstreams sources. Configure Amazon OpenSearch Service (Amazon Elasticsearch Service) as the \ndelivery stream's destination.\n​Amazon Kinesis Data Firehose 전송 스트림을 생성합니다 . 로그 그룹을 배달 스트림 소스로 구성합니\n다. Amazon OpenSearch Service(Amazon Elasticsearch Service) 를 전송 스트림의 대상으로 구성합니\n다.\n​",
      "D": "Install and configure Amazon Kinesis Agent on each application server to deliver the logs to Amazon \nKinesis Data Streams. Configure Kinesis Data Streams to deliver the logs to Amazon OpenSearch \nService (Amazon Elasticsearch Service).\n​각 애플리케이션 서버에 Amazon Kinesis 에이전트를 설치 및 구성하여 Amazon Kinesis Data Streams\n에 로그를 전달합니다 . Amazon OpenSearch Service(Amazon Elasticsearch Service) 에 로그를 전달하\n도록 Kinesis Data Streams 를 구성합니다 .\n​"
    },
    "정답": "A"
  },
  {
    "문제번호": 118,
    "질문": "A company is building a web-based application running on Amazon EC2 instances in multiple \nAvailability Zones. The web application will provide access to a repository of text documents totaling \nabout 900 TB in size. The company anticipates that the web application will experience periods of \nhigh demand. A solutions architect must ensure that the storage component for the text documents \ncan scale to meet the demand of the application at all times. The company is concerned about the \noverall cost of the solution.\nWhich storage solution meets these requirements MOST cost-effectively?\n​회사는 여러 가용 영역의 Amazon EC2 인스턴스에서 실행되는 웹 기반 애플리케이션을 구축하고 있습\n니다. 웹 애플리케이션은 약 900TB 크기의 텍스트 문서 저장소에 대한 액세스를 제공합니다 . 회사는 \n웹 응용 프로그램이 수요가 많은 기간을 경험할 것으로 예상합니다 . 솔루션 설계자는 텍스트 문서의 스\n토리지 구성 요소가 애플리케이션의 요구 사항을 항상 충족할 수 있도록 확장할 수 있는지 확인해야 합\n니다. 회사는 솔루션의 전체 비용에 대해 우려하고 있습니다 .\n어떤 스토리지 솔루션이 이러한 요구 사항을 가장 비용 효율적 으로 충족합니까 ?\n​",
    "보기": {
      "A": "Amazon Elastic Block Store (Amazon EBS)\n​",
      "B": "Amazon Elastic File System (Amazon EFS)\n​",
      "C": "Amazon OpenSearch Service (Amazon Elasticsearch Service)\n​",
      "D": "Amazon S3\n→ 스토리지 중 가장 비용 효율적임"
    },
    "정답": "D"
  },
  {
    "문제번호": 119,
    "질문": "A global company is using Amazon API Gateway to design REST APIs for its loyalty club users in \nthe us-east-1 Region and the ap-southeast-2 Region. A solutions architect must design a solution to \nprotect these API Gateway managed REST APIs across multiple accounts from SQL injection and \ncross-site scripting attacks.\nWhich solution will meet these requirements with the LEAST amount of administrative effort?EBS (Elastic Block Storage) - \n블록S3 (Simple Storage Service) - \n객체EFS (Elastic File System) - \n파일 공유\nEC2에 마운트 가능 EC2에 마운트 불가능 EC2에 마운트 가능\n빈번한 읽기/쓰기 추천빈번한 읽기와 때때로의 쓰기 \n추천빈번한 읽기/쓰기 추천\n서비스 붙을 때 AZ 제한 O 서비스 붙을 때 AZ 제한 X 서비스 붙을 때 AZ 제한 X\n가격 평균 가격 저렴 가격 비쌈\n​글로벌 회사는 Amazon API Gateway 를 사용하여 us-east-1 리전 및 ap-southeast-2 리전의 로열티 클\n럽 사용자를 위한 REST API를 설계하고 있습니다 . 솔루션 설계자는 SQL 주입 및 교차 사이트 스크립\n팅 공격으로부터 여러 계정에서 이러한 API Gateway 관리 REST API를 보호하는 솔루션을 설계해야 합\n니다.\n최소한의 관리 노력으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까 ?\n​",
    "보기": {
      "A": "Set up AWS WAF in both Regions. Associate Regional web ACLs with an API stage.\n​두 리전에 AWS WAF를 설정합니다 . 리전 웹 ACL을 API 단계와 연결합니다 .\n​",
      "B": "Set up AWS Firewall Manager in both Regions. Centrally configure AWS WAF rules.\n​두 리전에 AWS Firewall Manager 를 설정합니다 . AWS WAF 규칙을 중앙에서 구성합니다 .\n​\nAWS Firewall Manager\n: AWS WAF, AWS Shield Advanced, Amazon VPC 보안 그룹, AWS Network Firewall 및 Amazon \nRoute 53 Resolver DNS 방화벽을 비롯한 다양한 보호를 위해 여러 계정및 리소스에 대한 관리 및 유\n지 관리 작업을 간소화한다 . \nFirewall Manager 를 사용하면 보호를 한 번만 설정하면 새 계정과 리소스를 추가하는 경우에도 서비스\n가 자동으로 계정과 리소스 전체에 보호를 적용한다 .\n​\nAWS WAF\n: 고객이 정의한 조건에 따라 웹 요청을 허용, 차단 또는 모니터링 (계수)하는 규칙을 구성하여 공격으로\n부터 웹 애플리케이션을 보호하는 웹 애플리케이션 방화벽이다 . AWS WAF는 SQL 명령어 주입과 교차 \n사이트 스크립팅 (XSS) 과 같은 일반적인 공격 기술로부터 웹 사이트를 보호할 수 있다. 또한, 특정 \nUser-Agents, 배드 봇 또는 콘텐츠 스크래퍼의 공격을 차단하는 규칙을 생성할 수 있다. ​",
      "C": "Set up AWS Shield in bath Regions. Associate Regional web ACLs with an API stage.\n​bath Regions 에서 AWS Shield 를 설정합니다 . 리전 웹 ACL을 API 단계와 연결합니다 .\n​",
      "D": "Set up AWS Shield in one of the Regions. Associate Regional web ACLs with an API stage.\n한 리전에서 AWS Shield 를 설정합니다 . 리전 웹 ACL을 API 단계와 연결합니다 .\n​"
    },
    "정답": "B"
  },
  {
    "문제번호": 120,
    "질문": "A company has implemented a self-managed DNS solution on three Amazon EC2 instances behind \na Network Load Balancer (NLB) in the us-west-2 Region. Most of the company's users are located \nin the United States and Europe. The company wants to improve the performance and availability of \nthe solution. The company launches and configures three EC2 instances in the eu-west-1 Region \nand adds the EC2 instances as targets for a new NLB.\nWhich solution can the company use to route traffic to all the EC2 instances?\n​한 회사는 us-west-2 리전의 NLB(Network Load Balancer) 뒤에 있는 3개의 Amazon EC2 인스턴스에 \n자체 관리형 DNS 솔루션 을 구현했습니다 . 회사 사용자의 대부분은 미국과 유럽에 있습니다 . 회사는 솔\n루션의 성능과 가용성을 개선하기를 원합니다 . 회사는 eu-west-1 리전에서 3개의 EC2 인스턴스를 시작 \n및 구성하고 EC2 인스턴스를 새 NLB의 대상으로 추가합니다 .\n회사에서 트래픽을 모든 EC2 인스턴스로 라우팅하는 데 사용할 수 있는 솔루션 은 무엇입니까 ?\n​",
    "보기": {
      "B": "Create a standard accelerator in AWS Global Accelerator. Create endpoint groups in us-west-2 and \neu-west-1. Add the two NLBs as endpoints for the endpoint groups.\n​AWS Global Accelerator 에서 표준 액셀러레이터를 생성합니다 . us-west-2 및 eu-west-1 에서 엔드포인트 \n그룹을 생성합니다 . 엔드포인트 그룹에 대한 엔드포인트로 두 개의 NLB를 추가하십시오 .\n​\n​AWS Global Accelerator\n: 로컬 및 글로벌 사용자를 위해 애플리케이션 성능을 개선하기 위해 가속기 를 생성하는 서비스이며 , \n여러 AWS 리전에서 엔드포인트를 지원하는 글로벌 서비스 . \n표준 액셀러레이터의 경우 Global Accelerator 는 AWS 글로벌 네트워크를 사용하여 구성한 상태, 클라\n이언트 위치 및 정책을 기반으로 최적의 리전 엔드포인트로 트래픽을 라우팅하므로 애플리케이션의 가\n용성이 향상된다 . \n표준 액셀러레이터의 엔드포인트는 하나의 AWS 리전 또는 여러 리전에 있는 Network Load Balancer, \nApplication Load Balancer, Amazon EC2 인스턴스 또는 탄력적 IP 주소일 수 있다. \n서비스는 상태 또는 구성의 변경 사항에 즉시 반응하여 클라이언트의 인터넷 트래픽이 항상 정상적인 \n엔드포인트로 \n전달되도록 한다.\n​\n•표준 가속기를 사용하면 전 세계 사용자가 사용하는 인터넷 애플리케이션의 가용성을 향상시\n킬 수 있다. 표준 액셀러레이터를 사용하는 Global Accelerator 는 AWS 글로벌 네트워크를 통\n해 클라이언트와 가장 가까운 리전의 엔드포인트로 트래픽을 보낸다 .\n•사용자 지정 라우팅 가속기를 사용하면 한 명 이상의 사용자를 여러 대상 중 특정 대상에 매\n핑할 수 있다.",
      "A": "Create an Amazon Route 53 geolocation routing policy to route requests to one of the two NLBs. \nCreate an Amazon CloudFront distribution. Use the Route 53 record as the distribution’s origin.\n​두 NLB 중 하나로 요청을 라우팅하는 Amazon Route 53 지리적 위치 라우팅 정책을 생성합니다 . \nAmazon CloudFront 배포를 생성합니다 . Route 53 레코드를 배포의 오리진으로 사용합니다 .\n​",
      "C": "Attach Elastic IP addresses to the six EC2 instances. Create an Amazon Route 53 geolocation \nrouting policy to route requests to one of the six EC2 instances. Create an Amazon CloudFront \ndistribution. Use the Route 53 record as the distribution's origin.\n​탄력적 IP 주소를 6개의 EC2 인스턴스에 연결합니다 . 6개의 EC2 인스턴스 중 하나로 요청을 라우팅하\n는 Amazon Route 53 지리적 위치 라우팅 정책을 생성합니다 . Amazon CloudFront 배포를 생성합니다 . \nRoute 53 레코드를 배포의 오리진으로 사용합니다 .\n​",
      "D": "Replace the two NLBs with two Application Load Balancers (ALBs). Create an Amazon Route 53 \nlatency routing policy to route requests to one of the two ALBs. Create an Amazon CloudFront \ndistribution. Use the Route 53 record as the distribution’s origin.\n​2개의 NLB를 2개의 ALB(Application Load Balancer) 로 교체합니다 . 두 ALB 중 하나로 요청을 라우팅하\n는 Amazon Route 53 지연 시간 라우팅 정책을 생성합니다 . Amazon CloudFront 배포를 생성합니다 . \nRoute 53 레코드를 배포의 오리진으로 사용합니다 .\n​"
    },
    "정답": "B"
  },
  {
    "문제번호": 121,
    "질문": "A company is running an online transaction processing (OLTP) workload on AWS. This workload \nuses an unencrypted Amazon RDS DB instance in a Multi-AZ deployment. Daily database snapshots \nare taken from this instance.\nWhat should a solutions architect do to ensure the database and snapshots are always encrypted \nmoving forward?\n​회사는 AWS에서 OLTP( 온라인 트랜잭션 처리) 워크로드를 실행하고 있습니다 . 이 워크로드는 다중 AZ \n배포에서 암호화되지 않은 Amazon RDS DB 인스턴스 를 사용합니다 . 일일 데이터베이스 스냅샷은 이 \n인스턴스에서 가져옵니다 .\n데이터베이스와 스냅샷이 앞으로 항상 암호화 되도록 하려면 솔루션 설계자가 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Encrypt a copy of the latest DB snapshot. Replace existing DB instance by restoring the encrypted \nsnapshot.\n​최신 DB 스냅샷 사본을 암호화합니다 . 암호화된 스냅샷을 복원하여 기존 DB 인스턴스를 교체합니다 .\n​\n→ Amazon RDS DB 인스턴스를 생성할 때 암호화를 활성화할 수 있지만 생성 후에는 활성화할 수 없\n다. 그러나 DB 인스턴스의 스냅샷을 \n생성한 다음 해당 스냅샷의 암호화된 복사본을 생성하여 암호화되지 않은 DB 인스턴스에 암호화를 추\n가할 수 있다. \n그런 다음 암호화된 스냅샷에서 DB 인스턴스를 복원하여 원본 DB 인스턴스의 암호화된 복사본을 얻을 \n수 있다. \n​",
      "B": "Create a new encrypted Amazon Elastic Block Store (Amazon EBS) volume and copy the snapshots \nto it. Enable encryption on the DB instance.\n​새 암호화된 Amazon Elastic Block Store(Amazon EBS) 볼륨을 생성하고 여기에 스냅샷을 복사합니다 . \nDB 인스턴스에서 암호화를 활성화합니다 .\n​",
      "C": "Copy the snapshots and enable encryption using AWS Key Management Service (AWS KMS) \nRestore encrypted snapshot to an existing DB instance.\n​AWS Key Management Service(AWS KMS)를 사용하여 스냅샷을 복사하고 암호화를 활성화합니다 . 암\n호화된 스냅샷을 기존 DB 인스턴스로 복원합니다 .\n​",
      "D": "Copy the snapshots to an Amazon S3 bucket that is encrypted using server-side encryption with \nAWS Key Management Service (AWS KMS) managed keys (SSE-KMS).\n​AWS Key Management Service(AWS KMS) 관리형 키(SSE-KMS) 로 서버 측 암호화를 사용하여 암호화\n된 Amazon S3 버킷에 스냅샷을 복사합니다 .\n​"
    },
    "정답": null
  },
  {
    "문제번호": 122,
    "질문": "A company wants to build a scalable key management infrastructure to support developers who need \nto encrypt data in their applications.\nWhat should a solutions architect do to reduce the operational burden?\n​회사는 응용 프로그램의 데이터를 암호화해야 하는 개발자를 지원하기 위해 확장 가능한 키 관리 인프\n라를 구축하려고 합니다 .\n솔루션 설계자는 운영 부담을 줄이기 위해 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Use multi-factor authentication (MFA) to protect the encryption keys.\n​MFA(다단계 인증)를 사용하여 암호화 키를 보호합니다 .\n​",
      "B": "Use AWS Key Management Service (AWS KMS) to protect the encryption keys.\n​AWS Key Management Service(AWS KMS)를 사용하여 암호화 키를 보호합니다 .\n​\nAWS Key Management Service (AWS KMS)\n암호화 작업에 사용되는 키를 쉽게 생성하고 제어할 수 있도록 지원하는 관리형 서비스 . \n이 서비스는 고가용성 키 생성, 스토리지 , 관리 및 감사 솔루션을 제공하므로 , 이를 통해 자체 애플\n리케이션 내 데이터를 \n암호화 또는 디지털 방식으로 서명하거나 , AWS 서비스 전체에서 데이터의 암호화를 제어할 수 있\n다.\nAWS 서비스 전 비대칭 키를 사용 애플리케이션에서 개발자 및 증가하 규제 또는 규정 준",
      "C": "Use AWS Certificate Manager (ACM) to create, store, and assign the encryption keys.\n​AWS Certificate Manager(ACM) 를 사용하여 암호화 키를 생성, 저장 및 할당합니다 .\n​",
      "D": "Use an IAM policy to limit the scope of users who have access permissions to protect the \nencryption keys.\n​ IAM 정책을 사용하여 암호화 키를 보호할 수 있는 액세스 권한이 있는 사용자의 범위를 제한합니다 .\n​"
    },
    "정답": null
  },
  {
    "문제번호": 123,
    "질문": "A company has a dynamic web application hosted on two Amazon EC2 instances. The company has \nits own SSL certificate, which is on each instance to perform SSL termination.\nThere has been an increase in traffic recently, and the operations team determined that SSL \nencryption and decryption is causing the compute capacity of the web servers to reach their \nmaximum limit.\nWhat should a solutions architect do to increase the application's performance?\n​회사에 두 개의 Amazon EC2 인스턴스에서 호스팅되는 동적 웹 애플리케이션이 있습니다 . 회사에는 \nSSL 종료를 수행하기 위해 각 인스턴스에 있는 자체 SSL 인증서가 있습니다 .\n최근 트래픽이 증가하고 있으며 운영팀은 SSL 암호화 및 복호화로 인해 웹 서버의 컴퓨팅 용량이 최대 \n한도에 도달했다고 판단했습니다 .\n솔루션 설계자는 애플리케이션의 성능을 향상시키기 위해 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Create a new SSL certificate using AWS Certificate Manager (ACM). Install the ACM certificate on \neach instance.\n​AWS Certificate Manager(ACM) 를 사용하여 새 SSL 인증서를 생성합니다 . 각 인스턴스에 ACM 인증서\n를 설치합니다 .\n​",
      "B": "Create an Amazon S3 bucket Migrate the SSL certificate to the S3 bucket. Configure the EC2 \ninstances to reference the bucket for SSL termination.\n​Amazon S3 버킷 생성 SSL 인증서를 S3 버킷으로 마이그레이션합니다 . SSL 종료를 위해 버킷을 참조\n하도록 EC2 인스턴스를 구성합니다 .\n​",
      "C": "Create another EC2 instance as a proxy server. Migrate the SSL certificate to the new instance and 반에서 데이터를 \n보호할 책임이 있\n는 경우, \nAWS KMS를 사\n용하면 데이터에 \n대한 액세스를 제\n어하는 암호화 키\n를 중앙에서 관리\n할 수 있다.하여 데이터를 디\n지털 방식으로 서\n명하거나 확인해\n야 하는 개발자의 \n경우, \n이 서비스를 사용\n하여 필요한 프라\n이빗 키를 생성하\n고 관리할 수 있\n다. 데이터를 암호화\n해야 하는 개발자\n의 경우, \nAWS KMS와 함\n께 AWS \nEncryption SDK\n를 사용하면 코드\n에서 대칭 암호화 \n키를 손쉽게 생\n성, 사용 및 보호\n할 수 있다.는 애플리케이션\n을 지원하기 위해 \n확장 가능한 키 \n관리 인프라를 찾\n고 있는 경우, \n이 서비스를 사용\n하면 라이선스 비\n용과 운영 부담을 \n줄일 수 있다. 수 목적으로 데이\n터 보안을 입증할 \n책임이 있는 경우, \n이 서비스를 사용\n하면 데이터가 일\n관되게 보호되고 \n있음을 쉽게 입증\n할 수 있다. 또한 \n광범위한 산업 및 \n지역 규정 준수 체\n제에 적용된다 .\nconfigure it to direct connections to the existing EC2 instances.\n다른 EC2 인스턴스를 프록시 서버로 생성합니다 . SSL 인증서를 새 인스턴스로 마이그레이션하고 기존 \nEC2 인스턴스에 직접 연결하도록 구성합니다 .\n​",
      "D": "Import the SSL certificate into AWS Certificate Manager (ACM). Create an Application Load Balancer \nwith an HTTPS listener that uses the SSL certificate from ACM.\n​ SSL 인증서를 AWS Certificate Manager(ACM) 로 가져옵니다 . ACM의 SSL 인증서를 사용하는 HTTPS \n리스너로 Application Load Balancer 를 생성합니다 .\n​\n→ AWS Elastic Load Balancing : SSL 종료 지원\n: Amazon EC2 인스턴스를 사용하여 확장성이 뛰어나고 로드 밸런싱된 웹 사이트를 생성할 수 있으며 \nElastic Load Balancer 에서 처리할 전체 HTTPS 암호화 및 암호 해독 프로세스 (일반적으로 SSL 종료라\n고 함)를 쉽게 조정할 수 있다 . 사용자는 운영 오버헤드나 관리 복잡성이 거의 없이 암호화된 통신의 \n이점을 누릴 수 있다. \n지금까지는 각 EC2 인스턴스 내에서 종료 프로세스를 처리해야 했으므로 인스턴스의 부하가 추가되었\n으며 각 인스턴스에 X.509 인증서를 설치해야 했다.\n이 새 릴리스에서는 인증서를 AWS 계정에 업로드하기만 하면 로드 밸런서에 배포되도록 처리한다 .\n​\n1.인증서를 생성하거나 구매.\n2.AWS Management 콘솔 또는 iam-servercertupload 명령을 사용하여 인증서를 AWS 계정에 \n업로드한 다음 업로드된 인증서의 ID를 검색.\n3.HTTPS 리스너를 포함하는 새 로드 밸런서를 생성하고 이전 단계의 인증서 ID를 제공.\n4.상태 확인을 구성하고 평소와 같이 EC2 인스턴스를 로드 밸런서와 연결."
    },
    "정답": null
  },
  {
    "문제번호": 124,
    "질문": "A company has a highly dynamic batch processing job that uses many Amazon EC2 instances to \ncomplete it. The job is stateless in nature, can be started and stopped at any given time with no \nnegative impact, and typically takes upwards of 60 minutes total to complete. The company has \nasked a solutions architect to design a scalable and cost-effective solution that meets the \nrequirements of the job.\nWhat should the solutions architect recommend?\n​회사에 많은 Amazon EC2 인스턴스를 사용하여 완료하는 매우 동적인 배치 처리 작업이 있습니다 . 작\n업은 본질적으로 상태 비저장이며 부정적인 영향 없이 주어진 시간에 시작 및 중지할 수 있으며 일반\n적으로 완료하는 데 총 60분 이상이 걸립니다 . 회사는 솔루션 설계자에게 작업 요구 사항을 충족하는 \n확장 가능하고 비용 효율적인 솔루션을 설계하도록 요청했습니다 .\n솔루션 설계자는 무엇을 권장해야 합니까 ?\n​",
    "보기": {
      "A": "Implement EC2 Spot Instances.\n​EC2 스팟 인스턴스를 구현합니다 .\n​\n→ EC2 스팟 인스턴스\n: 스팟 인스턴스를 사용하면 AWS 클라우드에서 미사용 EC2 용량을 활용할 수 있다. \n스팟 인스턴스는 온디맨드 요금과 비교하여 최대 90% 할인된 금액으로 제공한다 . \n빅 데이터 , 컨테이너식 워크로드 , CI/CD, 웹 서버, 고성능 컴퓨팅 (HPC), 테스트 및 개발 워크로드 등 다\n양한 상태 비저장 , 내결함성 또는 \n유연한 애플리케이션에 스팟 인스턴스를 사용할 수 있다.\n​\n•1~6시간 이내 짧은 워크로드를 다루거나 , 갑작스런 피크 타임에 해당하는 컴퓨팅 리소스를 확\n보해야 할 때\n•단기적으로 수요가 많을 때",
      "B": "Purchase EC2 Reserved Instances.\n​ EC2 예약 인스턴스 구매.\n​",
      "C": "Implement EC2 On-Demand Instances.\n​EC2 온디맨드 인스턴스를 구현합니다 .\n​",
      "D": "Implement the processing on AWS Lambda.\n​AWS Lambda 에서 처리를 구현합니다 .\n​"
    },
    "정답": null
  },
  {
    "문제번호": 125,
    "질문": "A company runs its two-tier ecommerce website on AWS. The web tier consists of a load balancer \nthat sends traffic to Amazon EC2 instances. The database tier uses an Amazon RDS DB instance. \nThe EC2 instances and the RDS DB instance should not be exposed to the public internet. The \nEC2 instances require internet access to complete payment processing of orders through a third-party \nweb service. The application must be highly available.\nWhich combination of configuration options will meet these requirements? (Choose two.)\n회사는 AWS에서 2계층 전자상거래 웹사이트를 운영합니다 . 웹 계층은 트래픽을 Amazon EC2 인스턴\n스로 보내는 로드 밸런서로 구성됩니다 . 데이터베이스 계층은 Amazon RDS DB 인스턴스를 사용합니\n다. EC2 인스턴스 및 RDS DB 인스턴스는 공용 인터넷에 노출되어서는 안 됩니다 .EC2 인스턴스는 타\n사 웹 서비스를 통한 주문 결제 처리를 완료하기 위해 인터넷 액세스가 필요합니다 . 애플리케이션은 고\n가용성 이어야 합니다 .\n이러한 요구 사항을 충족하는 구성 옵션의 조합은 무엇입니까 ? (2개를 선택하세요 .)\n​",
    "보기": {
      "A": "Use an Auto Scaling group to launch the EC2 instances in private subnets. Deploy an RDS Multi-AZ \nDB instance in private subnets.\nAuto Scaling 그룹을 사용하여 프라이빗 서브넷에서 EC2 인스턴스를 시작합니다 . 프라이빗 서브넷에 \nRDS 다중 AZ DB \n인스턴스를 배포합니다 .\n​→ 고가용성 요구사항 적합\n​\nAuto Scaling Group\n: 동일한 사양, 환경 등(동일한 AMI)을 가지고 있는 EC2 인스턴스들의 그룹입 . Auto Scaling 은 인스턴\n스의 수를 \n트래픽에 따라 자동으로 조절함 . \n​\nE.\nConfigure a VPC with two public subnets, two private subnets, and two NAT gateways across two \nAvailability Zones. Deploy an Application Load Balancer in the public subnets.\n​2개의 가용 영역에 걸쳐 2개의 퍼블릭 서브넷 , 2개의 프라이빗 서브넷 및 2개의 NAT 게이트웨이로 \nVPC를 구성합니다 . 퍼블릭 서브넷에 Application Load Balancer 를 배포합니다 .\n→ 네트워크 분할 요청 요구사항 적합\n​\n인터넷 연결 로드 밸런서에 프라이빗 서브넷의 백엔드 Amazon EC2 인스턴스 연결할 때, ELB를 사용\n하여 수행\n: 프라이빗 서브넷에 있는 Amazon EC2 인스턴스를 연결하려면 백엔드 인스턴스에서 사용하는 프라이\n빗 서브넷과 동일한 가용 영역에\n퍼블릭 서브넷을 생성한다 . 그 후 퍼블릿 서브넷을 로드 밸런서와 연결한다 .",
      "B": "Configure a VPC with two private subnets and two NAT gateways across two Availability Zones. \nDeploy an Application Load Balancer in the private subnets.\n​2개의 가용 영역에 걸쳐 2개의 프라이빗 서브넷과 2개의 NAT 게이트웨이가 있는 VPC를 구성합니다 . \n프라이빗 서브넷에 Application Load Balancer 를 배포합니다 .\n​",
      "C": "Use an Auto Scaling group to launch the EC2 instances in public subnets across two Availability \nZones. Deploy an RDS Multi-AZ DB instance in private subnets.\n​Auto Scaling 그룹을 사용하여 2개의 가용 영역에 걸쳐 퍼블릭 서브넷에서 EC2 인스턴스를 시작합니\n다. 프라이빗 서브넷에 RDS 다중 AZ DB 인스턴스를 배포합니다 .\n​",
      "D": "Configure a VPC with one public subnet, one private subnet, and two NAT gateways across two \nAvailability Zones. Deploy an Application Load Balancer in the public subnet.\n2개의 가용 영역에 걸쳐 1개의 퍼블릭 서브넷 , 1개의 프라이빗 서브넷 및 2개의 NAT 게이트웨이로 \nVPC를 구성합니다 . 퍼블릭 서브넷에 Application Load Balancer 를 배포합니다 .\n​\nE.\nConfigure a VPC with two public subnets, two private subnets, and two NAT gateways across two \nAvailability Zones. Deploy an Application Load Balancer in the public subnets.\n​2개의 가용 영역에 걸쳐 2개의 퍼블릭 서브넷 , 2개의 프라이빗 서브넷 및 2개의 NAT 게이트웨이로 \nVPC를 구성합니다 . 퍼블릭 서브넷에 Application Load Balancer 를 배포합니다 .\n​"
    },
    "정답": null
  },
  {
    "문제번호": 126,
    "질문": "A solutions architect needs to implement a solution to reduce a company's storage costs. All the \ncompany's data is in the Amazon S3 Standard storage class. The company must keep all data for \nat least 25 years. Data from the most recent 2 years must be highly available and immediately \nretrievable.\nWhich solution will meet these requirements?\n​솔루션 설계자는 회사의 스토리지 비용을 줄이기 위한 솔루션을 구현해야 합니다 . 회사의 모든 데이터\n는 Amazon S3 Standard 스토리지 클래스에 있습니다 . 회사는 모든 데이터를 최소 25년 동안 보관해야 \n합니다 . 최근 2년 동안의 데이터는 가용성이 높고 즉시 검색할 수 있어야 합니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n​",
    "보기": {
      "A": "Set up an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive immediately.\n​객체를 즉시 S3 Glacier Deep Archive 로 전환하도록 S3 수명 주기 정책을 설정합니다 .\n​",
      "B": "Set up an S3 Lifecycle policy to transition objects to S3 Glacier Deep Archive after 2 years.\n​2년 후에 객체를 S3 Glacier Deep Archive 로 전환하도록 S3 수명 주기 정책을 설정합니다 .\n​\n▶오답\n​S3 Intelligent-Tiering\n: 성능 저하 또는 운영 오버헤드 없이 가장 경제적인 액세스 계층으로 데이터를 자동으로 이동함으로써 \n스토리지 비용을 최적화하도록 \n설계된 스토리지 클래스 . \nS3 Intelligent-Tiering 은 액세스 패턴이 변경될 때 액세스 계층 간에 세분화된 객체 수준에서 데이터를 \n이동함으로써 비용을 자동으로 절감한다 . \n이는 알 수 없거나 예측할 수 없는 액세스 패턴이 있는 데이터에 대한 스토리지 비용을 최적화하려는 \n경우 완벽한 스토리지 클래스이다 .",
      "C": "Use S3 Intelligent-Tiering. Activate the archiving option to ensure that data is archived in S3 Glacier \nDeep Archive.\n​S3 Intelligent-Tiering 을 사용합니다 . 데이터가 S3 Glacier Deep Archive 에 보관되도록 보관 옵션을 활성\n화합니다 .\n​",
      "D": "Set up an S3 Lifecycle policy to transition objects to S3 One Zone-Infrequent Access (S3 One \nZone-IA) immediately and to S3 Glacier Deep Archive after 2 years.\n​S3 수명 주기 정책을 설정하여 객체를 S3 One Zone-Infrequent Access(S3 One Zone-IA) 로 즉시 전환\n하고 2년 후에는 S3 Glacier Deep Archive 로 전환합니다 .\n​"
    },
    "정답": null
  },
  {
    "문제번호": 127,
    "질문": "A media company is evaluating the possibility of moving its systems to the AWS Cloud. The \ncompany needs at least 10 TB of storage with the maximum possible I/O performance for video \nprocessing, 300 TB of very durable storage for storing media content, and 900 TB of storage to \nmeet requirements for archival media that is not in use anymore.\nWhich set of services should a solutions architect recommend to meet these requirements?\n한 미디어 회사가 시스템을 AWS 클라우드로 이전할 가능성을 평가하고 있습니다 . 회사는 비디오 처리\n를 위한 가능한 최대 I/O 성능을 갖춘 최소 10TB의 스토리지 , 미디어 콘텐츠를 저장하기 위한 300TB\n의 매우 내구성 있는 스토리지 , 더 이상 사용하지 않는 아카이브 미디어에 대한 요구 사항을 충족하기 \n위해 900TB 의 스토리지가 필요합니다 .\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 어떤 서비스 세트를 권장해야 합니까 ?\n​",
    "보기": {
      "A": "Amazon EBS for maximum performance, Amazon S3 for durable data storage, and Amazon S3 \nGlacier for archival storage\n​최고의 성능을 위한 Amazon EBS, 내구성 있는 데이터 스토리지를 위한 Amazon S3, 아카이브 스토리\n지를 위한 Amazon S3 Glacier\n​",
      "B": "Amazon EBS for maximum performance, Amazon EFS for durable data storage, and Amazon S3 \nGlacier for archival storage\n​최고의 성능을 위한 Amazon EBS, 내구성 있는 데이터 스토리지를 위한 Amazon EFS, 아카이브 스토\n리지를 위한 Amazon S3 Glacier\n​",
      "C": "Amazon EC2 instance store for maximum performance, Amazon EFS for durable data storage, and \nAmazon S3 for archival storage\n​최고의 성능을 위한 Amazon EC2 인스턴스 스토어 , 내구성 있는 데이터 스토리지를 위한 Amazon \nEFS, 아카이브 스토리지를 위한 Amazon S3\n​",
      "D": "Amazon EC2 instance store for maximum performance, Amazon S3 for durable data storage, and \nAmazon S3 Glacier for archival storage\n​최고의 성능을 위한 Amazon EC2 인스턴스 스토어 , 내구성 있는 데이터 스토리지를 위한 Amazon S3, \n아카이브 스토리지를 위한 Amazon S3 Glacier\n​\nAmazon EC2 인스턴스 스토어\n: 인스턴스 스토어는 인스턴스에 블록 수준의 임시 스토리지를 제공한다 . 스토리지는 호스트 컴퓨터에 \n물리적으로 연결된 디스크에 위치한다 . 인스턴스 스토어는 버퍼, 캐시, scratch 데이터 및 기타 임시 콘\n텐츠와 같이 자주 변경되는 정보의 임시 스토리지나 로드가 분산된 \n웹 서버 풀과 같은 여러 인스턴스상에서 복제되는 데이터에 가장 적합하다 .\n일부 인스턴스 유형은 NVMe 또는 SATA 기반 SSD(Solid State Drive) 를 사용하여 높은 랜덤 I/O 성능\n을 제공한다 .\n인스턴스 스토어의 경우 루트 볼륨 최대 크기는 10GB 이지만 인스턴스 스토어는 최대 30TB까지 추가\n할 수 있다.\n​"
    },
    "정답": null
  },
  {
    "문제번호": 128,
    "질문": "A company wants to run applications in containers in the AWS Cloud. These applications are \nstateless and can tolerate disruptions within the underlying infrastructure. The company needs a \nsolution that minimizes cost and operational overhead.\nWhat should a solutions architect do to meet these requirements?\n​회사에서 AWS 클라우드의 컨테이너에서 애플리케이션을 실행하려고 합니다 . 이러한 애플리케이션은 \n상태 비저장이며 기본 인프라 내에서 중단을 허용할 수 있습니다 . 회사는 비용과 운영 오버헤드를 최소\n화하는 솔루션이 필요합니다 .\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Amazon EC2 인스턴스 스토어 Amazon EBS\n인스턴스와 동일한 물리서버에 존재 네트워크로 연결\n속도 빠름 인스턴스 스토어어 비해 속도 느림\n휘발성EC2인스턴스용 영구 스토리지\n데이터 유지\n비휘발성\n무료 유료\nUse Spot Instances in an Amazon EC2 Auto Scaling group to run the application containers.\n​Amazon EC2 Auto Scaling 그룹의 스팟 인스턴스를 사용하여 애플리케이션 컨테이너를 실행합니다 .\n​",
      "B": "Use Spot Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node group.\n​Amazon Elastic Kubernetes Service(Amazon EKS) 관리형 노드 그룹에서 스팟 인스턴스를 사용합니다 .\n​\nAmazon EKS, 관리형 노드 그룹에서 EC2 스팟 인스턴스에 대한 지원 추가\n: Kubernetes 클러스터에서 실행되는 중단 가능한 워크로드에 대해 스팟 인스턴스가 제공하는 급격한 \n비용 절감 및 확장을 활용할 수 있다.\n새로운 관리형 노드 그룹을 생성할 때 여러 인스턴스 유형을 제공할 수 있으므로 스팟에서 실행되는 \n애플리케이션의 가용성을 더욱 높일 수 있다. \n​\n관리형 노드 그룹을 생성할 때 용량 유형을 SPOT 으로 설정 하고 리소스 요구 사항을 충족하는 여러 \nEC2 인스턴스 유형을 설정하기만 하면 된다. \n관리형 노드 그룹은 최신 스팟 모범 사례를 기반으로 스팟 노드를 프로비저닝하고 관리한다 . 특히, 관\n리하는 모든 Amazon EC2 Auto Scaling 그룹에서 \n용량 최적화 할당 전략 및 용량 재조정을 활성화 하여 노드 그룹의 가용성을 향상시킵니다 .\n​\n​Amazon Elastic Kubernetes Service(Amazon EKS) \n: AWS에서 Kubernetes 를 쉽게 실행할 수 있는 관리형 서비스 . \n​\n스팟 인스턴스\n: 온디맨드 가격에서 대폭 할인되는 예비 EC2 용량이다 . 스팟 인스턴스는 스케줄러가 중단을 정상적으\n로 관리하기 위한 기본 제공 메커니즘을 제공하기 \n때문에 배치 처리, Apache Spark 를 사용하는 빅 데이터 ETL, 상태 비저장 API 엔드포인트와 같은 \nKubernetes 클러스터에서 내결함성 애플리케이션을 \n실행하는 데 매우 적합하다 .",
      "C": "Use On-Demand Instances in an Amazon EC2 Auto Scaling group to run the application containers.\n​Amazon EC2 Auto Scaling 그룹의 온디맨드 인스턴스를 사용하여 애플리케이션 컨테이너를 실행합니\n다.\n​",
      "D": "Use On-Demand Instances in an Amazon Elastic Kubernetes Service (Amazon EKS) managed node \ngroup.\n​Amazon Elastic Kubernetes Service(Amazon EKS) 관리형 노드 그룹에서 온디맨드 인스턴스를 사용합\n니다.\n​"
    },
    "정답": null
  },
  {
    "문제번호": 129,
    "질문": "​A company is running a multi-tier web application on premises. The web application is containerized \nand runs on a number of Linux hosts connected to a PostgreSQL database that contains user \nrecords. The operational overhead of maintaining the infrastructure and capacity planning is limiting \nthe company's growth. A solutions architect must improve the application's infrastructure.\nWhich combination of actions should the solutions architect take to accomplish this? (Choose two.)\n회사에서 온프레미스에서 다중 계층 웹 애플리케이션을 실행하고 있습니다 . 웹 애플리케이션은 컨테이\n너화되어 있으며 사용자 레코드가 포함된 PostgreSQL 데이터베이스에 연결된 여러 Linux 호스트에서 \n실행됩니다 . 인프라 및 용량 계획을 유지 관리하는 운영 오버헤드가 회사의 성장을 제한하고 있습니다 . \n솔루션 설계자는 애플리케이션의 인프라를 개선해야 합니다 .\n솔루션 설계자는 이를 달성하기 위해 어떤 조합의 조치를 취해야 합니까 ? (2개를 선택하세요 .)\n​",
    "보기": {
      "A": "Migrate the PostgreSQL database to Amazon Aurora.\n​PostgreSQL 데이터베이스를 Amazon Aurora 로 마이그레이션합니다 .\n→ Amazon Aurora 는 PostgreSQL 데이터베이스 지원.\n​\nE.\nMigrate the web application to be hosted on AWS Fargate with Amazon Elastic Container Service \n(Amazon ECS).\nAmazon Elastic Container Service(Amazon ECS)를 사용하여 AWS Fargate 에서 호스팅할 웹 애플리케\n이션을 마이그레이션합니다 .\n​→ 기존 Web App이 이미 컨테이너 온프레미스에서 실행되고 있기 때문에 ECS의 서버리스 Fargate 와 \n같은 클라우드 컨테이너 svc로\n마이그레이션 하는것이 논리적이다 .\n​\n※ Amazon Aurora / AWS Fargate = 서버리스 서비스 = 운영 오버헤드 적음 = 인프라 관리 최적화\n​\n▶오답",
      "B": "Migrate the web application to be hosted on Amazon EC2 instances.\n​Amazon EC2 인스턴스에서 호스팅할 웹 애플리케이션을 마이그레이션합니다 .\n​",
      "C": "Set up an Amazon CloudFront distribution for the web application content.\n​웹 애플리케이션 콘텐츠에 대한 Amazon CloudFront 배포를 설정합니다 .\n​",
      "D": "Set up Amazon ElastiCache between the web application and the PostgreSQL database.\n웹 애플리케이션과 PostgreSQL 데이터베이스 간에 Amazon ElastiCache 를 설정합니다 .\n​→ 새로운 Amazon ElastiCache svc를 처리하는 데 더 많은 작업비용이 소요됨 ."
    },
    "정답": null
  },
  {
    "문제번호": 130,
    "질문": "An application runs on Amazon EC2 instances across multiple Availability Zonas. The instances run \nin an Amazon EC2 Auto Scaling group behind an Application Load Balancer. The application \nperforms best when the CPU utilization of the EC2 instances is at or near 40%.\nWhat should a solutions architect do to maintain the desired performance across all instances in the \ngroup?\n​애플리케이션은 여러 가용 영역의 Amazon EC2 인스턴스에서 실행됩니다 . 인스턴스는 Application \nLoad Balancer 뒤의 Amazon EC2 Auto Scaling 그룹에서 실행됩니다 . 애플리케이션은 EC2 인스턴스의 \nCPU 사용률 이 40% 또는 거의 40%일 때 가장 잘 수행됩니다 .\n솔루션 설계자는 그룹의 모든 인스턴스에서 원하는 성능을 유지하기 위해 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Use a simple scaling policy to dynamically scale the Auto Scaling group.\n​Auto Scaling 그룹을 동적으로 확장하려면 간단한 확장 정책을 사용합니다 .\n​",
      "B": "Use a target tracking policy to dynamically scale the Auto Scaling group.\n​대상 추적 정책을 사용하여 Auto Scaling 그룹을 동적으로 확장합니다 .\n​\n→ Amazon EC2 Auto Scaling 의 대상 추적 조정 정책\n: 대상 추적 조정 정책을 생성하려면 Amazon CloudWatch 지표와 애플리케이션의 이상적인 평균 사용\n률 또는 \n처리량 (throughput) 수준을 나타내는 목표 값을 지정한다 . Amazon EC2 Auto Scaling 은 그룹을 스케일 \n아웃하여 (인스턴스 추가) \n피크 트래픽을 처리하고 그룹을 스케일 인하여 (더 적은 수의 인스턴스 실행) 사용률 또는 처리량\n(throughput) 이 낮은 기간에 비용을 \n절감할 수 있다.\n​\n예를 들어, 현재 인스턴스 2개에서 애플리케이션이 실행되고 있고 사용자가 애플리케이션 로드에 변경\n이 있는 경우 오토 스케일링의 CPU 사용량을 50% 정도로 유지시키려 한다고 가정할때 , 이로 인해 과\n도한 유휴 리소스를 유지하지 않고도 트래픽 급증을 처리할 수 있는 추가 용량을 확보할 수 있다.\n​\n평균 CPU 사용률 50%를 목표로 하는 대상 추적 조정 정책을 생성하면 이러한 요구 사항을 충족할 수 \n있다. \n그러면 오토 스케일링 그룹이 인스턴스 수를 조정하여 실제 지표 값을 50%로 또는 50%에 가깝게 유\n지한다 .",
      "C": "Use an AWS Lambda function ta update the desired Auto Scaling group capacity.\n​AWS Lambda 함수를 사용하여 원하는 Auto Scaling 그룹 용량을 업데이트합니다 .\n​",
      "D": "Use scheduled scaling actions to scale up and scale down the Auto Scaling group.\n​예약된 조정 작업을 사용하여 Auto Scaling 그룹을 확장 및 축소합니다 .\n​"
    },
    "정답": null
  },
  {
    "문제번호": 131,
    "질문": "A company is developing a file-sharing application that will use an Amazon S3 bucket for storage. \nThe company wants to serve all the files through an Amazon CloudFront distribution. The company \ndoes not want the files to be accessible through direct navigation to the S3 URL.\nWhat should a solutions architect do to meet these requirements?\n​한 회사에서 Amazon S3 버킷을 스토리지로 사용할 파일 공유 애플리케이션을 개발 중입니다 . 회사는 \nAmazon CloudFront 배포를 통해 모든 파일을 제공하려고 합니다 . 회사는 S3 URL에 대한 직접 탐색을 \n통해 파일에 액세스하는 것을 원하지 \n않습니다 .솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Write individual policies for each S3 bucket to grant read permission for only CloudFront access.\n​각 S3 버킷에 대한 개별 정책을 작성하여 CloudFront 액세스에 대해서만 읽기 권한을 부여합니다 .\n​",
      "B": "Create an IAM user. Grant the user read permission to objects in the S3 bucket. Assign the user to \nCloudFront.\n​ IAM 사용자를 생성합니다 . 사용자에게 S3 버킷의 객체에 대한 읽기 권한을 부여합니다 . 사용자를 \nCloudFront 에 할당합니다 .\n​",
      "C": "Write an S3 bucket policy that assigns the CloudFront distribution ID as the Principal and assigns \nthe target S3 bucket as the Amazon Resource Name (ARN).\n​CloudFront 배포 ID를 보안 주체로 할당하고 대상 S3 버킷을 Amazon 리소스 이름(ARN) 으로 할당하는 \nS3 버킷 정책을 \n작성합니다 .\n​",
      "D": "Create an origin access identity (OAI). Assign the OAI to the CloudFront distribution. Configure the \nS3 bucket permissions so that only the OAI has read permission.\n​원본 액세스 ID(OAI) 를 생성합니다 . CloudFront 배포에 OAI를 할당합니다 . OAI만 읽기 권한을 갖도록 \nS3 버킷 권한을 \n구성합니다 .\n​\n→ CloudFront 배포를 사용하여 Amazon S3 버킷에 대한 액세스를 제한하는 방법\n: CloudFront 원본 액세스 ID(OAI) 생성"
    },
    "정답": null
  },
  {
    "문제번호": 132,
    "질문": "A company’s website provides users with downloadable historical performance reports. The website \nneeds a solution that will scale to meet the company’s website demands globally. The solution \nshould be cost-effective, limit the provisioning of infrastructure resources, and provide the fastest \npossible response time.\nWhich combination should a solutions architect recommend to meet these requirements?\n​회사의 웹사이트는 사용자에게 다운로드 가능한 과거 성과 보고서를 제공합니다 . 웹 사이트에는 전 세\n계적으로 회사의 웹 사이트 요구 사항을 충족하도록 확장할 수 있는 솔루션이 필요합니다 . 솔루션은 비\n용 효율적이어야 하고 인프라 리소스 프로비저닝을 제한하며 가능한 가장 빠른 응답 시간을 제공해야 \n합니다 .\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 어떤 조합을 권장해야 합니까 ?\n​",
    "보기": {
      "A": "Amazon CloudFront and Amazon S3\n​Amazon CloudFront 및 Amazon S3\n​",
      "B": "AWS Lambda and Amazon DynamoDB\n​AWS Lambda 및 Amazon DynamoDB\n​",
      "C": "Application Load Balancer with Amazon EC2 Auto Scaling\n​Amazon EC2 Auto Scaling 이 있는 Application Load Balancer\n​",
      "D": "Amazon Route 53 with internal Application Load Balancers\n​내부 Application Load Balancer 가 있는 Amazon Route 53\n​"
    },
    "정답": null
  },
  {
    "문제번호": 133,
    "질문": "A company runs an Oracle database on premises. As part of the company’s migration to AWS, the \ncompany wants to upgrade the database to the most recent available version. The company also \nwants to set up disaster recovery (DR) for the database. The company needs to minimize the \noperational overhead for normal operations and DR setup. The company also needs to maintain \naccess to the database's underlying operating system.\nWhich solution will meet these requirements?\n회사는 온프레미스에서 Oracle 데이터베이스를 실행합니다 . 회사는 AWS로 마이그레이션하는 과정에서 \n데이터베이스를 사용 가능한 최신 버전으로 업그레이드하려고 합니다 . 회사는 또한 데이터베이스에 대\n한 재해 복구(DR)를 설정하려고 합니다 . 회사는 정상 운영 및 DR 설정을 위한 운영 오버헤드를 최소화\n해야 합니다 . 회사는 또한 데이터베이스의 기본 운영 체제에 대한 액세스를 유지 관리해야 합니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n​",
    "보기": {
      "A": "Migrate the Oracle database to an Amazon EC2 instance. Set up database replication to a different \nAWS Region.\n​Oracle 데이터베이스를 Amazon EC2 인스턴스로 마이그레이션합니다 . 다른 AWS 리전으로 데이터베이\n스 복제를 설정합니다 .\n​",
      "B": "Migrate the Oracle database to Amazon RDS for Oracle. Activate Cross-Region automated backups \nto replicate the snapshots to another AWS Region.\n​Oracle 데이터베이스를 Oracle 용 Amazon RDS로 마이그레이션합니다 . 교차 리전 자동 백업을 활성화\n하여 다른 AWS 리전에 스냅샷을 복제합니다 .\n​",
      "C": "Migrate the Oracle database to Amazon RDS Custom for Oracle. Create a read replica for the \ndatabase in another AWS Region.\n​Oracle 데이터베이스를 Oracle 용 Amazon RDS Custom 으로 마이그레이션합니다 . 다른 AWS 리전의 데\n이터베이스에 대한 읽기 전용 복제본을 생성합니다 .\n​\nAmazon RDS Custom\n→ 데이터베이스 관리 작업 및 운영을 자동화합니다 . RDS Custom 은 데이터베이스 관리자가 데이터베\n이스 \n환경 및 운영 체제에 액세스하고 사용자 지정할 수 있도록 한다. \nRDS Custom 을 사용하면 레거시 , 커스텀 및 패키지 애플리케이션의 요구 사항에 맞게 커스터마이징할 \n수 있다.\n​\n▶오답",
      "D": "Migrate the Oracle database to Amazon RDS for Oracle. Create a standby database in another \nAvailability Zone.\nOracle 데이터베이스를 Oracle 용 Amazon RDS로 마이그레이션합니다 . 다른 가용 영역에 대기 데이터\n베이스를 생성합니다 .\n→ RDS Oracle 은 OS(기본운영체제 )에 대한 엑세스를 허용하지 않는다 . 대기 데이터베이스는 DR(재해\n복구)이 아닌 \nHA(고가용성 )에 사용된다 ."
    },
    "정답": null
  },
  {
    "문제번호": 134,
    "질문": "A company wants to move its application to a serverless solution. The serverless solution needs to \nanalyze existing and new data by using SQL. The company stores the data in an Amazon S3 Amazon RDS Amazon RDS CustomAmazon EC2 상용 데이\n터베이스\n전체 데이터베이스 및 운영체제\n를\nAWS로 완전 관리해야 하는 경\n우종속 애플리케이션을 사용할 수 \n있도록 \n데이터 베이스 및 기본 운영 체\n제에 대한\n관리 권한이 필요한 경우완전한 관리 책임을 원하고 관\n리형 컴퓨팅\n서비스가 필요한 경우\nbucket. The data requires encryption and must be replicated to a different AWS Region.\nWhich solution will meet these requirements with the LEAST operational overhead?\n한 회사에서 애플리케이션을 서버리스 솔루션으로 이동하려고 합니다 . 서버리스 솔루션은 SQL을 사용\n하여 기존 및 신규 데이터를 분석해야 합니다 . 회사는 데이터를 Amazon S3 버킷에 저장합니다 . 데이\n터는 암호화가 필요하며 다른 AWS 리전에 복제해야 합니다 .\n최소한의 운영 오버헤드 로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까 ?\n​",
    "보기": {
      "A": "Create a new S3 bucket. Load the data into the new S3 bucket. Use S3 Cross-Region Replication \n(CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption \nwith AWS KMS multi-Region kays (SSE-KMS). Use Amazon Athena to query the data.\n​새 S3 버킷을 생성합니다 . 데이터를 새 S3 버킷에 로드합니다 . S3 교차 리전 복제(CRR) 를 사용하여 \n암호화된 객체를 다른 리전의 S3 버킷에 복제합니다 . AWS KMS 다중 리전 kay(SSE-KMS) 로 서버 측 \n암호화를 사용합니다 . \nAmazon Athena 를 사용하여 데이터를 쿼리합니다 .\n​",
      "B": "Create a new S3 bucket. Load the data into the new S3 bucket. Use S3 Cross-Region Replication SSE-S3 SSE-KMS SSE-C\nAmazon S3 관리형 키를 사용\n한 \n서버측 암호화AWS Key Management \nService 에 저장된 KMS 키를 사\n용한 서버 측 암호화고객 제공 키를 사용한 서버 측 \n암호화\n각 객체는 고유한 키로 암호화\n된다. 또한 추가 보안 조치로 \n주기적으로 교체되는 루트 키를 \n사용하여 키 자체를 암호화한\n다. Amazon S3 서버 측 암호\n화는 \n가장 강력한 블록 암호 중 하나\n인 256비트 Advanced \nEncryption Standard(AES-256)\n를 사용하여 데이터를 암호화한\n다.​\nAmazon S3의 객체에 대한 무\n단 액세스에 대응하여 추가적인 \n보호를 제공하는 -KMS 키를 사\n용하려면 별도의 권한이 필요하\n다. SSE-KMS 도 KMS 키가 사\n용된 때와 사용 주체를 표시하\n는 감사 추적 기능을 제공한다 . \n또한 고객 관리형 키를 생성하\n고 관리하거나 사용자 , 서비스 \n및 리전에 고유한 AWS 관리형 \nCMK를 사용할 수 있다.사용자는 암호화 키를 관리하고 \nAmazon S3는 암호화 (디스크에 \n쓸 때) 및 해독(객체에 액세스\n할 때)을 관리한다 .\n(CRR) to replicate encrypted objects to an S3 bucket in another Region. Use server-side encryption \nwith AWS KMS multi-Region keys (SSE-KMS). Use Amazon RDS to query the data.\n​새 S3 버킷을 생성합니다 . 데이터를 새 S3 버킷에 로드합니다 . S3 교차 리전 복제(CRR) 를 사용하여 \n암호화된 객체를 다른 리전의 S3 버킷에 복제합니다 . AWS KMS 다중 리전 키(SSE-KMS) 로 서버 측 \n암호화를 사용합니다 . Amazon RDS를 사용하여 데이터를 쿼리합니다 .\n→ 기존 S3 버킷이 존재함으로 새로운 S3 버킷을 생성할 필요가 없다. \n여러개의 버킷 관리 = 운영 오버헤드 상승\n​",
      "C": "Load the data into the existing S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate \nencrypted objects to an S3 bucket in another Region. Use server-side encryption with Amazon S3 \nmanaged encryption keys (SSE-S3). Use Amazon Athena to query the data.\n​기존 S3 버킷에 데이터를 로드합니다 . S3 교차 리전 복제(CRR) 를 사용하여 암호화된 객체를 다른 리\n전의 S3 버킷에 복제합니다 . Amazon S3 관리형 암호화 키(SSE-S3) 로 서버 측 암호화를 사용합니다 . \nAmazon Athena 를 사용하여 데이터를 쿼리합니다 .\n​→ 기본의 버킷을 유지하며 , Amazon Athena 사용 (서버리스 ) = 운영 오버헤드 최소화\n​\n​\n▶오답",
      "D": "Load the data into the existing S3 bucket. Use S3 Cross-Region Replication (CRR) to replicate \nencrypted objects to an S3 bucket in another Region. Use server-side encryption with Amazon S3 \nmanaged encryption keys (SSE-S3). Use Amazon RDS to query the data.\n​기존 S3 버킷에 데이터를 로드합니다 . S3 교차 리전 복제(CRR) 를 사용하여 암호화된 객체를 다른 리\n전의 S3 버킷에 복제합니다 . Amazon S3 관리형 암호화 키(SSE-S3) 로 서버 측 암호화를 사용합니다 . \nAmazon RDS를 사용하여 데이터를 쿼리합니다 .\n→ Amazon RDS는 관계형 DB"
    },
    "정답": null
  },
  {
    "문제번호": 135,
    "질문": "A company runs workloads on AWS. The company needs to connect to a service from an external \nprovider. The service is hosted in the provider's VPC. According to the company’s security team, the \nconnectivity must be private and must be restricted to the target service. The connection must be \ninitiated only from the company’s VPC.\nWhich solution will mast these requirements?\n​회사는 AWS에서 워크로드를 실행합니다 . 회사는 외부 공급자의 서비스에 연결해야 합니다 . 서비스는 \n공급자의 VPC에서 호스팅 됩니다 . 회사 보안 팀에 따르면 연결은 비공개여야 하며 대상 서비스로 제한\n되어야 합니다 . 연결은 회사의 VPC에서만 시작되어야 합니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n​",
    "보기": {
      "C": "Create a NAT gateway in a public subnet of the company’s VPUpdate the route table to connect to \nthe target service.\n​회사의 VPUpdate 라우팅 테이블의 퍼블릭 서브넷에 NAT 게이트웨이를 생성하여 대상 서비스에 연결\n합니다 .\n​",
      "A": "Create a VPC peering connection between the company's VPC and the provider's VPC. Update the \nroute table to connect to the target service.\n회사의 VPC와 공급자의 VPC 간에 VPC 피어링 연결을 생성합니다 . 대상 서비스에 연결하도록 라우팅 \n테이블을 \n업데이트합니다 .\n​",
      "B": "Ask the provider to create a virtual private gateway in its VPC. Use AWS PrivateLink to connect to \nthe target service.\n​공급자에게 VPC에 가상 프라이빗 게이트웨이를 생성하도록 요청합니다 . AWS PrivateLink 를 사용하여 \n대상 서비스에 \n연결합니다 .\n​",
      "D": "Ask the provider to create a VPC endpoint for the target service. Use AWS PrivateLink to connect to \nthe target service.\n​공급자에게 대상 서비스에 대한 VPC 엔드포인트를 생성하도록 요청합니다 . AWS PrivateLink 를 사용하\n여 대상 서비스에 \n연결합니다 .\n​→ AWS PrivateLink \n: 트래픽을 공용 인터넷에 노출하지 않고 VPC, AWS 서비스 및 사내 네트워크 간의 개인 연결을 제공\n한다.\n다양한 계정 및 VPC에서 서비스를 쉽게 연결하여 네트워크 아키텍처를 크게 간소화할 수 있다.\nAWS PrivateLink 에서 제공하는 인터페이스 VPC 엔드포인트는 AWS Partners 에서 호스틍하는 서비스와 \nAWS Marketplace 에서\n사용할 수 있게 지원도는 솔루션에 연결한다 .\n​\n▶오답\n가상 프라이빗 게이트웨이 (Virtual Private Gateway)\n: VGW는 가상 사설 네트워크인 VPC와 온프레미스 네트워크를 연결하는 방식 중 가장 오래된 방식이\n다.\nVGW는 VPC와 온프레미스 네트워크를 연결하는 VPC 쪽의 라우터 역할을 한다.\nVGW를 이용해 온프레스 네트워크와 연결하기 위해서는 온프레미스 네트워크 쪽에 CGW(Customer \nGateway) 를 구성해 줘야 한다.\nVGW의 특징\n•GW는 여러 개의 온프레미스 라우터와 연결이 가능합니다 .\n•다만 VGW는 하나의 VPC에만 연결할 수 있습니다 .\n•VGW는 Site-to-Site VPN, Direct Connect 연결을 통해 온프레미스 네트워크와 연결이 가능합\n니다.\n•VGW 생성시 동적 라우팅을 사용할 경우 ASN(Autonomous System Number) 를 지정해 줘야합\n니다."
    },
    "정답": null
  },
  {
    "문제번호": 136,
    "질문": "A company is migrating its on-premises PostgreSQL database to Amazon Aurora PostgreSQL. The \non-premises database must remain online and accessible during the migration. The Aurora database \nmust remain synchronized with the on-premises database.\nWhich combination of actions must a solutions architect take to meet these requirements? (Choose \ntwo.)\n​회사는 온프레미스 PostgreSQL 데이터베이스를 Amazon Aurora PostgreSQL 로 마이그레이션 하고 있습\n니다. 온-프레미스 데이터베이스는 온라인 상태를 유지하고 마이그레이션 중에 액세스 할 수 있어야 합\n니다. Aurora 데이터베이스는 온프레미스 데이터베이스와 동기화된 상태를 유지해야 합니다 .\n이러한 요구 사항을 충족하기 위해 솔루션 설계자가 취해야 하는 조치의 조합은 무엇입니까 ? (2개를 \n선택하세요 .)\n​",
    "보기": {
      "A": "Create an ongoing replication task.\n​지속적인 복제 작업을 만듭니다 .\n​",
      "B": "Create a database backup of the on-premises database.\n​온프레미스 데이터베이스의 데이터베이스 백업을 생성합니다 .\n​",
      "C": "Create an AWS Database Migration Service (AWS DMS) replication server.\n​AWS Database Migration Service(AWS DMS) 복제 서버를 생성합니다 .\n​\n→AWS Database Migration Service (AWS DMS)\n: DMS를 사용하면 일회성 마이그레이션을 수행하고 지속적인 변경 사항을 복제하여 소스와 대상을 동\n기화 상태로 유지할 수 있다.\nOracle 에서 Oracle 로의 동종 마이그레이션은 물론 Oracle 또는 Microsoft SQL Server 에서 Amazon \nAurora 로의 서로 다른 데이터베이스 플랫폼 간의 이기종 마이그레이션을 지원한다 . \nAWS Database Migration Service 를 사용하면 지원되는 소스에서 지원되는 대상으로 짧은 지연 시간으\n로 데이터를 지속적으로 복제할 수도 있다. \n예를 들어 여러 소스에서 Amazon Simple Storage Service(Amazon S3)로 복제하여 가용성과 확장성이 \n뛰어난 데이터 레이크 솔루션을 구축할 수 있다. \n​",
      "D": "Convert the database schema by using the AWS Schema Conversion Tool (AWS SCT).\nAWS Schema Conversion Tool(AWS SCT)을 사용하여 데이터베이스 스키마를 변환합니다 .\n​\nE.\nCreate an Amazon EventBridge (Amazon CloudWatch Events) rule to monitor the database \nsynchronization.\n데이터베이스 동기화를 모니터링하는 Amazon EventBridge(Amazon CloudWatch Events) 규칙을 생성\n합니다 .\n​"
    },
    "정답": null
  },
  {
    "문제번호": 137,
    "질문": "A company uses AWS Organizations to create dedicated AWS accounts for each business unit to \nmanage each business unit's account independently upon request. The root email recipient missed a \nnotification that was sent to the root user email address of one account. The company wants to \nensure that all future notifications are not missed. Future notifications must be limited to account \nadministrators.\nWhich solution will meet these requirements?\n​회사는 AWS Organizations 를 사용하여 각 사업부에 대한 전용 AWS 계정을 생성하여 요청 시 각 사업\n부의 계정을 독립적으로 관리합니다 . 루트 이메일 수신자가 한 계정의 루트 사용자 이메일 주소로 전송\n된 알림을 놓쳤습니다 . 회사는 향후 모든 알림을 놓치지 않기를 원합니다 . 향후 알림은 계정 관리자로 \n제한되어야 합니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n​",
    "보기": {
      "A": "Configure the company’s email server to forward notification email messages that are sent to the \nAWS account root user email address to all users in the organization.\n​AWS 계정 루트 사용자 이메일 주소로 전송되는 알림 이메일 메시지를 조직의 모든 사용자에게 전달하\n도록 회사 이메일 서버를 구성합니다 .\n​",
      "B": "Configure all AWS account root user email addresses as distribution lists that go to a few \nadministrators who can respond to alerts. Configure AWS account alternate contacts in the AWS \nOrganizations console or programmatically.\n​모든 AWS 계정 루트 사용자 이메일 주소를 알림에 응답할 수 있는 소수의 관리자에게 전달되는 배포 \n목록으로 구성합니다 . AWS Organizations 콘솔에서 또는 프로그래밍 방식으로 AWS 계정 대체 연락처\n를 구성합니다 .\n​\n​\nAWS Organization 의 관리계정 모범 사례 - 관리 계정의 루트 사용자에게 그룹 이메일 주소 사용\n•회사에서 관리하는 이메일 주소를 사용한다 . 퍼블릭 이메일 제공자 또는 제3자가 관리하는 이\n메일 제공자는 이용하지 않는다 .\n•받은 메시지를 고위 비즈니스 관리자 목록으로 직접 전달하는 이메일 주소를 사용한다 . 예를 \n들어 액세스 확인을 위해 AWS가 계정 소유자에게 연락해야 하는 경우 이메일 메시지를 여러 \n당사자에게 배포한다 . 이러한 방식은 개인이 휴가 중이거나 아프거나 회사를 떠난 경우에도 \n응답이 지연될 위험을 줄이는 데 도움이 된다.",
      "C": "Configure all AWS account root user email messages to be sent to one administrator who is \nresponsible for monitoring alerts and forwarding those alerts to the appropriate groups.\n​경보를 모니터링하고 해당 경보를 적절한 그룹에 전달할 책임이 있는 한 명의 관리자에게 모든 AWS \n계정 루트 사용자 이메일 메시지를 보내도록 구성합니다 .\n​",
      "D": "Configure all existing AWS accounts and all newly created accounts to use the same root user email \naddress. Configure AWS account alternate contacts in the AWS Organizations console or \nprogrammatically.\n​동일한 루트 사용자 이메일 주소를 사용하도록 기존의 모든 AWS 계정과 새로 생성된 모든 계정을 구\n성합니다 . AWS Organizations 콘솔에서 또는 프로그래밍 방식으로 AWS 계정 대체 연락처를 구성합니\n다.\n​"
    },
    "정답": null
  },
  {
    "문제번호": 138,
    "질문": "A company runs its ecommerce application on AWS. Every new order is published as a massage in \na RabbitMQ queue that runs on an Amazon EC2 instance in a single Availability Zone. These \nmessages are processed by a different application that runs on a separate EC2 instance. This \napplication stores the details in a PostgreSQL database on another EC2 instance. All the EC2 \ninstances are in the same Availability Zone.\nThe company needs to redesign its architecture to provide the highest availability with the least \noperational overhead.\nWhat should a solutions architect do to meet these requirements?\n회사는 AWS에서 전자 상거래 애플리케이션을 실행합니다 . 모든 새 주문은 단일 가용 영역의 Amazon \nEC2 인스턴스에서 실행되는 RabbitMQ 대기열에 마사지로 게시됩니다 . 이러한 메시지는 별도의 EC2 \n인스턴스에서 실행되는 다른 애플리케이션에서 처리됩니다 . 이 애플리케이션은 다른 EC2 인스턴스의 \nPostgreSQL 데이터베이스에 세부 정보를 저장합니다 . 모든 EC2 인스턴스는 동일한 가용 영역에 있습\n니다.\n회사는 최소한의 운영 오버헤드 로 최고의 가용성을 제공하도록 아키텍처를 재설계해야 합니다 .\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Migrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ. \nCreate a Multi-AZ Auto Scaling group for EC2 instances that host the application. Create another \nMulti-AZ Auto Scaling group for EC2 instances that host the PostgreSQL database.\n​대기열을 Amazon MQ에서 RabbitMQ 인스턴스의 중복 쌍(활성/대기)으로 마이그레이션합니다 . 애플리\n케이션을 호스팅하는 EC2 인스턴스에 대한 다중 AZ Auto Scaling 그룹을 생성합니다 . PostgreSQL 데\n이터베이스를 호스팅하는 EC2 인스턴스에 대해 다른 다중 AZ Auto Scaling 그룹을 생성합니다 .\n​",
      "B": "Migrate the queue to a redundant pair (active/standby) of RabbitMQ instances on Amazon MQ. \nCreate a Multi-AZ Auto Scaling group for EC2 instances that host the application. Migrate the \ndatabase to run on a Multi-AZ deployment of Amazon RDS for PostgreSQL.\n​대기열을 Amazon MQ에서 RabbitMQ 인스턴스의 중복 쌍(활성/대기)으로 마이그레이션합니다 . 애플리\n케이션을 호스팅하는 EC2 인스턴스에 대한 다중 AZ Auto Scaling 그룹을 생성합니다 . PostgreSQL 용 \nAmazon RDS의 다중 AZ 배포에서 실행하도록 데이터베이스를 마이그레이션합니다 .\n​→ Amazon MQ로 마이그레이션을 하면 대기열 관리에 대한 오버헤드가 줄어들며 , Amazon RDS는 필\n요한 도구 및 소프트웨이를 서비스로\n제공하므로 운영에 미치는 영향이 적다.\n​\n고가용성을 위한 Amazon MQ 활성/대기 브로커 (내결함성 )\n활성/대기 브로커 는 두 개의 서로 다른 가용 영역에 있는 두 개의 브로커가 중복 페어로 구성된다 . \n이러한 브로커는 애플리케이션 및 Amazon EFS와 동기식으로 통신한다 . \nAmazon EFS 스토리지 볼륨은 데이터를 여러 가용 영역(AZ)에 중복 저장하여 최고 수준의 내구성과 \n가용성을 제공하도록 설계되었다 .",
      "C": "Create a Multi-AZ Auto Scaling group for EC2 instances that host the RabbitMQ queue. Create \nanother Multi-AZ Auto Scaling group for EC2 instances that host the application. Migrate the \ndatabase to run on a Multi-AZ deployment of Amazon RDS for PostgreSQL.\nRabbitMQ 대기열을 호스팅하는 EC2 인스턴스용 다중 AZ Auto Scaling 그룹을 생성합니다 . 애플리케이\n션을 호스팅하는 EC2 인스턴스에 대해 다른 다중 AZ Auto Scaling 그룹을 생성합니다 . PostgreSQL 용 \nAmazon RDS의 다중 AZ 배포에서 실행하도록 데이터베이스를 마이그레이션합니다 .\n​",
      "D": "Create a Multi-AZ Auto Scaling group for EC2 instances that host the RabbitMQ queue. Create \nanother Multi-AZ Auto Scaling group for EC2 instances that host the application. Create a third \nMulti-AZ Auto Scaling group for EC2 instances that host the PostgreSQL database\nRabbitMQ 대기열을 호스팅하는 EC2 인스턴스용 다중 AZ Auto Scaling 그룹을 생성합니다 . 애플리케이\n션을 호스팅하는 EC2 인스턴스에 대해 다른 다중 AZ Auto Scaling 그룹을 생성합니다 . PostgreSQL 데\n이터베이스를 호스팅하는 EC2 인스턴스에 대한 세 번째 다중 AZ Auto Scaling 그룹 생성\n​"
    },
    "정답": null
  },
  {
    "문제번호": 139,
    "질문": "A reporting team receives files each day in an Amazon S3 bucket. The reporting team manually \nreviews and copies the files from this initial S3 bucket to an analysis S3 bucket each day at the \nsame time to use with Amazon QuickSight. Additional teams are starting to send more files in larger \nsizes to the initial S3 bucket.\nThe reporting team wants to move the files automatically analysis S3 bucket as the files enter the \ninitial S3 bucket. The reporting team also wants to use AWS Lambda functions to run \npattern-matching code on the copied data. In addition, the reporting team wants to send the data \nfiles to a pipeline in Amazon SageMaker Pipelines.\nWhat should a solutions architect do to meet these requirements with the LEAST operational \noverhead?\n보고 팀은 Amazon S3 버킷에서 매일 파일을 수신합니다 . 보고 팀은 이 초기 S3 버킷의 파일을 수동으\n로 검토하고 Amazon QuickSight 와 함께 사용하기 위해 매일 같은 시간에 분석 S3 버킷으로 복사합니\n다. 추가 팀이 초기 S3 버킷에 더 큰 크기의 더 많은 파일을 보내기 시작했습니다 .\n보고 팀은 파일이 초기 S3 버킷에 들어갈 때 자동으로 분석 S3 버킷을 이동하려고 합니다 . 또한 보고 \n팀은 AWS Lambda 함수를 사용하여 복사된 데이터에서 패턴 일치 코드를 실행하려고 합니다 . 또한 보\n고 팀은 데이터 파일을 Amazon SageMaker Pipelines 의 파이프라인으로 보내려고 합니다 .\n최소한의 운영 오버헤드 로 이러한 요구 사항을 충족하기 위해 솔루션 설계자는 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Create a Lambda function to copy the files to the analysis S3 bucket. Create an S3 event \nnotification for the analysis S3 bucket. Configure Lambda and SageMaker Pipelines as destinations of \nthe event notification. Configure s3:ObjectCreated:Put as the event type.\n​분석 S3 버킷에 파일을 복사하는 Lambda 함수를 생성합니다 . 분석 S3 버킷에 대한 S3 이벤트 알림을 \n생성합니다 . 이벤트 알림의 대상으로 Lambda 및 SageMaker 파이프라인을 구성합니다 . \ns3:ObjectCreated:Put 을 이벤트 유형으로 구성합니다 .\n​",
      "B": "Create a Lambda function to copy the files to the analysis S3 bucket. Configure the analysis S3 \nbucket to send event notifications to Amazon EventBridge (Amazon CloudWatch Events). Configure \nan ObjectCreated rule in EventBridge (CloudWatch Events). Configure Lambda and SageMaker \nPipelines as targets for the rule.\n​ 분석 S3 버킷에 파일을 복사하는 Lambda 함수를 생성합니다 . Amazon EventBridge(Amazon \nCloudWatch Events) 에 이벤트 알림을 보내도록 분석 S3 버킷을 구성합니다 . EventBridge(CloudWatch \n이벤트 )에서 ObjectCreated 규칙을 구성합니다 . 규칙의 대상으로 Lambda 및 SageMaker 파이프라인을 \n구성합니다 .\n​\n→ Lambda 를 사용하여 파일을 다른 버킷에 복사하는 것 보다 S3복제를 이용하여 파일을 복사하는 것\n이 운영 오버헤드를 줄임",
      "C": "Configure S3 replication between the S3 buckets. Create an S3 event notification for the analysis S3 \nbucket. Configure Lambda and SageMaker Pipelines as destinations of the event notification. \nConfigure s3:ObjectCreated:Put as the event type.\n​S3 버킷 간에 S3 복제를 구성합니다 . 분석 S3 버킷에 대한 S3 이벤트 알림을 생성합니다 . 이벤트 알림\n의 대상으로 Lambda 및 SageMaker 파이프라인을 구성합니다 . s3:ObjectCreated:Put 을 이벤트 유형으\n로 구성합니다 .\n​",
      "D": "Configure S3 replication between the S3 buckets. Configure the analysis S3 bucket to send event \nnotifications to Amazon EventBridge (Amazon CloudWatch Events). Configure an ObjectCreated rule \nin EventBridge (CloudWatch Events). Configure Lambda and SageMaker Pipelines as targets for the \nrule.\n​S3 버킷 간에 S3 복제를 구성합니다 . Amazon EventBridge(Amazon CloudWatch Events) 에 이벤트 알\n림을 보내도록 분석 S3 버킷을 구성합니다 . EventBridge(CloudWatch 이벤트 )에서 ObjectCreated 규칙\n을 구성합니다 . 규칙의 대상으로 Lambda 및 SageMaker 파이프라인을 구성합니다 .\n→ 운영 오버헤드가 가장 적음\n​\n▶오답"
    },
    "정답": null
  },
  {
    "문제번호": 140,
    "질문": "A solutions architect needs to help a company optimize the cost of running an application on AWS. \nThe application will use Amazon EC2 instances, AWS Fargate, and AWS Lambda for compute within \nthe architecture.\nThe EC2 instances will run the data ingestion layer of the application. EC2 usage will be sporadic \nand unpredictable. Workloads that run on EC2 instances can be interrupted at any time. The \napplication front end will run on Fargate, and Lambda will serve the API layer. The front-end \nutilization and API layer utilization will be predictable over the course of the next year.\nWhich combination of purchasing options will provide the MOST cost-effective solution for hosting this \napplication? (Choose two.)\n​솔루션 설계자는 회사가 AWS에서 애플리케이션을 실행하는 비용을 최적화할 수 있도록 도와야 합니\n다. 애플리케이션은 아키텍처 내 컴퓨팅을 위해 Amazon EC2 인스턴스 , AWS Fargate 및 AWS \nLambda 를 사용합니다 .\nEC2 인스턴스는 애플리케이션의 데이터 수집 계층을 실행합니다 . EC2 사용은 산발적이고 예측할 수 \n없습니다 .EC2 인스턴스에서 실행되는 워크로드는 언제든지 중단될 수 있습니다 . 애플리케이션 프런트 \n엔드는 Fargate 에서 실행되고 Lambda 는 API 계층을 제공합니다 . 프론트엔드 활용도와 API 계층 활용\n도는 내년에 예측할 수 있습니다 .\n이 애플리케이션을 호스팅하는 데 가장 비용 효율적인 솔루션을 제공하는 구매 옵션 조합은 무엇입니\n까? (2개를 선택하세요 .)\n​",
    "보기": {
      "A": "Use Spot Instances for the data ingestion layer\n​데이터 수집 계층에 스팟 인스턴스 사용\n​",
      "B": "Use On-Demand Instances for the data ingestion layer\n​데이터 수집 계층에 온디맨드 인스턴스 사용\n​",
      "C": "Purchase a 1-year Compute Savings Plan for the front end and API layer.\n​프런트 엔드 및 API 계층에 대한 1년 Compute Savings Plan을 구매합니다 .\n​\n→ Compute Saving Plan\n: Compute Savings Plans는 최고의 유연성을 제공하고 비용을 최대 66%까지 줄이는 데 도움이 된다. \n이 계획은 인스턴스 패밀리 , 크기, AZ, 지역, OS 또는 테넌시에 관계없이 EC2 인스턴스 사용량에 자동\n으로 적용된다 . \nFargate 및 Lambda 사용에도 적용된다 .",
      "D": "Purchase 1-year All Upfront Reserved instances for the data ingestion layer.\n데이터 수집 계층에 대한 1년 전체 선결제 예약 인스턴스를 구매합니다 .\n​\nE.\nPurchase a 1-year EC2 instance Savings Plan for the front end and API layer.\n​프런트 엔드 및 API 계층을 위한 1년 EC2 인스턴스 Savings Plan을 구매합니다 .\n​"
    },
    "정답": null
  },
  {
    "문제번호": 141,
    "질문": "​A company runs a web-based portal that provides users with global breaking news, local alerts, and \nweather updates. The portal delivers each user a personalized view by using mixture of static and \ndynamic content. Content is served over HTTPS through an API server running on an Amazon EC2 \ninstance behind an Application Load Balancer (ALB). The company wants the portal to provide this \ncontent to its users across the world as quickly as possible.\nHow should a solutions architect design the application to ensure the LEAST amount of latency for \nall users?\n한 회사는 사용자에게 글로벌 속보, 지역 경보 및 날씨 업데이트를 제공하는 웹 기반 포털을 운영합니\n다. 포털은 정적 콘텐츠와 동적 콘텐츠를 혼합하여 각 사용자에게 개인화된 보기를 제공합니다 . 콘텐츠\n는 ALB(Application Load Balancer) 뒤의 Amazon EC2 인스턴스에서 실행되는 API 서버를 통해 \nHTTPS 를 통해 제공됩니다 . 회사는 포털이 이 콘텐츠를 가능한 한 빨리 전 세계 사용자에게 제공하기\n를 원합니다 .\n솔루션 설계자는 모든 사용자의 대기 시간을 최소화하도록 애플리케이션을 어떻게 설계해야 합니까 ?\n​",
    "보기": {
      "A": "Deploy the application stack in a single AWS Region. Use Amazon CloudFront to serve all static and \ndynamic content by specifying the ALB as an origin.\n​단일 AWS 리전에 애플리케이션 스택을 배포합니다 . Amazon CloudFront 를 사용하여 ALB를 오리진으\n로 지정하여 모든 정적 및 동적 콘텐츠를 제공합니다 .\n​\nAmazon CloudFront\n: .html, .css, .js 및 이미지 파일과 같은 정적 및 동적 웹 콘텐츠를 사용자에게 빠르게 배포하는 웹 서\n비스이다 .\n배포를 생성할 때 CloudFront 에서 파일에 대한 요청을 보내는 오리진을 지정한다 . \nCloudFront 에서 다양한 종류의 오리진을 사용할 수 있습니다 . 예를 들어 Amazon S3 버킷, MediaStore \n컨테이너 , MediaPackage \n채널, Application Load Balancer 또는 AWS Lambda 함수 URL을 사용할 수 있다.",
      "B": "Deploy the application stack in two AWS Regions. Use an Amazon Route 53 latency routing policy \nto serve all content from the ALB in the closest Region.\n​두 AWS 리전에 애플리케이션 스택을 배포합니다 . Amazon Route 53 지연 시간 라우팅 정책을 사용하\n여 가장 가까운 리전의 ALB에서 모든 콘텐츠를 제공합니다 .\n​",
      "C": "Deploy the application stack in a single AWS Region. Use Amazon CloudFront to serve the static \ncontent. Serve the dynamic content directly from the ALB.\n​단일 AWS 리전에 애플리케이션 스택을 배포합니다 . Amazon CloudFront 를 사용하여 정적 콘텐츠를 제\n공합니다 . ALB에서 직접 동적 콘텐츠를 제공합니다 .\n​",
      "D": "Deploy the application stack in two AWS Regions. Use an Amazon Route 53 geolocation routing \npolicy to serve all content from the ALB in the closest Region.\n​두 AWS 리전에 애플리케이션 스택을 배포합니다 . Amazon Route 53 지리적 위치 라우팅 정책을 사용\n하여 가장 가까운 리전에서 ALB의 모든 콘텐츠를 제공합니다 .\n​"
    },
    "정답": "A"
  },
  {
    "문제번호": 142,
    "질문": "A gaming company is designing a highly available architecture. The application runs on a modified \nLinux kernel and supports only UDP-based traffic. The company needs the front-end tier to provide \nthe best possible user experience. That tier must have low latency, route traffic to the nearest edge \nlocation, and provide static IP addresses for entry into the application endpoints.\nWhat should a solutions architect do to meet these requirements?\n게임 회사는 고가용성 아키텍처를 설계하고 있습니다 . 애플리케이션은 수정된 Linux 커널에서 실행되며 \nUDP 기반 트래픽 만 지원합니다 . 회사는 최상의 사용자 경험을 제공하기 위해 프런트 엔드 계층이 필요\n합니다 . 해당 계층은 대기 시간이 짧고 가장 가까운 엣지 로케이션으로 트래픽을 라우팅하고 애플리케\n이션 엔드포인트에 진입하기 위한 고정 IP 주소를 제공해야 합니다 .\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Configure Amazon Route 53 to forward requests to an Application Load Balancer. Use AWS Lambda \nfor the application in AWS Application Auto Scaling.\n요청을 Application Load Balancer 로 전달하도록 Amazon Route 53을 구성합니다 . AWS Application \nAuto Scaling 의 애플리케이션에 AWS Lambda 를 사용합니다 .\n​",
      "B": "Configure Amazon CloudFront to forward requests to a Network Load Balancer. Use AWS Lambda \nfor the application in an AWS Application Auto Scaling group.\n요청을 Network Load Balancer 로 전달하도록 Amazon CloudFront 를 구성합니다 . AWS Application Auto \nScaling 그룹의 애플리케이션에 AWS Lambda 를 사용합니다 .\n​",
      "C": "Configure AWS Global Accelerator to forward requests to a Network Load Balancer. Use Amazon \nEC2 instances for the application in an EC2 Auto Scaling group.\n요청을 Network Load Balancer 로 전달하도록 AWS Global Accelerator 를 구성합니다 . EC2 Auto \nScaling 그룹의 애플리케이션에 Amazon EC2 인스턴스를 사용합니다 .\n​\n→ AWS Global Accelerator / Amazon CloudFront\nAWS 글로벌 네트워크와 전 세계의 에지 위치를 사용하는 별도의 서비스이다 . \nGlobal Accelerator 는 에지에서 패킷을 하나 이상의 AWS 영역에서 실행되는 애플리케이션에 프록시하\n여 TCP 또는 UDP를 통해 \n광범위한 애플리케이션의 성능을 향상시킨다 . Global Accelerator 는 게임(UDP), IoT(MQTT), Voice over \nIP와 같은 비 HTTP 사용 \n사례뿐만 아니라 특히 정적 IP 주소나 결정론적 빠른 지역 페일오버가 필요한 HTTP 사용 사례에 적합\n하다. \n두 서비스 모두 DDoS 보호를 위해 AWS Shield 와 통합된니다 .",
      "D": "Configure Amazon API Gateway to forward requests to an Application Load Balancer. Use Amazon \nEC2 instances for the application in an EC2 Auto Scaling group.\n요청을 Application Load Balancer 로 전달하도록 Amazon API Gateway 를 구성합니다 . EC2 Auto \nScaling 그룹의 애플리케이션에 Amazon EC2 인스턴스를 사용합니다 .\n​"
    },
    "정답": "C"
  },
  {
    "문제번호": 143,
    "질문": "A company wants to migrate its existing on-premises monolithic application to AWS. The company \nwants to keep as much of the front-end code and the backend code as possible. However, the \ncompany wants to break the application into smaller applications. A different team will manage each \napplication. The company needs a highly scalable solution that minimizes operational overhead.\nWhich solution will meet these requirements?\n회사에서 기존 온프레미스 모놀리식 애플리케이션을 AWS로 마이그레이션하려고 합니다 . 회사는 프론\n트엔드 코드와 백엔드 코드를 최대한 많이 유지하려고 합니다 . 그러나 회사는 응용 프로그램을 더 작은 \n응용 프로그램으로 나누기를 원합니다 . 다른 팀에서 각 애플리케이션을 관리합니다 . 회사는 운영 오버\n헤드를 최소화 하는 확장성이 뛰어난 솔루션이 필요합니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n​",
    "보기": {
      "A": "Host the application on AWS Lambda. Integrate the application with Amazon API Gateway.\nAWS Lambda 에서 애플리케이션을 호스팅합니다 . 애플리케이션을 Amazon API Gateway 와 통합합니다 .\n​",
      "B": "Host the application with AWS Amplify. Connect the application to an Amazon API Gateway API that \nis integrated with AWS Lambda.\nAWS Amplify 를 사용하여 애플리케이션을 호스팅합니다 . AWS Lambda 와 통합된 Amazon API Gateway \nAPI에 애플리케이션을 연결합니다 .\n​",
      "C": "Host the application on Amazon EC2 instances. Set up an Application Load Balancer with EC2 \ninstances in an Auto Scaling group as targets.\nAmazon EC2 인스턴스에서 애플리케이션을 호스팅합니다 . Auto Scaling 그룹의 EC2 인스턴스를 대상\n으로 하여 Application Load Balancer 를 설정합니다 .\n​",
      "D": "Host the application on Amazon Elastic Container Service (Amazon ECS). Set up an Application \nLoad Balancer with Amazon ECS as the target.\nAmazon Elastic Container Service(Amazon ECS)에서 애플리케이션을 호스팅합니다 . Amazon ECS를 \n대상으로 하여 Application Load Balancer 를 설정합니다 .\n​\n​→ 다른 컨테이너로 분할하여 활성화 할 수 있으며 , 관리형 서비스이므로 요건에 충족한다 .\nAmazon ECS"
    },
    "정답": "D"
  },
  {
    "문제번호": 144,
    "질문": "A company recently started using Amazon Aurora as the data store for its global ecommerce \napplication. When large reports are run, developers report that the ecommerce application is \nperforming poorly. After reviewing metrics in Amazon CloudWatch, a solutions architect finds that the \nReadIOPS and CPUUtilizalion metrics are spiking when monthly reports run.\nWhat is the MOST cost-effective solution?\n한 회사는 최근 글로벌 전자 상거래 애플리케이션의 데이터 저장소로 Amazon Aurora 를 사용하기 시작\n했습니다 . 대규모 보고서가 실행되면 개발자는 전자상거래 애플리케이션의 성능이 좋지 않다고 보고합\n니다. Amazon CloudWatch 의 지표를 검토한 후 솔루션 설계자는 월별 보고서가 실행될 때 ReadIOPS \n및 CPUUtilizalion 지표가 급증하고 있음을 발견했습니다 .\n가장 비용 효율적 인 솔루션은 무엇입니까 ?\n​",
    "보기": {
      "A": "Migrate the monthly reporting to Amazon Redshift.\n월별 보고를 Amazon Redshift 로 마이그레이션합니다 .\n​",
      "B": "Migrate the monthly reporting to an Aurora Replica.\n월별 보고를 Aurora 복제본으로 마이그레이션합니다 .\n​\n→ Amazon Aurora\n: 데이터베이스 워크로드를 위해 특별히 구축된 SSD 지원 가상화 스토리지 계층을 활용하여 읽기 전용 \n복제본의 이점을 더욱 확장한다 . Amazon Aurora 읽기 전용 복제본은 소스 인스턴스와 동일한 기본 스\n토리지를 공유하므로 비용이 절감되고 데이터를 복제본 \n노드에 복사할 필요가 없다. \n​",
      "C": "Migrate the Aurora database to a larger instance class.\nAurora 데이터베이스를 더 큰 인스턴스 클래스로 마이그레이션합니다 .\n​",
      "D": "Increase the Provisioned IOPS on the Aurora instance.\nAurora 인스턴스에서 프로비저닝된 *IOPS 를 늘립니다 .\n** IOPS(Input/Output Operations Per Second, IOPS) : HDD, SDD 또는 NVMe 등 저장장치의 속도를 나\n타내는데 사용되는 측정단위\n​\n​Docker 컨테이너를 지원하는 확장성과 성능이 뛰어난 컨테이너 관리 서비스\n자체 클러스터 관리 인프라를 \n설치, 운영, 조정할 필요 없이, \n간편하게 컨테이너를 애플리케\n이션의 빌딩 블록으로 사용Docker 컨테이너를 사용하여 \n장기 실행 애플리케이션 , 서비\n스 및 배치 프로세스를 예약애플리케이션 가용성을 유지 관\n리하고 애플리케이션 용량 요구 \n사항에 따라 컨테이너 규모를 \n확장하거나 축소"
    },
    "정답": "B"
  },
  {
    "문제번호": 145,
    "질문": "A company hosts a website analytics application on a single Amazon EC2 On-Demand Instance. The \nanalytics software is written in PHP and uses a MySQL database. The analytics software, the web \nserver that provides PHP, and the database server are all hosted on the EC2 instance. The \napplication is showing signs of performance degradation during busy times and is presenting 5xx \nerrors. The company needs to make the application scale seamlessly.\nWhich solution will meet these requirements MOST cost-effectively?\n회사는 단일 Amazon EC2 온디맨드 인스턴스에서 웹 사이트 분석 애플리케이션을 호스팅합니다 . 분석 \n소프트웨어는 PHP로 작성되었으며 MySQL 데이터베이스를 사용합니다 . 분석 소프트웨어 , PHP를 제공\n하는 웹 서버 및 데이터베이스 서버는 모두 EC2 인스턴스에서 호스팅됩니다 . 응용 프로그램은 바쁜 시\n간 동안 성능 저하 징후를 보이고 5xx 오류를 표시합니다 . 회사는 애플리케이션을 원활하게 확장해야 \n합니다 .\n어떤 솔루션이 이러한 요구 사항을 가장 비용 효율적으로 충족합니까 ?\n​",
    "보기": {
      "A": "Migrate the database to an Amazon RDS for MySQL DB instance. Create an AMI of the web \napplication. Use the AMI to launch a second EC2 On-Demand Instance. Use an Application Load \nBalancer to distribute the load to each EC2 instance.\n데이터베이스를 Amazon RDS for MySQL DB 인스턴스로 마이그레이션합니다 . 웹 애플리케이션의 AMI\n를 생성합니다 . AMI를 사용하여 두 번째 EC2 온디맨드 인스턴스를 시작합니다 . Application Load \nBalancer 를 사용하여 각 EC2 인스턴스에 로드를 분산합니다 .\n​",
      "B": "Migrate the database to an Amazon RDS for MySQL DB instance. Create an AMI of the web \napplication. Use the AMI to launch a second EC2 On-Demand Instance. Use Amazon Route 53 \nweighted routing to distribute the load across the two EC2 instances.\n데이터베이스를 Amazon RDS for MySQL DB 인스턴스로 마이그레이션합니다 . 웹 애플리케이션의 AMI\n를 생성합니다 . AMI를 사용하여 두 번째 EC2 온디맨드 인스턴스를 시작합니다 . Amazon Route 53 가\n중 라우팅을 사용하여 두 EC2 인스턴스에 로드를 분산합니다 .\n​",
      "C": "Migrate the database to an Amazon Aurora MySQL DB instance. Create an AWS Lambda function \nto stop the EC2 instance and change the instance type. Create an Amazon CloudWatch alarm to \ninvoke the Lambda function when CPU utilization surpasses 75%.\n데이터베이스를 Amazon Aurora MySQL DB 인스턴스로 마이그레이션합니다 . AWS Lambda 함수를 생\n성하여 EC2 인스턴스를 중지하고 인스턴스 유형을 변경합니다 . CPU 사용률이 75%를 초과할 때 \nLambda 함수를 호출하는 Amazon CloudWatch 경보를 생성합니다 .\n​",
      "D": "Migrate the database to an Amazon Aurora MySQL DB instance. Create an AMI of the web \napplication. Apply the AMI to a launch template. Create an Auto Scaling group with the launch \ntemplate Configure the launch template to use a Spot Fleet. Attach an Application Load Balancer to \nthe Auto Scaling group.\n데이터베이스를 Amazon Aurora MySQL DB 인스턴스로 마이그레이션합니다 . 웹 애플리케이션의 AMI\n를 생성합니다 . 시작 템플릿에 AMI를 적용합니다 . 시작 템플릿으로 Auto Scaling 그룹 생성 스팟 집합\n을 사용하도록 시작 템플릿을 구성합니다 . Auto Scaling 그룹에 Application Load Balancer 를 연결합니\n다.\n​\n​→ 스팟 플릿 (스팟 집합) = 스팟 + 온디맨드 인스턴스\n: 사용자가 지정한 기준에 따라 시작되는 스팟 인스턴스의 집합이며 선택적으로 온디맨드 인스턴스 집\n합이다 . \n스팟 플릿은 사용자의 요구 사항을 충족하는 스팟 용량 풀을 선택하고 플릿에 대한 목표 용량을 충족\n하는 스팟 인스턴스를 시작한다 . \n기본적으로 스팟 집합은 플릿에서 스팟 인스턴스가 종료된 후 교체 인스턴스를 시작하여 목표 용량을 \n유지하도록 설정되어 있다. \n스팟 플릿을 인스턴스가 종료된 후에는 유지되지 않는 일회성 요청으로 제출할 수도 있다. \n스팟 플릿 요청에 온디맨드 인스턴스 요청을 포함할 수 있다.\n​"
    },
    "정답": "D"
  },
  {
    "문제번호": 146,
    "질문": "A company runs a stateless web application in production on a group of Amazon EC2 On-Demand \nInstances behind an Application Load Balancer. The application experiences heavy usage during an \n8-hour period each business day. Application usage is moderate and steady overnight. Application \nusage is low during weekends.\nThe company wants to minimize its EC2 costs without affecting the availability of the application.\nWhich solution will meet these requirements?\n회사는 Application Load Balancer 뒤의 Amazon EC2 온디맨드 인스턴스 그룹에서 프로덕션 환경에서 \n상태 비저장 웹 애플리케이션을 실행합니다 . 매일 8시간 동안 애플리케이션 사용량이 많습니다 . 응용 \n프로그램 사용량은 보통이고 밤새 안정적입니다 . 주말에는 애플리케이션 사용량이 적습니다 .\n이 회사는 애플리케이션의 가용성에 영향을 주지 않으면서 EC2 비용을 최소화 하려고 합니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n​",
    "보기": {
      "A": "Use Spot Instances for the entire workload.\n전체 워크로드에 대해 스팟 인스턴스를 사용합니다 .\n​",
      "B": "Use Reserved Instances for the baseline level of usage. Use Spot instances for any additional \ncapacity that the application needs.\n기본 사용량 수준에 대해 예약 인스턴스를 사용합니다 . 애플리케이션에 필요한 추가 용량에 대해 스팟 \n인스턴스를 \n사용합니다 .\n​\n▶오답\n​C.\nUse On-Demand Instances for the baseline level of usage. Use Spot Instances for any additional \ncapacity that the application needs.\n기준 사용 수준에 대해 온디맨드 인스턴스를 사용합니다 . 애플리케이션에 필요한 추가 용량에 대해 스\n팟 인스턴스를 \n사용합니다 .\n→ 현재 온디맨드를 사용중이기 때문에 정답에서 제외.",
      "C": "Use On-Demand Instances for the baseline level of usage. Use Spot Instances for any additional \ncapacity that the application needs.\n기준 사용 수준에 대해 온디맨드 인스턴스를 사용합니다 . 애플리케이션에 필요한 추가 용량에 대해 스\n팟 인스턴스를 \n사용합니다 .\n​",
      "D": "Use Dedicated Instances for the baseline level of usage. Use On-Demand Instances for any \nadditional capacity that the application needs.\n기본 사용량 수준에 대해 전용 인스턴스를 사용합니다 . 애플리케이션에 필요한 추가 용량에 대해 온디\n맨드 인스턴스를 \n사용합니다 .\n​"
    },
    "정답": "B"
  },
  {
    "문제번호": 147,
    "질문": "A company needs to retain application log files for a critical application for 10 years. The application \nteam regularly accesses logs from the past month for troubleshooting, but logs older than 1 month \nare rarely accessed. The application generates more than 10 TB of logs per month.\nWhich storage option meets these requirements MOST cost-effectively?\n회사는 중요한 애플리케이션에 대한 애플리케이션 로그 파일을 10년 동안 보관해야 합니다 . 애플리케이\n션 팀은 문제 해결을 위해 지난 달의 로그에 정기적으로 액세스하지만 1개월 이상 된 로그는 거의 액\n세스하지 않습니다 . 애플리케이션은 매월 10TB 이상의 로그를 생성합니다 .\n이러한 요구 사항을 가장 비용 효율적 으로 충족하는 스토리지 옵션은 무엇입니까 ?\n​",
    "보기": {
      "A": "Store the logs in Amazon S3. Use AWS Backup to move logs more than 1 month old to S3 Glacier \nDeep Archive.\nAmazon S3에 로그를 저장합니다 . AWS Backup 을 사용하여 1개월 이상 된 로그를 S3 Glacier Deep \nArchive 로 이동합니다 .\n​",
      "B": "Store the logs in Amazon S3. Use S3 Lifecycle policies to move logs more than 1 month old to S3 \nGlacier Deep Archive.\nAmazon S3에 로그를 저장합니다 . S3 수명 주기 정책을 사용하여 1개월 이상 된 로그를 S3 Glacier \nDeep Archive 로 \n이동합니다 .​\n→ S3 + Glacier 가 가장 비용 효율적",
      "C": "Store the logs in Amazon CloudWatch Logs. Use AWS Backup to move logs more than 1 month old \nto S3 Glacier Deep Archive.\nAmazon CloudWatch Logs에 로그를 저장합니다 . AWS Backup 을 사용하여 1개월 이상 된 로그를 S3 \nGlacier Deep Archive 로 이동합니다 .\n​",
      "D": "Store the logs in Amazon CloudWatch Logs. Use Amazon S3 Lifecycle policies to move logs more \nthan 1 month old to S3 Glacier Deep Archive.\nAmazon CloudWatch Logs에 로그를 저장합니다 . Amazon S3 수명 주기 정책을 사용하여 1개월 이상 \n된 로그를 \nS3 Glacier Deep Archive 로 이동합니다 .\n​"
    },
    "정답": "B"
  },
  {
    "문제번호": 148,
    "질문": "A company has a data ingestion workflow that includes the following components:\nAn Amazon Simple Notification Service (Amazon SNS) topic that receives notifications about new \ndata deliveries\nAn AWS Lambda function that processes and stores the data\nThe ingestion workflow occasionally fails because of network connectivity issues. When failure occurs, \nthe corresponding data is not ingested unless the company manually reruns the job.\nWhat should a solutions architect do to ensure that all notifications are eventually processed?\n회사에 다음 구성 요소가 포함된 데이터 수집 워크플로가 있습니다 .\n새 데이터 전송에 대한 알림을 수신하는 Amazon Simple Notification Service(Amazon SNS) 주제 데이\n터\n를 처리하고 저장하는 AWS Lambda 함수\n네트워크 연결 문제로 인해 때때로 수집 워크플로가 실패합니다 . . 장애가 발생하면 회사에서 수동으로 \n작업을 재실행하지 않는 한 해당 데이터를 수집하지 않습니다 .\n모든 알림이 최종적으로 처리되도록 솔루션 설계자는 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Configure the Lambda function for deployment across multiple Availability Zones.\n여러 가용 영역에 배포하기 위해 Lambda 함수를 구성합니다 .\n​",
      "B": "Modify the Lambda function's configuration to increase the CPU and memory allocations for the \nfunction.\nLambda 함수의 구성을 수정하여 함수에 대한 CPU 및 메모리 할당을 늘립니다 .\n​",
      "C": "Configure the SNS topic’s retry strategy to increase both the number of retries and the wait time \nbetween retries.\n재시도 횟수와 재시도 간 대기 시간을 모두 늘리도록 SNS 주제의 재시도 전략을 구성합니다 .\n​",
      "D": "Configure an Amazon Simple Queue Service (Amazon SQS) queue as the on-failure destination. \nModify the Lambda function to process messages in the queue.\nAmazon Simple Queue Service(Amazon SQS) 대기열을 장애 발생 대상으로 구성합니다 . 대기열의 메\n시지를 처리하도록 Lambda 함수를 수정합니다 ."
    },
    "정답": "D"
  },
  {
    "문제번호": 149,
    "질문": "A company has a service that produces event data. The company wants to use AWS to process the \nevent data as it is received. The data is written in a specific order that must be maintained \nthroughout processing. The company wants to implement a solution that minimizes operational \noverhead.\nHow should a solutions architect accomplish this?\n회사에 이벤트 데이터를 생성하는 서비스가 있습니다 . 회사는 AWS를 사용하여 이벤트 데이터를 수신\n하는 대로 처리하려고 합니다 . 데이터는 처리 전반에 걸쳐 유지되어야 하는 특정 순서로 작성됩니다 . \n회사는 운영 오버헤드를 최소화하는 솔루션을 구현하려고 합니다 .\n솔루션 설계자는 이를 어떻게 달성해야 합니까 ?\n​",
    "보기": {
      "A": "Create an Amazon Simple Queue Service (Amazon SQS) FIFO queue to hold messages. Set up an \nAWS Lambda function to process messages from the queue.\n메시지를 보관할 Amazon Simple Queue Service(Amazon SQS) FIFO 대기열을 생성합니다 . 대기열의 \n메시지를 처리하도록 AWS Lambda 함수를 설정합니다 .\n​\n특정 순서 = FIFO 대기열\n​",
      "B": "Create an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications containing \npayloads to process. Configure an AWS Lambda function as a subscriber.\n처리할 페이로드가 포함된 알림을 전달하기 위해 Amazon Simple Notification Service(Amazon SNS) 주\n제를 생성합니다 . AWS Lambda 함수를 구독자로 구성합니다 .\n​",
      "C": "Create an Amazon Simple Queue Service (Amazon SQS) standard queue to hold messages. Set up \nan AWS Lambda function to process messages from the queue independently.\n메시지를 보관할 Amazon Simple Queue Service(Amazon SQS) 표준 대기열을 생성합니다 . 대기열의 \n메시지를 독립적으로 처리하도록 AWS Lambda 함수를 설정합니다 .\n​",
      "D": "Create an Amazon Simple Notification Service (Amazon SNS) topic to deliver notifications containing \npayloads to process. Configure an Amazon Simple Queue Service (Amazon SQS) queue as a \nsubscriber.\n처리할 페이로드가 포함된 알림을 전달하기 위해 Amazon Simple Notification Service(Amazon SNS) 주\n제를 생성합니다 . Amazon Simple Queue Service(Amazon SQS) 대기열을 구독자로 구성합니다 .\n​"
    },
    "정답": "A"
  },
  {
    "문제번호": 150,
    "질문": "A company is migrating an application from on-premises servers to Amazon EC2 instances. As part \nof the migration design requirements, a solutions architect must implement infrastructure metric \nalarms. The company does not need to take action if CPU utilization increases to more than 50% \nfor a short burst of time. However, if the CPU utilization increases to more than 50% and read IOPS \non the disk are high at the same time, the company needs to act as soon as possible. The \nsolutions architect also must reduce false alarms.\nWhat should the solutions architect do to meet these requirements?\n회사는 온프레미스 서버에서 Amazon EC2 인스턴스로 애플리케이션을 마이그레이션하고 있습니다 . 마\n이그레이션 설계 요구 사항의 일부로 솔루션 설계자는 인프라 메트릭 경보를 구현해야 합니다 . CPU 사\n용률이 단기간에 50% 이상으로 증가하는 경우 회사는 조치를 취할 필요가 없습니다 . 하지만 CPU 사용\n률이 50% 이상으로 증가하고 디스크의 읽기 IOPS가 동시에 높다면 회사에서 최대한 빨리 조치를 취해\n야 합니다 .솔루션 설계자는 또한 오경보를 줄여야 합니다 .\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Create Amazon CloudWatch composite alarms where possible.\n가능한 경우 Amazon CloudWatch 복합 경보를 생성합니다 .\n​\n→ Amazon CloudWatch 복합 경보\n: 복합 경보는 다른 경보의 상태를 모니터링하여 상태를 확인한다 .복합 경보를 사용하면 경보 노이즈를 \n줄일 수 있다. \n예를 들어 기본 지표 경보가 특정 조건을 충족하면 ALARM 상태로 전환되는 복합 경보를 만들 수 있\n다. \n그런 다음 기본 지표 경보가 작업을 수행하지 않도록 구성하여 , 기본 지표 경보가 ALARM 상태로 전환\n될 때 복합 경보가 \nALARM 상태로 전환되고 알림을 보내도록 설정할 수 있다.\n​\n•SNS 주제에 알림\n•AWS Systems Manager Ops Center 에 OpsItem 생성\n•AWS Systems Manager Incident Manager 에 인시던트 생성\n​\n복합 경보의 모든 기본 경보는 복합 경보와 동일한 AWS 계정 및 리전에 있어야 한다. \n단일 복합 경보로 100개의 기본 경보를 모니터링할 수 있고, 150개의 복합 경보로 단일 기본 경보를 \n모니터링할 수 있다.\n​\n▶오답\nAmazon CloudWatch Synthetics\n: 경량 모듈식 카나리 (Canary) 테스트를 사용하여 웹 애플리케이션 모니터링 서비스이다 .\n전체 트랜잭션에서 끊어진 링크나 작동하지 않는 링크, 단계별 작업 완료, 페이지 로드 오류, UI 자산의 \n로드 지연 시간, 복잡한 마법사 흐름 또는 체크아웃 흐름을 모니터할 수 있다. 애플리케이션에 고객 트\n래픽이 없더라도 지속적으로 고객 경험을 확인하고 , 고객보다 먼저 문제를 발견할 수 있다.",
      "B": "Create Amazon CloudWatch dashboards to visualize the metrics and react to issues quickly.\nAmazon CloudWatch 대시보드를 생성하여 지표를 시각화하고 문제에 신속하게 대응합니다 .\n​",
      "C": "Create Amazon CloudWatch Synthetics canaries to monitor the application and raise an alarm.\nAmazon CloudWatch Synthetics 카나리아를 생성하여 애플리케이션을 모니터링하고 경보를 발생시킵니\n다.\n​",
      "D": "Create single Amazon CloudWatch metric alarms with multiple metric thresholds where possible.\n가능한 경우 여러 지표 임계값으로 단일 Amazon CloudWatch 지표 경보를 생성합니다 .\n​"
    },
    "정답": "A"
  },
  {
    "문제번호": 151,
    "질문": "A company wants to migrate its on-premises data center to AWS. According to the company's \ncompliance requirements, the company can use only the ap-northeast-3 Region. Company \nadministrators are not permitted to connect VPCs to the internet.\nWhich solutions will meet these requirements? (Choose two.)\n회사에서 온프레미스 데이터 센터를 AWS로 마이그레이션하려고 합니다 . 회사의 규정 준수 요구 사항\n에 따라 회사는 ap-northeast-3 지역만 사용할 수 있습니다 . 회사 관리자는 VPC를 인터넷에 연결할 수 \n없습니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ? (2개를 선택하세요 .)\n​",
    "보기": {
      "A": "Use AWS Control Tower to implement data residency guardrails to deny internet access and deny \naccess to all AWS Regions except ap-northeast-3.\nAWS Control Tower 를 사용하여 데이터 상주 가드레일을 구현하여 인터넷 액세스를 거부하고 \nap-northeast-3 를 제외한 모든 AWS 리전에 대한 \n액세스를 거부합니다 .\n​\n→ AWS Control Tower\n: AWS Control Tower 는 안전한 다중 계정 AWS 환경을 설정하고 관리하는 가장 쉬운 방법을 제공한\n다.\n잘 설계된 새로운 다중 계정 환경을 쉽게 설정하고 보안, 운영 및 내부 규정 준수를 위한 규칙으로 \nAWS 워크로드를 \n관리할 수 있는 단일 위치를 제공한다 .\nAWS Control Tower 는 AWS 환경의 지속적인 거버넌스를 위한 \"가드레일 \"을 제공한다 . 가드레일은 선\n택한 정책을 준수하지 \n않는 리소스 배포를 방지하거나 프로비저닝된 리소스의 비준수를 감지하여 거버넌스 제어를 제공한다 . \nAWS Control Tower 는 기준선을 설정하기 위한 AWS CloudFormation, 구성 변경을 방지하기 위한 \nAWS Organizations \n서비스 제어 정책(SCP), 부적합을 지속적으로 감지하기 위한 AWS Config 규칙과 같은 여러 빌딩 블록\n을 사용하여 \n가드레일을 자동으로 구현한다 .\n​\nAWS Control Tower 는 데이터 상주를 지원하는 일련의 예방 및 탐지 가드레일을 제공한다 . \n데이터 상주를 통해 고객 콘텐츠를 호스팅하는 위치를 제어할 수 있다. \n이를 통해 여러 지역에서 호스팅할지 또는 정의된 지역에서 제자리에 보관할지 선택할 수 있다.\n​\n​",
      "B": "Use rules in AWS WAF to prevent internet access. Deny access to all AWS Regions except \nap-northeast-3 in the AWS account settings.\nAWS WAF의 규칙을 사용하여 인터넷 액세스를 방지합니다 . AWS 계정 설정에서 ap-northeast-3 을 제\n외한 모든 AWS 리전에 대한 액세스를 거부합니다 .\n​",
      "C": "Use AWS Organizations to configure service control policies (SCPS) that prevent VPCs from gaining \ninternet access. Deny access to all AWS Regions except ap-northeast-3.\nAWS Organizations 를 사용하여 VPC가 인터넷에 액세스하지 못하도록 하는 서비스 제어 정책(SCPS) 을 \n구성합니다 . ap-northeast-3 을 제외한 모든 AWS 리전에 대한 액세스를 거부합니다 .\n​",
      "D": "Create an outbound rule for the network ACL in each VPC to deny all traffic from 0.0.0.0/0. Create \nan IAM policy for each user to prevent the use of any AWS Region other than ap-northeast-3.\n각 VPC의 네트워크 ACL에 대한 아웃바운드 규칙을 생성하여 0.0.0.0/0 의 모든 트래픽을 거부합니다 . \nap-northeast-3 이외의 AWS 리전을 사용하지 못하도록 각 사용자에 대한 IAM 정책을 생성합니다 .\n​\nE.\nUse AWS Config to activate managed rules to detect and alert for internet gateways and to detect \nand alert for new resources deployed outside of ap-northeast-3.\nAWS Config 를 사용하여 관리형 규칙을 활성화하여 인터넷 게이트웨이를 감지 및 경고하고 \nap-northeast-3 외부에 배포된 새 리소스를 감지 및 경고합니다 .\n​"
    },
    "정답": "A"
  },
  {
    "문제번호": 152,
    "질문": "A company uses a three-tier web application to provide training to new employees. The application is \naccessed for only 12 hours every day. The company is using an Amazon RDS for MySQL DB \ninstance to store information and wants to minimize costs.\nWhat should a solutions architect do to meet these requirements?\n회사에서 3계층 웹 응용 프로그램을 사용하여 신입 직원에게 교육을 제공합니다 . 애플리케이션은 매일 \n12시간 동안만 액세스됩니다 . 회사는 Amazon RDS for MySQL DB 인스턴스 를 사용하여 정보를 저장하\n고 비용을 최소화 하려고 합니다 .\n솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까 ?\n​",
    "보기": {
      "A": "Configure an IAM policy for AWS Systems Manager Session Manager. Create an IAM role for the \npolicy. Update the trust relationship of the role. Set up automatic start and stop for the DB instance.\nAWS Systems Manager Session Manager 에 대한 IAM 정책을 구성합니다 . 정책에 대한 IAM 역할을 생성\n합니다 . 역할의 신뢰 관계를 업데이트하십시오 . DB 인스턴스에 대한 자동 시작 및 중지를 설정합니다 .\n​\n→세션 매니저가 아닌 AWS Systems Manager State Manager 에 대한 IAM 정책을 구성해야함 .",
      "B": "Create an Amazon ElastiCache for Redis cache cluster that gives users the ability to access the \ndata from the cache when the DB instance is stopped. Invalidate the cache after the DB instance is \nstarted.\nDB 인스턴스가 중지될 때 사용자가 캐시의 데이터에 액세스할 수 있는 기능을 제공하는 Redis 용 \nAmazon ElastiCache 캐시 클러스터를 생성합니다 . DB 인스턴스가 시작된 후 캐시를 무효화합니다 .\n​",
      "C": "Launch an Amazon EC2 instance. Create an IAM role that grants access to Amazon RDS. Attach \nthe role to the EC2 instance. Configure a cron job to start and stop the EC2 instance on the \ndesired schedule.\nAmazon EC2 인스턴스를 시작합니다 . Amazon RDS에 대한 액세스 권한을 부여하는 IAM 역할을 생성\n합니다 . 역할을 EC2 인스턴스에 연결합니다 . 원하는 일정에 따라 EC2 인스턴스를 시작 및 중지하도록 \n크론 작업을 구성합니다 .\n​",
      "D": "Create AWS Lambda functions to start and stop the DB instance. Create Amazon EventBridge \n(Amazon CloudWatch Events) scheduled rules to invoke the Lambda functions. Configure the Lambda \nfunctions as event targets for the rules.\nAWS Lambda 함수를 생성하여 DB 인스턴스를 시작 및 중지합니다 . Amazon EventBridge(Amazon \nCloudWatch Events) 예약 규칙을 생성하여 Lambda 함수를 호출합니다 . 규칙에 대한 이벤트 대상으로 \nLambda 함수를 구성합니다 .\n​\n→ AWS Lambda 를 사용하며 Amazon RDS 중지 및 시작 예약\n: AWS Lambda 및 Amazon EventBridge 를 사용하여 Lambda 함수가 컴퓨팅 비용을 절약하기 위해 특\n정 태그로 유휴 데이터베이스를 중지 및 시작하도록 예약할 수 있는 솔루션을 제시합\n​\nAWS RDS\n: 클라우드에서 관계형 데이터베이스를 쉽게 설정, 운영 및 확장할 수 있다. \nAmazon RDS는 DBA가 대부분의 일상적인 작업을 자동화하여 조직에 가치를 더하는 다른 중요한 작업\n에 집중할 수 있도록 도와준다 .\n​\nAWS Lambda\n: 서버를 관리하지 않고도 코드를 실행할 수 있는 컴퓨팅 서비스 . 서버 프로비저닝 , 운영 체제 구성, 애\n플리케이션 설치 등에 대해 걱정할 필요가 없다.\n​\nAmazon EventBridge\n: 이벤트를 생성하고 이에 대한 응답으로 특정 작업을 할당할 수 있는 간단한 규칙을 사용한다 .\n​\n​\n▶오답"
    },
    "정답": "D"
  },
  {
    "문제번호": 153,
    "질문": "A company sells ringtones created from clips of popular songs. The files containing the ringtones are \nstored in Amazon S3 Standard and are at least 128 KB in size. The company has millions of files, \nbut downloads are infrequent for ringtones older than 90 days. The company needs to save money \non storage while keeping the most accessed files readily available for its users.\nWhich action should the company take to meet these requirements MOST cost-effectively?\n회사에서 인기 있는 노래 클립으로 만든 벨소리를 판매합니다 . 벨소리가 포함된 파일은 Amazon S3 \nStandard 에 저장되며 크기는 최소 128KB 입니다 . 이 회사에는 수백만 개의 파일이 있지만 90일보다 오\n래된 벨소리의 경우 다운로드가 드뭅니다 .회사는 가장 많이 액세스하는 파일을 사용자가 쉽게 사용할 \n수 있도록 유지하면서 스토리지 비용을 절약해야 합니다 .\n이러한 요구 사항을 가장 비용 효율적 으로 충족하기 위해 회사는 어떤 조치를 취해야 합니까 ?\n​",
    "보기": {
      "A": "Configure S3 Standard-Infrequent Access (S3 Standard-IA) storage for the initial storage tier of the \nobjects.\n객체의 초기 스토리지 계층에 대해 S3 Standard-Infrequent Access(S3 Standard-IA) 스토리지를 구성합\n니다.\n​",
      "B": "Move the files to S3 Intelligent-Tiering and configure it to move objects to a less expensive storage \ntier after 90 days.\n파일을 S3 Intelligent-Tiering 으로 이동하고 90일 후에 객체를 더 저렴한 스토리지 계층으로 이동하도록 \n구성합니다 .\n​",
      "C": "Configure S3 inventory to manage objects and move them to S3 Standard-Infrequent Access (S3 \nStandard-1A) after 90 days.\n객체를 관리하도록 S3 인벤토리를 구성하고 90일 후에 객체를 S3 Standard-Infrequent Access(S3 \nStandard-1A) 로 이동합니다 .\n​",
      "D": "Implement an S3 Lifecycle policy that moves the objects from S3 Standard to S3 Standard-Infrequent \nAccess (S3 Standard-1A) after 90 days.\n90일 후에 객체를 S3 Standard 에서 S3 Standard-Infrequent Access(S3 Standard-1A) 로 이동하는 S3 \n수명 주기 정책을 구현합니다 .\n​"
    },
    "정답": "D"
  },
  {
    "문제번호": 154,
    "질문": "A company needs to save the results from a medical trial to an Amazon S3 repository. The \nrepository must allow a few scientists to add new files and must restrict all other users to read-only \naccess. No users can have the ability to modify or delete any files in the repository. The company \nmust keep every file in the repository for a minimum of 1 year after its creation date.\nWhich solution will meet these requirements?\n회사는 의료 시험의 결과를 Amazon S3 리포지토리에 저장해야 합니다 . 리포지토리는 일부 과학자가 \n새 파일을 추가할 수 있도록 허용해야 하고 다른 모든 사용자는 읽기 전용 액세스로 제한해야 합니다 . \n어떤 사용자도 저장소의 파일을 수정하거나 삭제할 수 없습니다 .회사는 모든 파일을 생성일로부터 최소 \n1년 동안 저장소에 보관해야 합니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n​",
    "보기": {
      "A": "Use S3 Object Lock in governance mode with a legal hold of 1 year.\n거버넌스 모드에서 1년의 법적 보유 기간으로 S3 Object Lock을 사용하십시오 .\n​",
      "B": "Use S3 Object Lock in compliance mode with a retention period of 365 days.\n보존 기간이 365일인 규정 준수 모드에서 S3 Object Lock을 사용합니다 .\nAmazon S3 객체 잠금 보관모드\n거버넌스 모드 (Governance mode) 규정 준수 모드(Compliance mode)\n•특별한 권한이 없는 한 사용자는 객체 \n버전을 덮어쓰거나 삭제하거나 잠금 설\n정을 변경할 수 없다. \n•거버넌스 모드를 사용하면 대부분의 사•보호된 객체 버전은 AWS 계정의 루트 \n사용자를 포함한 어떤 사용자도 덮어쓰\n거나 삭제할 수 없다.\n•규정 준수 모드에서 객체를 잠그면 보",
      "C": "Use an IAM role to restrict all users from deleting or changing objects in the S3 bucket. Use an S3 \nbucket policy to only allow the IAM role.\nIAM 역할을 사용하여 모든 사용자가 S3 버킷의 객체를 삭제하거나 변경하지 못하도록 제한합니다 . S3 \n버킷 정책을 사용하여 IAM 역할만 허용합니다 .\n​",
      "D": "Configure the S3 bucket to invoke an AWS Lambda function every time an object is added. \nConfigure the function to track the hash of the saved object so that modified objects can be marked \naccordingly.\n객체가 추가될 때마다 AWS Lambda 함수를 호출하도록 S3 버킷을 구성합니다 . 수정된 개체가 그에 따\n라 표시될 수 있도록 저장된 개체의 해시를 추적하는 기능을 구성합니다 .\n​"
    },
    "정답": "B"
  },
  {
    "문제번호": 155,
    "질문": "A large media company hosts a web application on AWS. The company wants to start caching \nconfidential media files so that users around the world will have reliable access to the files. The \ncontent is stored in Amazon S3 buckets. The company must deliver the content quickly, regardless \nof where the requests originate geographically.\nWhich solution will meet these requirements?\n대규모 미디어 회사는 AWS에서 웹 애플리케이션을 호스팅합니다 . 이 회사는 전 세계 사용자가 파일에 \n안정적으로 액세스할 수 있도록 기밀 미디어 파일 캐싱을 시작하려고 합니다 . 콘텐츠는 Amazon S3 버\n킷에 저장됩니다 . 회사는 요청의 지리적 위치에 관계없이 콘텐츠를 신속하게 제공해야 합니다 .\n어떤 솔루션이 이러한 요구 사항을 충족합니까 ?\n​",
    "보기": {
      "A": "Use AWS DataSync to connect the S3 buckets to the web application.\nAWS DataSync 를 사용하여 S3 버킷을 웹 애플리케이션에 연결합니다 .\n​",
      "B": "Deploy AWS Global Accelerator to connect the S3 buckets to the web application.\nAWS Global Accelerator 를 배포하여 S3 버킷을 웹 애플리케이션에 연결합니다 .\n​",
      "C": "용자가 객체를 삭제하지 못하도록 보호\n하지만 , 필요에 따라 일부 사용자에게 \n보관 설정을 변경하거나 객체를 삭제할 \n수 있는 권한을 부여할 수 있다.\n•규정 준수 모드 보관 기간을 생성하기 \n전에 거버넌스 모드를 사용하여 보관 \n기간 설정을 테스트할 수도 있다.관 모드를 변경할 수 없으며 보관 기간\n을 줄일 수 없다. \n•규정 준수 모드는 보관 기간 동안 객체 \n버전을 덮어쓰거나 삭제할 수 없도록 \n하는 데 도움이 된다.\nDeploy Amazon CloudFront to connect the S3 buckets to CloudFront edge servers.\nAmazon CloudFront 를 배포하여 S3 버킷을 CloudFront 엣지 서버에 연결합니다 .",
      "D": "Use Amazon Simple Queue Service (Amazon SQS) to connect the S3 buckets to the web \napplication.\nAmazon Simple Queue Service(Amazon SQS)를 사용하여 S3 버킷을 웹 애플리케이션에 연결합니다 .\n​"
    },
    "정답": "C"
  },
  {
    "문제번호": 156,
    "질문": "A company produces batch data that comes from different databases. The company also produces \nlive stream data from network sensors and application APIs. The company needs to consolidate all \nthe data into one place for business analytics. The company needs to process the incoming data \nand then stage the data in different Amazon S3 buckets. Teams will later run one-time queries and \nimport the data into a business intelligence tool to show key performance indicators (KPIs).\nWhich combination of steps will meet these requirements with the LEAST operational overhead? \n(Choose two.)\n회사는 다른 데이터베이스에서 가져온 배치 데이터를 생성합니다 . 이 회사는 또한 네트워크 센서 및 애\n플리케이션 API에서 라이브 스트림 데이터를 생성합니다 . 회사는 비즈니스 분석을 위해 모든 데이터를 \n한 곳으로 통합해야 합니다 . 회사는 수신 데이터를 처리한 다음 다른 Amazon S3 버킷에 데이터를 준\n비해야 합니다 . 팀은 나중에 일회성 쿼리를 실행하고 데이터를 비즈니스 인텔리전스 도구로 가져와 핵\n심 성과 지표(KPI)를 표시합니다 .\n가장 적은 운영 오버헤드로 이러한 요구 사항을 충족하는 단계 조합은 무엇입니까 ? (2개를 선택하세\n요.)\n​",
    "보기": {
      "A": "Use Amazon Athena for one-time queries. Use Amazon QuickSight to create dashboards for KPIs.\n일회성 쿼리에는 Amazon Athena 를 사용하십시오 . Amazon QuickSight 를 사용하여 KPI용 대시보드를 \n생성합니다 .\n​",
      "B": "Use Amazon Kinesis Data Analytics for one-time queries. Use Amazon QuickSight to create \ndashboards for KPIs.\n일회성 쿼리에 Amazon Kinesis Data Analytics 를 사용합니다 . Amazon QuickSight 를 사용하여 KPI용 대\n시보드를 생성합니다 .\n​",
      "C": "Create custom AWS Lambda functions to move the individual records from the databases to an \nAmazon Redshift cluster.\n개별 레코드를 데이터베이스에서 Amazon Redshift 클러스터로 이동하는 사용자 지정 AWS Lambda 함\n수를 생성합니다 .\n​",
      "D": "Use an AWS Glue extract, transform, and load (ETL) job to convert the data into JSON format. \nLoad the data into multiple Amazon OpenSearch Service (Amazon Elasticsearch Service) clusters.\nAWS Glue 추출, 변환 및 로드(ETL) 작업을 사용하여 데이터를 JSON 형식으로 변환합니다 . 여러 \nAmazon OpenSearch Service(Amazon Elasticsearch Service) 클러스터에 데이터를 로드합니다 .\n​\nE.\nUse blueprints in AWS Lake Formation to identify the data that can be ingested into a data lake. \nUse AWS Glue to crawl the source, extract the data, and load the data into Amazon S3 in Apache \nParquet format.\nAWS Lake Formation 의 *블루 프린트를 사용하여 데이터 레이크에 수집할 수 있는 데이터를 식별합니\n다. AWS Glue를 사용하여 소스를 크롤링하고 , 데이터를 추출하고 , 데이터를 Apache Parquet 형식으로 \nAmazon S3에 로드합니다 .\n* 블루 프린트 : 데이터 레이크에 데이터를 손쉽게 입수할 수 있도록 도와주는 데이터 관리 템플릿 .\n​\n​"
    },
    "정답": "A"
  }
]